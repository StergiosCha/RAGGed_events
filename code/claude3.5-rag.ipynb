{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159c15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Multi-Knowledge Graph RAG System with Chunking - CLAUDE 4 VERSION\n",
      "Anthropic API Key loaded successfully.\n",
      "Loaded text from part_aa\n",
      "üìÑ Using YOUR text from part_aa\n",
      "üìù Text length: 398568 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:52:55,303 - INFO - Loaded location ontology from locations.owl\n",
      "2025-05-29 08:52:55,312 - INFO - Use pytorch device_name: mps\n",
      "2025-05-29 08:52:55,312 - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude 4 client initialized successfully.\n",
      "\n",
      "üìö Setting up RAG vector store...\n",
      "üìö Vector store created with 1980 text segments\n",
      "üî¢ Total tokens in text: 86,945\n",
      "üìä Text is large, chunking into smaller pieces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:53:09,574 - INFO - Processing chunk 1 (117220 chars) - CLAUDE 4 WITH RAG\n",
      "2025-05-29 08:53:09,680 - INFO - Retrieved 7 relevant chunks via RAG for chunk 1\n",
      "2025-05-29 08:53:09,689 - INFO - Found potential locations in chunk 1: ['Epidamnus', 'King Cyrus', 'Pellichas', 'Lacedaemon\\n\\nThe Athenians', 'Pausanias With', 'Cyclades', 'Megarians', 'Europe', 'All Lacedaemonians', 'Greece', 'Leos', 'Corinth But', 'Each', 'Mygdonia', 'Bottica', 'Boeotian', 'Richard Crawley\\nWith Permission', 'Proteas', 'Take', 'Ambraciots', 'Aristogiton', 'Aeginetans', 'Alexander', 'Adimantus', 'Corinthians There', 'Ameinocles', 'Surely', 'Everywhere', 'Acheron', 'While Callias', 'Sicily With', 'Corcyra Not', 'Eurystheus', 'THE PROJECT GUTENBERG EBOOK THE HISTORY', 'Athenian', 'Heraclids Atreus', 'Gods', 'These Corcyraeans', 'Archidamus Last', 'CHAPTER XVIII\\nCHAPTER XIX\\nCHAPTER', 'Olynthus', 'But Archidamus', 'Phthiotis', 'Being', 'Ionians', 'Hipparchus', 'Neither', 'Medes After', 'Richard Crawley\\nRelease Date', 'Athens They', 'Boeotians', 'Argives', 'Spartan', 'Macedonia', 'Difficulty', 'Engaged', 'Lycomedes', 'Further', 'War', 'Causes', 'Pisistratus', 'Vote', 'Sestos', 'Illyrian', 'Corinthian', 'Samos And', 'Did', 'Medes They', 'Greece\\nThis Translation', 'Isarchus When', 'Summoning', 'Epidamnian', 'Hence', 'Sicily', 'Acherusian', 'THE PELOPONNESIAN WAR', 'Chersonese', 'Ozolian Locrians', 'Besides', 'Peloponnesian Confederacy', 'Hellespont', 'Cyrus', 'Peloponnese', 'Almost', 'Phaeacians This', 'BOOK VII\\nCHAPTER XXI\\nCHAPTER XXII\\nCHAPTER XXIII\\n\\nBOOK VIII\\nCHAPTER XXIV\\nCHAPTER XXV\\nCHAPTER XXVI\\nBOOK', 'Median', 'Asia', 'Upon', 'The Project Gutenberg', 'Pitane', 'The Mede', 'Euboea', 'Others', 'Peloponnese With', 'Here', 'Corcyraeans The', 'For', 'Sparta', 'Euboea The Lacedaemonians', 'Pelops', 'Corcyraean', 'Eurytimus', 'Subsequently', 'Thucydides\\nTranslator', 'Men', 'Phoenicians', 'Achilles', 'Philoctetes', 'Meikiades', 'Aisimides', 'Median War', 'CHAPTER XII\\nCHAPTER XIII\\nCHAPTER XIV\\n\\nBOOK', 'Peloponnesian War\\nAuthor', 'Delian Apollo About', 'Thessalians', 'Eleans', 'Believing', 'Darius', 'Euthycles', 'Perseus', 'Samians Dating', 'Having', 'Hellenic', 'Point Leukimme', 'Croesus', 'Rhenea', 'Samos Again', 'Lacedaemon', 'Project Gutenberg License', 'Marseilles', 'Salamis This', 'START', 'Work', 'Eurystheus And', 'Persians', 'Remember', 'Nor', 'THE HISTORY', 'Eurybatus', 'Troezen', 'The Affair', 'Could', 'Perhaps', 'While', 'Aware', 'Ephyre', 'Our', 'Leagrus', 'Sailing', 'Advancing', 'Cadmeis', 'The History', 'Peloponnese Complaints', 'Archestratus', 'Time', 'Mede Not', 'Corcyra', 'Times', 'The Progress', 'Corcyra They', 'Perdiccas', 'Calliades', 'Thessaly', 'Chalcidice', 'Abstinence', 'Lysicles', 'Confidence', 'However', 'Athenians', 'Returning', 'Well', 'Salamis', 'Ambracia', 'Hellen', 'Secondly', 'CHAPTER XVI\\nCHAPTER XVII\\n\\nBOOK', 'Ionian', 'Cephallonia', 'Apollo', 'Chios', 'Atreus', 'Again', 'Chalcidians', 'Hera', 'Turning', 'Olympic', 'Chalcis', 'Strepsa', 'Epicles Their', 'Ambraciot', 'Hellenes', 'The Thebans', 'Ilium', 'Samos', 'Callicrates', 'Hellas Never', 'Kestrine', 'Empire\\n\\nThe', 'Marathon', 'Lacedaemonius', 'Corinth Great', 'Perceiving', 'Thyamis', 'Andocides', 'Lake Bolbe', 'Sthenelaidas', 'Pydna Accordingly', 'Isarchidas', 'Athenians After', 'Hellas The', 'Strombichus', 'Arne', 'Leukimme Neither', 'The Athenians', 'Lacedaemonians Their', 'Lysimachus', 'From', 'They', 'Thrace', 'Formerly', 'Gigonus', 'Aetolians', 'Leogoras', 'Achaeans', 'Thirdly', 'And Perdiccas', 'Peloponnese But', 'Troy But', 'Aphytis', 'Whereas', 'Are', 'Polycrates', 'The Translator', 'The Lacedaemonians', 'Philip', 'Arcadians', 'Thus', 'United States', 'Instantly', 'Meanwhile', 'Cyllene', 'Samians', 'Hellas And', 'Corcyraeans', 'Thracian', 'Derdas', 'Produced', 'Iolaus', 'Corinthians After', 'Thessalus', 'Sicyon', 'Panathenaic', 'There', 'Title', 'Unable', 'Italy', 'Leotychides', 'Deucalion', 'Sermylians', 'Thucydides\\nThis', 'Epidamnus Advertisement', 'Eratocleides', 'Persian', 'Pale', 'Argos', 'Illyrians Sitting', 'Language', 'Hippias', 'Hellas For', 'Athenians That', 'Asopius Arrived', 'Epidaurus', 'His\\nGreat Predecessor', 'Ionia', 'Still', 'You', 'The Corcyraeans', 'Troy', 'Now', 'Mede', 'CHAPTER III\\nCHAPTER', 'Timanthes', 'Corinth', 'Dorian', 'CHAPTER VII\\nCHAPTER VIII\\n\\nBOOK III\\nCHAPTER', 'Persia', 'The Peloponnesian War', 'Immediately', 'Commencement', 'David Widger', 'King', 'Hellenes There', 'Corinth For', 'Corcyraeans After', 'Xenoclides', 'Eight', 'Phliasians', 'With', 'Seeing', 'Elis', 'Trojan War', 'CHAPTER III\\nCongress', 'Medes', 'Hellas Moreover', 'Chrysippus', 'Macedonian', 'Pallene So', 'Invited', 'Sybota Thus', 'Supremacy', 'Megarian', 'Carians', 'Corinthians', 'Aristeus', 'Minos', 'Defeated', 'Hermione', 'UTF', 'CHAPTER', 'Pelasgian', 'Callias', 'Samian', 'Halys', 'BOOK', 'Bottiaeans', 'Various', 'Chimerium', 'Anactorium', 'Translated', 'Attica', 'Phocaeans', 'Dorians', 'Athens', 'Thesprotis', 'Homer', 'Timanor', 'Arcadia', 'Aegina', 'English\\nCharacter', 'Elean', 'Delos', 'Such', 'Leukimme', 'The Corinthians', 'Failing', 'CONTENTS\\nBOOK', 'Ambracian', 'Now Agamemnon', 'Carthaginians', 'Trojan', 'Old', 'Peloponnesians', 'Indeed', 'Any', 'Phalius', 'Lesbos Both', 'Apollonia', 'Peloponnesian', 'Corcyraeans But', 'Sixty', 'The State', 'Peloponnesian War\\n\\nThucydides', 'Whenever', 'Archetimus', 'Athenians Thus', 'Planted', 'Albert Imrie', 'Ilium Twenty', 'Pallene', 'CONNOP THIRLWALL\\nHistorian', 'Hellas All', 'Aristides', 'Danaans', 'Gulf', 'Glaucon', 'Both', 'Delphi', 'Phormio', 'Corinthians When', 'Leucas', 'Ionic Gulf Its', 'Ionia There', 'Cambyses', 'Assuredly', 'The Corinthian', 'Eretria', 'Lacedaemonians', 'Tyndareus', 'Lacedaemonian', 'Leucadians', 'Diotimus', 'Remaining', 'Troy What', 'Respectfully Inscribed', 'Alive', 'Macedonians', 'Their', 'Heraclids', 'The Median War', 'Instead', 'Without', 'Sybota', 'Pydna', 'Wars', 'Homer Born', 'Agamemnon', 'Actium', 'Peloponnese And', 'Acarnanians', 'She', 'Cimon', 'Receiving', 'Beroea', 'Xerxes', 'Hellas', 'Zacynthus', 'Thucydides', 'Harmodius', 'According', 'Even', 'Thesprotis This', 'Arrived', 'Concerning', 'Epidamnians', 'God', 'Themistocles', 'Phoenician', 'Weigh', 'Accordingly', 'Sicilian', 'Trojans', 'Boeotia', 'Accordingly Attica', 'Peloponnesian War', 'Athenians Ten', 'Mycale', 'Thebes', 'Not', 'Mede Still', 'Abronichus', 'Taulantians', 'Megara', 'Therme']\n",
      "2025-05-29 08:53:09,690 - INFO - Registered new location: Epidamnus\n",
      "2025-05-29 08:53:09,690 - INFO - Found entities in chunk 1: ['The Project Gutenberg', 'The History', 'Peloponnesian War', 'Thucydides\\nThis', 'United States']...\n",
      "2025-05-29 08:53:09,691 - INFO - Enriched 1 locations with coordinates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Created 4 chunks\n",
      "\n",
      "üîÑ Processing chunks for event extraction with Claude 4 RAG...\n",
      "\n",
      "üîÑ Processing chunk 1/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:53:10,194 - INFO - Retrieved 0 facts from Wikidata for 'The Project Gutenberg'\n",
      "2025-05-29 08:53:10,936 - WARNING - Wikidata returned status 400 for Thucydides\n",
      "This\n",
      "2025-05-29 08:53:11,161 - WARNING - DBpedia returned status 400 for Thucydides\n",
      "This\n",
      "2025-05-29 08:53:18,391 - INFO - Retrieved 0 facts from Wikidata for 'Project Gutenberg License'\n",
      "2025-05-29 08:53:18,611 - INFO - Retrieved 0 facts from DBpedia for 'Project Gutenberg License'\n",
      "2025-05-29 08:53:21,971 - WARNING - Wikidata returned status 400 for Peloponnesian War\n",
      "Author\n",
      "2025-05-29 08:53:22,194 - WARNING - DBpedia returned status 400 for Peloponnesian War\n",
      "Author\n",
      "2025-05-29 08:53:32,775 - WARNING - Wikidata returned status 400 for Thucydides\n",
      "Translator\n",
      "2025-05-29 08:53:32,988 - WARNING - DBpedia returned status 400 for Thucydides\n",
      "Translator\n",
      "2025-05-29 08:53:37,785 - WARNING - Wikidata returned status 400 for Richard Crawley\n",
      "Release Date\n",
      "2025-05-29 08:53:38,002 - WARNING - DBpedia returned status 400 for Richard Crawley\n",
      "Release Date\n",
      "2025-05-29 08:53:54,697 - WARNING - ‚è∞ Timeout: 1/18 KG queries still pending, continuing with available results\n",
      "2025-05-29 08:53:59,117 - INFO - KG retrieval completed: 17/18 successful, 18 cache hits\n",
      "2025-05-29 08:54:12,072 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 08:54:12,077 - INFO - Generated RAG-enhanced RDF for chunk 1 using Claude 4\n",
      "2025-05-29 08:54:13,090 - INFO - Processing chunk 2 (115641 chars) - CLAUDE 4 WITH RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 2/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:54:13,319 - INFO - Retrieved 7 relevant chunks via RAG for chunk 2\n",
      "2025-05-29 08:54:13,329 - INFO - Found potential locations in chunk 2: ['Epidamnus', 'Diplomatic Skirmishes', 'Evarchus', 'Second Congress', 'Aristonus', 'Cyprian', 'Colonae', 'Anactorians', 'Lacedaemonians The', 'Cyclades', 'Woe', 'Long Wall', 'Hystaspes', 'Megarians', 'Lesbian', 'Athenians The', 'Polymedes', 'Afterwards', 'Zeus Meilichios', 'Nine Ways', 'Chalcidians Thus Sitalces', 'Ramphias', 'Fresh', 'Anxious', 'Boeotian', 'Sallying', 'Larisa', 'Proteas', 'Ambraciots', 'Aeginetans', 'Alexander', 'Abderite', 'Astacus', 'Marea', 'Molossian', 'Everywhere', 'Miletus The Byzantines', 'Thera', 'Epicles', 'Leonidas', 'Plataeans While', 'Paleans', 'Soon', 'Nile', 'Megarid', 'Thria', 'Athenians These', 'Aegean', 'Athenian', 'Thronium', 'Altogether', 'Leave', 'Athenians Reinforced', 'Ionia Going', 'Naucleides', 'Setting', 'Egyptians Inaros', 'Thrius', 'But Themistocles', 'Plataea', 'One', 'Being', 'Ionians', 'Oeniadae', 'Libyans', 'Eumachus', 'Samos Sixteen', 'Athens They', 'Let', 'Pharos', 'Boeotians', 'Pellene', 'Argives', 'Stesagoras', 'Helot', 'Themistocles\\n\\nAfter', 'Spartan', 'Locrians The', 'Cranian', 'Considering', 'Cyrene', 'The King', 'Further', 'Theagenes', 'Point Ichthys', 'Egypt Meanwhile Orestes', 'Eleusinian Demeter', 'Feast', 'War', 'Encamping', 'Hagnon', 'Thuriats', 'Aegina Above', 'Corcyra Still', 'Did', 'Euphamidas', 'Xanthippus', 'Phaleric Then', 'Pegae', 'This Teres', 'Carcinus', 'Touching', 'Attica The', 'Asopus', 'Sicily', 'Zeus Accordingly', 'Aenesias', 'Besides', 'Hellenes This', 'Aegina But', 'Miltiades Next', 'Caunus', 'Mendesian', 'Eurymachus', 'Abandoning', 'Hellespont', 'Peloponnese', 'Pharsalus', 'Median', 'Egypt Finding', 'Itys', 'Echecratidas', 'Upon', 'Whether', 'Brooks', 'Artabazus', 'Attic', 'Hellene', 'Lemnos', 'Pheia', 'The Mede', 'Euboea', 'Daskylion', 'Familiarity', 'Peloponnese With', 'Others', 'Ephesus After', 'Helots The', 'Tanagraeans', 'Timocrates', 'Sparta', 'For', 'Hearing', 'Corcyraean', 'Delay', 'Subsequently', 'Miletus Victory', 'Boeum', 'Athens But Athens', 'Athens With', 'Psammetichus', 'Cyprians', 'Individuals', 'Phoenicians', 'Strymon', 'Tereus', 'Acarnania Failing', 'Median War', 'First Invasion', 'Lesbians', 'Chians', 'Alarmed', 'Tolmaeus They', 'Thessalians', 'Megabuzus', 'Under Cecrops', 'Meanwhile Cylon', 'Eleans', 'Teres', 'Athens Meanwhile', 'Brazen House', 'Synoecia', 'Hellenic', 'Kitium', 'Accordingly Cylon', 'Beginning', 'Pythian', 'Thessaly They', 'Medism', 'Brilessus While', 'Lacedaemon', 'Make', 'Pharsalians', 'The Phaleric', 'Opuntian', 'Persians', 'Oenoe', 'Troezen', 'Pheraeans The Larisaean', 'Dolopian', 'While', 'Haliae', 'Our', 'Crete', 'Samos Their', 'Sailing', 'Byzantium', 'Knots', 'Corcyra', 'Fellow', 'Phocians', 'Egyptians', 'Pursued', 'Erineum They', 'Eleusis', 'Perdiccas', 'Pleistoanax', 'Thessaly', 'His', 'Callirhoe', 'Pandion', 'Besieged', 'However', 'For Pausanias', 'Destroying', 'Amyrtaeus', 'Nisaea The Megarians', 'Crissaean Gulf', 'Athenians', 'Attacked', 'Methone', 'Long Walls Meanwhile', 'Well', 'Salamis', 'Boeotarchs', 'Spartans', 'Olympian Zeus', 'Ionian', 'Phalerum', 'Preparations', 'Megabazus', 'Chios', 'Ithome', 'Megabates', 'Enneacrounos', 'Corinthians They', 'Making', 'Pausanias The Athenians', 'Again', 'Cephallenia', 'Manifold', 'Cropia', 'Lastly', 'Secret', 'Olympic', 'Dionysus', 'Chalcis', 'Pheia The', 'Gyrtonians', 'Euboea After', 'Your', 'Thriasian', 'Nine Pipes', 'Hellenes', 'Gongylus', 'Sardis', 'The Thebans', 'Samos', 'Athens This', 'Archidamus', 'Acarnanian Evarchus', 'Sitting', 'Foremost', 'Naxos', 'Lampsacus', 'Munychia', 'Marathon', 'Perceiving', 'Powers', 'Tolmaeus', 'Oropians', 'Tlepolemus', 'Eleusinians', 'Oenoe During', 'Athens The', 'Epidaurians', 'Asiatic Magnesia', 'Hellas The', 'Piraeus Meanwhile', 'Twice', 'Union', 'Scyros', 'The Athenians', 'Hellanicus', 'Epidaurians Meanwhile', 'Chrysis', 'From', 'Alope', 'They', 'Thrace', 'Ennea Hodoi', 'Achaean', 'Digging', 'Erechtheus', 'Orestes', 'Admetus', 'Funeral Oration', 'Phrygia', 'Achaeans', 'Attica This', 'Diasia', 'White Castle Within', 'Odrysians Again', 'Archidamus Even', 'Eion', 'Oropus', 'Orchomenus', 'Achaia', 'Athens Meanwhile Inaros', 'Long Walls', 'Melesippus', 'Samaeans', 'The Lacedaemonians', 'Geraneia', 'Egypt They', 'Thus', 'Helots', 'Carystus', 'Meanwhile', 'Samians', 'King Xerxes', 'Acarnania', 'Ancient', 'Gracious', 'Xenotimus', 'Mount Aegaleus', 'Cleopompus', 'Should', 'Hellas And', 'Thracian', 'Corcyraeans', 'Zeus', 'Sicyon', 'Cylon', 'Cilicians', 'Pythes', 'Pericles', 'Outside Peloponnese', 'Cimon The', 'There', 'Italy', 'Other', 'Brasidas', 'Chian', 'Olympia', 'Nay', 'Larisaeans', 'Acropolis This', 'Oracles', 'Zacynthians', 'Nymphodorus', 'Yet', 'Tragia', 'Opus', 'Agesander Not', 'Tellis', 'Meanwhile Pericles', 'Persian', 'Pharnaces', 'Peloponnese Such', 'Argos', 'Pericles\\n\\nThe', 'Corinth The Athenians', 'Dorkis', 'Slow', 'Pissuthnes', 'Eurymedon', 'Ionia', 'Diemporus', 'Still', 'Delphians Immediately', 'Athens Therefore', 'Clinias', 'You', 'Anticles', 'Pericles The Athenians', 'Now', 'Boeotia After', 'Mede', 'Cimon Sixty', 'Between', 'Egypt Arriving', 'Corinth', 'Oenophyta', 'Dorian', 'Thyrea', 'Phocis', 'Piraeus', 'Immediately', 'Perioeci', 'Eretrian', 'Mede Meanwhile Pausanias', 'Athens After', 'Deep', 'Beautiful', 'Euboean', 'Cranians', 'Pellenians', 'King', 'Palaira', 'Procne', 'After Pausanias', 'Troad', 'Places', 'With', 'Kitinium', 'Elis', 'Medes', 'Sicyonians', 'Treasurers', 'Thracians Teres', 'Acarnanian', 'Timoxenus', 'Zopyrus', 'Acharnae', 'Megarian', 'Thessalians Meanwhile', 'Myronides', 'Admitting', 'Corinthians', 'Hermione', 'Taenarus', 'Nisaea', 'Athens Not', 'CHAPTER', 'Except', 'Pelasgian', 'Milesians', 'Among', 'Euboea Such', 'Theban', 'Samian', 'BOOK', 'Rheiti', 'Nicomedes', 'Marshes', 'Pausanias For', 'Pythian Apollo', 'Xanthippus The Athenians', 'About', 'Confident', 'Suppose', 'Cyprus', 'Zeuxis', 'Attica', 'The Plataeans', 'Fairwater', 'Acropolis The Athenians', 'Released', 'Coasting', 'Dorians', 'Locris', 'Athens', 'Edonians', 'Naupactus', 'Phocians Some', 'Pythodorus', 'Aegina', 'Myos', 'Delos', 'Such', 'Aristonymus', 'Melos', 'Histiaeans', 'Thessalian', 'Mount Parnes', 'Peloponnesians', 'Libyan', 'Indeed', 'Any', 'None', 'Till', 'Peloponnesian', 'King Artaxerxes', 'Miltiades', 'Sixty', 'Messenians', 'Consider', 'Plataean', 'Pegae For', 'Doris', 'Prosopitis', 'Memphis', 'Amphipolis They', 'Thracians', 'Tanagra', 'Cranonians', 'Perdiccas Coming', 'Menon', 'Thebans', 'Athens Indeed', 'Then', 'Sitalces', 'Tolmides', 'Its', 'King Pleistoanax', 'Magnesia', 'Urged', 'Thrace Tereus', 'Chaeronea', 'Delphi', 'Phormio', 'Taenarus The Lacedaemonians', 'Leontiades', 'Leucas', 'Dismissing', 'King Pleistarchus', 'Opuntian Locrians', 'Taking', 'Thasos', 'Argolis', 'Ozolian Locrians The Athenians', 'Stroebus Upon', 'Ithome Most', 'The Byzantines', 'Poseidon', 'Three', 'Pausanias Matter', 'Isthmus', 'Locrians', 'Lacedaemonians', 'Zeal', 'Lacedaemonian', 'Lesbos After', 'Leucadians', 'Argilus', 'Pronaeans Not', 'Cyprus These', 'Athene', 'Egypt', 'Nisaea Atalanta', 'Cleomenes', 'Macedonians', 'Apart', 'Their', 'Egypt After', 'Out', 'Cecruphalia', 'Eumolpus', 'Pydna', 'Farmers', 'Pythangelus', 'Peloponnese And', 'Acarnanians', 'Caria', 'Cimon', 'Peloponnese After', 'Thasians', 'Socrates', 'Acharnians', 'Antigenes This', 'Xerxes', 'Histiaea', 'Hellas', 'Laconia', 'Zacynthus', 'Daulian', 'Thucydides', 'Myronides After', 'Now Plataea', 'Even', 'Megara Now', 'Arrived', 'Sollium', 'Earth', 'Leocrates', 'Dionysia', 'God', 'Themistocles', 'Phoenician', 'Plataeans', 'Kaiadas', 'Athens Personally', 'Anthesterion', 'Acropolis', 'Accordingly', 'Phyleides', 'Boeotia', 'Graea', 'Athens Pericles', 'Peloponnesian War', 'Libya', 'Aethaeans', 'Disease', 'Corinthians The Lacedaemonians', 'Sadocus', 'Xanthippus Landing', 'Thebes', 'Full', 'Attica Without', 'Pausanias', 'Drabescus', 'Not', 'Cleombrotus', 'Diacritus', 'Theseus', 'Priene Worsted', 'Thebes For Plataea', 'Capital', 'Coronea', 'Pharsalian', 'Daulis', 'For Themistocles', 'Lesbos', 'Pyrasians', 'Megara', 'Athens Towards', 'Egyptian', 'Therme', 'Onetorides', 'Odrysians', 'Phocis They']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:54:13,329 - INFO - Found entities in chunk 2: ['Themistocles', 'Piraeus', 'Athenians', 'For', 'Between']...\n",
      "2025-05-29 08:54:13,329 - INFO - Enriched 1 locations with coordinates\n",
      "2025-05-29 08:54:13,541 - INFO - Retrieved 0 facts from DBpedia for 'Mede Meanwhile Pausanias'\n",
      "2025-05-29 08:54:13,743 - INFO - Retrieved 0 facts from Wikidata for 'Mede Meanwhile Pausanias'\n",
      "2025-05-29 08:54:25,307 - INFO - KG retrieval completed: 4/4 successful, 32 cache hits\n",
      "2025-05-29 08:54:42,683 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 08:54:42,688 - INFO - Generated RAG-enhanced RDF for chunk 2 using Claude 4\n",
      "2025-05-29 08:54:43,701 - INFO - Processing chunk 3 (115922 chars) - CLAUDE 4 WITH RAG\n",
      "2025-05-29 08:54:43,859 - INFO - Retrieved 7 relevant chunks via RAG for chunk 3\n",
      "2025-05-29 08:54:43,867 - INFO - Found potential locations in chunk 3: ['Opposite', 'Paravaeans', 'Chalcidians The', 'Athens There', 'Amphiraus', 'Discovering', 'Anactorians', 'The Athenian', 'Limnaea', 'Treres', 'Pieria', 'Megarians', 'Europe', 'Lesbian', 'Amphilochian Argos', 'Athenians The', 'Spardacus', 'Afterwards', 'Bottiaea', 'Each', 'Mygdonia', 'Molossians', 'Hatred', 'Acarnanians The', 'Ambraciots', 'Photys', 'Alexander', 'Amyntas', 'Astacus', 'This Seuthes', 'Bottice', 'Strong', 'Phagres', 'Numberless', 'Maedians', 'Axius', 'Patrae', 'Almopians', 'Athenian', 'Molycrian Rhium', 'Crestonia', 'Achaea', 'Rhia', 'Olynthus', 'Thermopylae', 'Plataea', 'One', 'Lesbos However', 'Tegean', 'Oeniadae', 'Geraestus', 'Neither', 'Meanwhile Phormio', 'Amphilochia', 'Corinth Passing', 'Gortys', 'Boeotians', 'Argives', 'Oroedus', 'Spartan', 'Pella', 'Macedonia', 'Further', 'War', 'Olympiad', 'Nicias', 'Hagnon', 'Archelaus', 'Steersmen', 'Pericles When', 'Mount Pindus', 'Oroedus There', 'Corinthian', 'Isocrates', 'Passing', 'Spartolus', 'Euxine The', 'Attica The', 'Laeaean Paeonians', 'Polichnitans', 'Sicily', 'Ameiniades', 'Callimachus', 'Against', 'Besides', 'Mitylenians', 'Almopia These Macedonians', 'Athens Their', 'Timagoras', 'Hellespont', 'Cyrus', 'Cyrrhus', 'Peloponnese', 'These Macedonians', 'Asia', 'Upon', 'Stratodemus', 'Lycia', 'Athenians Besides', 'Odomanti', 'Lemnos', 'Others', 'Euboea', 'Timocrates', 'Echinades', 'Philemon', 'Agatharchidas The Peloponnesians', 'For', 'Achelous', 'Sparta', 'Men', 'Phormio The Peloponnesians', 'Halieis', 'For Athens', 'Pangaeus', 'Agraeans', 'Strymon', 'Odrysians', 'Median War', 'Evenus', 'Doberus', 'Lesbians', 'Chians', 'Twenty', 'Malean Apollo', 'Lower Macedonia', 'Thessalians', 'This Argos', 'Dii', 'Teres', 'Pontus', 'The Peloponnesians', 'Scythians', 'Hellenic', 'Beginning', 'The Plataean', 'Supplications', 'Piraeus There', 'Policy', 'Bordering', 'Lacedaemon', 'Make', 'Nestus', 'Callimachus The', 'Leucas The Corinthians', 'Nor', 'Remember', 'Troezen', 'Messenian', 'Achaean Rhium', 'Owing', 'While', 'Athenians Their', 'Our', 'Crete', 'Advancing', 'Byzantium', 'The Chalcidians', 'Dyme', 'Fourth', 'Perseverance', 'Amphilochians', 'Athenians The Lesbians', 'Sabylinthus', 'Sicilians', 'Tilataeans', 'Coronta', 'Stopped', 'Paeonians', 'Perdiccas', 'Chalcidice', 'His', 'King Archidamus', 'Confidence', 'However', 'The Hellenes', 'Crissaean Gulf', 'Athenians', 'The Mitylenians', 'Cynes', 'Cithaeron', 'Salamis', 'Ambracia', 'Olympian Zeus', 'Fear', 'Dolopia', 'Apollo', 'Messenians After', 'Meanwhile Sitalces', 'Again', 'Lemnians', 'Cephallenia', 'Perplexed', 'Chalcidians', 'Phanomachus', 'Turning', 'Euripides', 'Revolt', 'Position', 'Chalcis', 'Bradidas', 'Mitylene', 'Hebrus', 'Droi', 'Rhodope The', 'Your', 'Thrace These', 'Ambraciot', 'Hellenes', 'Corinth Not', 'Bottia', 'Europus', 'Methymna', 'Archidamus', 'Winter', 'Since', 'Magnetes', 'Aneristus', 'Stratonice', 'Rather', 'Archidamus The Plataeans', 'Asopius', 'Eleian', 'Peloponnese Indeed', 'Athens The', 'Ethiopia', 'The Athenians', 'Andros But Pericles', 'King Antichus', 'From', 'The Peloponnesian', 'Euxine', 'Gortynia', 'They', 'Naval Victories', 'Thrace', 'Rhodian Dorieus', 'Achaeans', 'CHAPTER VII\\nSecond Year', 'Ambracian Gulf', 'CHAPTER VIII\\nThird Year', 'Paeonia', 'Trust', 'Alcidas', 'Fort Budorum', 'Liberator', 'Crissaean Gulf Six', 'Philip', 'Hermaeondas', 'Rhodope', 'The Acarnanians', 'Thus', 'Abdera', 'Panormus', 'Meanwhile', 'Oskius This', 'Cyllene', 'Malea', 'Acarnania', 'Bisaltia', 'Settling', 'Tenedians', 'Mount Rhodope', 'Cleopompus', 'Athenians You', 'Crissaean', 'Laeaeans', 'Should', 'Thracian', 'Zeus', 'Deinias', 'Idomene', 'Sicyon', 'Xenophon', 'Pericles', 'Dersaeans', 'There', 'Acarnan Such', 'Unable', 'Pierians', 'Getae The', 'Brasidas', 'Chian', 'Olympia', 'Stratus', 'Corinthian Aristeus', 'Yet', 'Persian', 'Stratians', 'Methymnians', 'Argos', 'Theolytus', 'Imbrians', 'Fall', 'Corinth The Athenians', 'Nicolaus', 'Hestiodorus', 'Epidaurus', 'Achaea The Athenians', 'Assembling', 'Elimiots', 'Phaselis', 'Still', 'Arcturus', 'Clinias', 'You', 'Amphilochian', 'Megara This', 'Now', 'Amphilochus', 'Mede', 'Between', 'Sinking', 'Corinth', 'Dorian', 'Elated', 'Piraeus', 'Immediately', 'Cercine', 'Great', 'Cnemus', 'Justice', 'King', 'Inland', 'Nicanor', 'Athenians The Chalcidians', 'Cretan', 'With', 'Elis', 'Trojan War', 'Naupactus The Lacedaemonians', 'Medes', 'Haemus', 'Naupactus For', 'Acarnanian', 'Amphiaraus Dissatisfied', 'Chalcidian', 'Aristeus', 'Corinthians', 'Sintians', 'Hermione', 'Nisaea', 'Among', 'Theban', 'Bottiaeans', 'Phoenicia', 'About', 'Triballi', 'Anactorium', 'Pollis', 'Attica', 'Putting', 'The Plataeans', 'Anapus', 'Athens', 'Homer', 'Naupactus', 'Sitalces\\n\\nThe', 'Lacedaemonian Timocrates', 'Such', 'This Rhium', 'Laurium', 'Failing', 'Provisions', 'Attica During', 'Preparation', 'The Hellenic', 'Paralian', 'Paeonian', 'Peloponnesians', 'Indeed', 'Graciously', 'Rhium', 'Laconian', 'Alcmaeon', 'Crusis', 'Peloponnesian', 'Messenians', 'Physca', 'Whenever', 'Plataean', 'Armed', 'Machaon', 'Thracians', 'Meleas', 'Naupactus Thus', 'Thebans', 'Athens Indeed', 'Sitalces', 'Mitylene\\n\\nThe', 'Peloponnesians During', 'Thracian Irruption', 'Mitylenians After', 'Under', 'Leucadian', 'The Plague', 'Molycrium', 'Pierian Gulf', 'King But', 'Zeuxidamus', 'Melesander', 'Ionian Gulf', 'Phormio', 'Plataeans Others', 'Mounts Haemus', 'Leucas', 'Cydonia', 'Argos This', 'Externally', 'Anthemus', 'Atalanta', 'Chaonians', 'Agrianes', 'Temenids', 'Poseidon', 'Crete For Nicias', 'Isthmus', 'Lacedaemonians', 'Lacedaemonian', 'Had', 'Leucadians', 'Comfort', 'Cydonians', 'Piraeus The Peloponnesians', 'Egypt', 'Orestians', 'Panaeans', 'Arriving', 'BOOK III\\nCHAPTER', 'Athenian And', 'Macedonians', 'Their', 'Born', 'Aristocleides', 'Acarnanians', 'Anthemus The Macedonians', 'Caria', 'Nericus', 'Mount Scombrus', 'Fifth Years', 'More', 'Cleippides', 'Hellas', 'Laconia', 'Zacynthus', 'Thucydides', 'Athens The Mitylenians', 'According', 'Naupactus The Athenians', 'Arrived', 'Thesprotians', 'Seuthes', 'Suddenly', 'Plataeans', 'Edonians From Eordia', 'Sicilian', 'Eordians', 'Learchus', 'Libya', 'Odrysian', 'Disease', 'Sadocus', 'Sitalces Laying', 'Alcmaeon The Athenians', 'The Chalcidian', 'Pausanias', 'Respect', 'Not', 'Cleombrotus', 'Lycophron', 'Pentacosiomedimni', 'Plataea The', 'Stratus The Lacedaemonians', 'King Tharyps', 'Cease', 'Lesbos', 'Mount Pangaeus', 'Megara', 'Danube', 'Prasiai', 'Pharnabazus', 'Investment', 'Getae', 'Lyncestae', 'Atintanians']\n",
      "2025-05-29 08:54:43,867 - INFO - Found entities in chunk 3: ['There', 'Against', 'Further', 'Athenian', 'Athens']...\n",
      "2025-05-29 08:54:43,867 - INFO - Enriched 0 locations with coordinates\n",
      "2025-05-29 08:54:43,868 - INFO - KG retrieval completed: 0/0 successful, 36 cache hits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 3/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:55:01,340 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 08:55:01,347 - INFO - Generated RAG-enhanced RDF for chunk 3 using Claude 4\n",
      "2025-05-29 08:55:02,356 - INFO - Processing chunk 4 (47590 chars) - CLAUDE 4 WITH RAG\n",
      "2025-05-29 08:55:02,447 - INFO - Retrieved 7 relevant chunks via RAG for chunk 4\n",
      "2025-05-29 08:55:02,451 - INFO - Found potential locations in chunk 4: ['Antissians', 'Ionia From Ephesus Alcidas', 'Chians', 'Fifth Year', 'Notium', 'Elean', 'Delos', 'Wine', 'Such', 'Athens Such', 'Megarians', 'The Peloponnesians', 'Hellas Although', 'Hellenic', 'Athenians The', 'Alcidas', 'Execution', 'Icarus', 'Afterwards', 'Myus', 'Paralian', 'Niceratus', 'Artemisium', 'Peloponnesians', 'Indeed', 'Arcadians', 'Plain', 'Ladders', 'Helots', 'Peloponnesian', 'Lacedaemon', 'Make', 'Meanwhile', 'Consider', 'Plataean', 'Patmos', 'Teutiaplus', 'Samians', 'Peloponnese Meanwhile', 'Androcrates', 'Mitylenians Their', 'Mitylenian', 'Thebans', 'Diodotus', 'Tolmides', 'Eresus', 'Our', 'Mitylene The', 'Crossing', 'Cyme', 'Athenian', 'Meander', 'Anaia', 'Hope', 'Plataea', 'Teian', 'Pleistoanax', 'Lacon', 'Daimachus', 'Let', 'Cleaenetus', 'Colophonians', 'Lysicles', 'Erythraeid', 'Boeotians', 'Fortune', 'However', 'Luckily', 'Methymnians', 'The Lacedaemonian', 'Athenians', 'Hippias', 'Theaenetus', 'Lacedaemonians', 'The Mitylenians', 'Lacedaemonian', 'Cleon', 'Confess', 'Were', 'Cithaeron', 'Salamis', 'Pissuthnes', 'War', 'Salaethus', 'Arcadian', 'Nicias', 'Ionian', 'Epicurus', 'Clarus', 'Mitylene Going', 'Myconus', 'Ionia', 'Myonnesus', 'Still', 'Cleomenes', 'Their', 'Megara Accordingly', 'Starting', 'King Pausanias', 'Salaminian', 'Ithome', 'Fire', 'Now', 'Diodotus The', 'Mede', 'Again', 'Towards', 'Antissa', 'Budorum', 'Mitylene', 'Coroebus', 'Minoa', 'Caria', 'Attica However', 'Ephesus', 'Colophon', 'Astymachus', 'Hellas', 'Besides', 'Hellenes', 'Thucydides', 'Mitylenians', 'With', 'Arrived', 'Trial', 'Peloponnese', 'Plataeans', 'Tenedos', 'Eucrates', 'Either', 'Median', 'Though', 'Accordingly', 'Another', 'Upon', 'Carians', 'Eupompides', 'Mitylenians The', 'Cleon After', 'Ammias', 'Nevertheless', 'Druoskephalai', 'Nisaea', 'Mitylene Fears', 'Here', 'Thebes', 'Sandius', 'Hysiae', 'For', 'Paches', 'Not', 'Pausanias', 'Sparta', 'Corcyraean Revolution\\n\\nDuring', 'Theban', 'Mitylene Wishing', 'Aeimnestus', 'Asopolaus', 'Oakheads After', 'Only', 'Ionia This', 'About', 'Compassion', 'The Athenians', 'Aeolic', 'Punish', 'Embatum', 'Itamenes', 'Lesbos CHAPTER', 'Attica', 'Lesbos', 'Word', 'They', 'The Plataeans', 'Pyrrha', 'Lesbians', 'Megara', 'Athens', 'Erythrae']\n",
      "2025-05-29 08:55:02,451 - INFO - Found entities in chunk 4: ['Antissa', 'Pyrrha', 'Eresus', 'Methymnians', 'Antissians']...\n",
      "2025-05-29 08:55:02,451 - INFO - Enriched 0 locations with coordinates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 4/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:55:02,662 - INFO - Retrieved 0 facts from DBpedia for 'Methymnians'\n",
      "2025-05-29 08:55:02,725 - INFO - Retrieved 0 facts from Wikidata for 'Methymnians'\n",
      "2025-05-29 08:55:03,888 - INFO - Retrieved 0 facts from Wikidata for 'Antissians'\n",
      "2025-05-29 08:55:04,112 - INFO - Retrieved 0 facts from DBpedia for 'Antissians'\n",
      "2025-05-29 08:55:11,325 - INFO - Retrieved 0 facts from Wikidata for 'Mitylenians'\n",
      "2025-05-29 08:55:11,544 - INFO - Retrieved 0 facts from DBpedia for 'Mitylenians'\n",
      "2025-05-29 08:55:29,531 - INFO - KG retrieval completed: 11/11 successful, 25 cache hits\n",
      "2025-05-29 08:55:40,648 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 08:55:40,651 - INFO - Generated RAG-enhanced RDF for chunk 4 using Claude 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved enhanced RDF to extracted_events_rag_with_multi_kg_claude2.ttl\n",
      "üìä Processing Statistics (Claude 4 with RAG):\n",
      "   - Total chunks processed: 4\n",
      "   - Successful chunks: 4\n",
      "   - Unique entities found: 44\n",
      "   - Total KG facts retrieved: 0\n",
      "   - Cache hits: 111\n",
      "   - Locations found: 2\n",
      "   - Locations with coordinates: 2\n",
      "   - Location duplicates avoided: 1\n",
      "   - Unique global locations: 1\n",
      "   - RAG queries for RDF generation: 4\n",
      "   - LLM used: Claude 4 (Anthropic)\n",
      "\n",
      "üîó Knowledge Graph Connector Statistics:\n",
      "   - Wikidata: 6/10 requests (60.0% success)\n",
      "   - DBpedia: 5/9 requests (55.6% success)\n",
      "   - ConceptNet: 0/14 requests (0.0% success)\n",
      "   - Location enrichment: 55/223 locations enriched (24.7% success)\n",
      "\n",
      "üìù Sample of generated RDF:\n",
      "============================================================\n",
      "@prefix ste: <http://www.example.org/ste#> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
      "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
      "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
      "\n",
      "# Historical Events with RAG-Enhanced Embedded Location Data (CLAUDE 4)\n",
      "ste:Event1 a ste:Event, dbp:HistoricalPublication ;\n",
      "    ste:hasType \"Publication of History of Peloponnesian War\" ;\n",
      "    ste:hasAgent \"Thucydides\" ;\n",
      "    ste:hasTime \"431 BC\" ;\n",
      "    ste:hasLocation \"Athens\" ;\n",
      "    ste:hasResult \"Creation of historical account of Peloponnesian War\" ;\n",
      "    ste:hasRAGContext \"yes\" ;\n",
      "    ste:hasLLM \"Claude4\" .\n",
      "\n",
      "ste:Event2 a ste:Event, dbp:Translation ;\n",
      "    ste:hasType \"English translation of Peloponnesian War history\" ;\n",
      "    ste:hasAgent \"Richard Crawley\" ;\n",
      "    ste:hasTime \"2003-03-15\" ;\n",
      "    ste:hasLocation \"Project Gutenberg\" ;\n",
      "    ...\n",
      "============================================================\n",
      "\n",
      "ü§ñ Claude 4 RAG System Ready!\n",
      "\n",
      "üí° Try asking questions like:\n",
      "   - 'What battles happened in Sicily?'\n",
      "   - 'Who were the main leaders mentioned?'\n",
      "   - 'What events occurred in 415 BC?'\n",
      "   - 'Describe the naval engagements'\n",
      "   - 'What was the outcome of the siege?'\n",
      "\n",
      "‚ùì Start interactive Claude 4 RAG session? (y/n): n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:57:28,954 - INFO - Processing RAG query with Claude 4: 'What are the main events mentioned in the text?...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° You can also query programmatically:\n",
      "   result = multi_kg_system.rag_query('your question', claude_client)\n",
      "\n",
      "üîç Running sample queries with Claude 4:\n",
      "\n",
      "‚ùì Sample query: 'What are the main events mentioned in the text?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:57:29,273 - INFO - KG retrieval completed: 0/0 successful, 0 cache hits\n",
      "2025-05-29 08:57:36,168 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 08:57:36,172 - INFO - Processing RAG query with Claude 4: 'Which locations are mentioned?...'\n",
      "2025-05-29 08:57:36,224 - INFO - KG retrieval completed: 0/0 successful, 3 cache hits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Answer: Based on the provided text chunks, several significant events are mentioned:\n",
      "\n",
      "1. Political/Military Events:\n",
      "- The affairs of Corcyra and Potidaea, which served as pretexts for \"the present war\" (likel...\n",
      "üìä Sources: 3 chunks, 0 KG facts\n",
      "ü§ñ LLM: Claude 4\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Sample query: 'Which locations are mentioned?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:57:41,520 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 08:57:41,525 - INFO - Processing RAG query with Claude 4: 'Who are the key people involved?...'\n",
      "2025-05-29 08:57:41,605 - INFO - KG retrieval completed: 0/0 successful, 0 cache hits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Answer: Based on the provided text chunks, these locations are specifically mentioned:\n",
      "\n",
      "1. Thracian towns (referenced in relation to military action)\n",
      "2. Peloponnesian region (implied by mention of \"Peloponnes...\n",
      "üìä Sources: 3 chunks, 9 KG facts\n",
      "ü§ñ LLM: Claude 4\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Sample query: 'Who are the key people involved?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 08:57:48,674 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Answer: From the provided text chunks, there are very few specific individuals explicitly named or identified. The text appears to be discussing groups of people rather than specific historical figures. Here'...\n",
      "üìä Sources: 3 chunks, 0 KG facts\n",
      "ü§ñ LLM: Claude 4\n",
      "----------------------------------------\n",
      "\n",
      "üéâ Process complete! Check extracted_events_rag_with_multi_kg_claude2.ttl for RDF results.\n",
      "üìä System ran with Claude 4 + RAG + Knowledge Graph + Location enrichment.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Multi-Knowledge Graph RAG System with Text Chunking - CLAUDE 4 VERSION\n",
    "Handles large texts by processing them in chunks to avoid token limits\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal\n",
    "from rdflib.namespace import XSD, SKOS\n",
    "\n",
    "# Configuration\n",
    "INPUT_TEXT_FILE = \"part_aa\"\n",
    "ONTOLOGY_PATH = \"wiki.owl\"\n",
    "LOCATION_ONTOLOGY_PATH = \"locations.owl\"\n",
    "OUTPUT_RAG_TTL = 'extracted_events_rag_with_multi_kg_claude2.ttl'  # CHANGED: Different output file\n",
    "OUTPUT_RAG_OWL = 'extracted_events_rag_with_multi_kg_claude2.owl'  # CHANGED: Different output file\n",
    "KG_CACHE_FILE = 'kg_cache.json'\n",
    "LOCATION_CACHE_FILE = 'location_cache.json'\n",
    "KG_ANALYSIS_REPORT = 'multi_kg_analysis_report.txt'\n",
    "\n",
    "# Token limits - UPDATED FOR CLAUDE 4\n",
    "MAX_TOKENS_PER_REQUEST = 150000  # Claude 4 has higher token limits\n",
    "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
    "\n",
    "# Namespaces\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "STE = Namespace(\"http://www.example.org/ste#\")\n",
    "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
    "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
    "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
    "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
    "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
    "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Imports - UPDATED FOR CLAUDE\n",
    "try:\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    # CHANGED: Import Anthropic instead of OpenAI\n",
    "    import anthropic\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    print(\"pip install rdflib python-dotenv anthropic langchain langchain-community faiss-cpu sentence-transformers tiktoken requests\")\n",
    "    exit(1)\n",
    "\n",
    "@dataclass\n",
    "class LocationInfo:\n",
    "    \"\"\"Location information with coordinates\"\"\"\n",
    "    name: str\n",
    "    latitude: Optional[float] = None\n",
    "    longitude: Optional[float] = None\n",
    "    country: Optional[str] = None\n",
    "    region: Optional[str] = None\n",
    "    source: str = \"extracted\"\n",
    "    confidence: float = 1.0\n",
    "    uri: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class EnhancedKnowledgeFact:\n",
    "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    source: str\n",
    "    confidence: float = 1.0\n",
    "    context: Optional[str] = None\n",
    "    temporal: Optional[str] = None\n",
    "    spatial: Optional[str] = None\n",
    "    evidence_score: float = 1.0\n",
    "    source_uri: Optional[str] = None\n",
    "\n",
    "class LocationExtractor:\n",
    "    \"\"\"Extracts and enriches location information\"\"\"\n",
    "    \n",
    "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
    "        self.ontology_path = ontology_path\n",
    "        self.location_graph = None\n",
    "        self.location_cache = self._load_location_cache()\n",
    "        self.load_location_ontology()\n",
    "        \n",
    "    def _load_location_cache(self) -> Dict:\n",
    "        \"\"\"Load location cache\"\"\"\n",
    "        if os.path.exists(LOCATION_CACHE_FILE):\n",
    "            try:\n",
    "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load location cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_location_cache(self):\n",
    "        \"\"\"Save location cache\"\"\"\n",
    "        try:\n",
    "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save location cache: {e}\")\n",
    "    \n",
    "    def load_location_ontology(self):\n",
    "        \"\"\"Load locations.owl ontology\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.ontology_path):\n",
    "                self.location_graph = Graph()\n",
    "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
    "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
    "            else:\n",
    "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
    "                self.location_graph = None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading location ontology: {e}\")\n",
    "            self.location_graph = None\n",
    "    \n",
    "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract potential location names from text\"\"\"\n",
    "        location_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
    "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
    "        ]\n",
    "        \n",
    "        locations = []\n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            locations.extend(matches)\n",
    "        \n",
    "        location_stopwords = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', \n",
    "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
    "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', \n",
    "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday', \n",
    "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "        }\n",
    "        \n",
    "        filtered_locations = []\n",
    "        for loc in locations:\n",
    "            loc = loc.strip()\n",
    "            if (loc not in location_stopwords and len(loc) > 2 and \n",
    "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
    "                filtered_locations.append(loc)\n",
    "        \n",
    "        return list(set(filtered_locations))\n",
    "    \n",
    "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location info from local ontology\"\"\"\n",
    "        if not self.location_graph:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
    "                ?location rdfs:label ?label .\n",
    "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
    "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
    "                OPTIONAL {{ ?location geo:long ?long }}\n",
    "                OPTIONAL {{ ?location dbp:country ?country }}\n",
    "                OPTIONAL {{ ?location dbp:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            results = self.location_graph.query(query)\n",
    "            for row in results:\n",
    "                return LocationInfo(\n",
    "                    name=location_name,\n",
    "                    latitude=float(row.lat) if row.lat else None,\n",
    "                    longitude=float(row.long) if row.long else None,\n",
    "                    country=str(row.country) if row.country else None,\n",
    "                    region=str(row.region) if row.region else None,\n",
    "                    source=\"local_ontology\",\n",
    "                    uri=str(row.location) if row.location else None\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
    "                <{entity_uri}> geo:lat ?lat ;\n",
    "                               geo:long ?long .\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                bindings = data.get('results', {}).get('bindings', [])\n",
    "                \n",
    "                if bindings:\n",
    "                    binding = bindings[0]\n",
    "                    return LocationInfo(\n",
    "                        name=location_name,\n",
    "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
    "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
    "                        country=binding.get('country', {}).get('value', ''),\n",
    "                        region=binding.get('region', {}).get('value', ''),\n",
    "                        source=\"dbpedia\",\n",
    "                        uri=entity_uri\n",
    "                    )\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Try multiple query strategies to get the right location\n",
    "            queries = [\n",
    "                # Try exact label match first\n",
    "                f\"\"\"\n",
    "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
    "                  ?item rdfs:label \"{location_name}\"@en .\n",
    "                  ?item wdt:P625 ?coord .\n",
    "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
    "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}\n",
    "                LIMIT 5\n",
    "                \"\"\",\n",
    "                # Try with additional filters for places/locations\n",
    "                f\"\"\"\n",
    "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
    "                  ?item rdfs:label \"{location_name}\"@en .\n",
    "                  ?item wdt:P625 ?coord .\n",
    "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
    "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
    "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}\n",
    "                LIMIT 5\n",
    "                \"\"\"\n",
    "            ]\n",
    "            \n",
    "            for query in queries:\n",
    "                params = {'query': query, 'format': 'json'}\n",
    "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    bindings = data.get('results', {}).get('bindings', [])\n",
    "                    \n",
    "                    if bindings:\n",
    "                        # Prefer results with country information\n",
    "                        best_binding = None\n",
    "                        for binding in bindings:\n",
    "                            if binding.get('country'):\n",
    "                                best_binding = binding\n",
    "                                break\n",
    "                        \n",
    "                        if not best_binding:\n",
    "                            best_binding = bindings[0]\n",
    "                        \n",
    "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
    "                        \n",
    "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
    "                        if coord_match:\n",
    "                            longitude = float(coord_match.group(1))\n",
    "                            latitude = float(coord_match.group(2))\n",
    "                            \n",
    "                            return LocationInfo(\n",
    "                                name=location_name,\n",
    "                                latitude=latitude,\n",
    "                                longitude=longitude,\n",
    "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
    "                                source=\"wikidata\",\n",
    "                                uri=best_binding.get('item', {}).get('value', '')\n",
    "                            )\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
    "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
    "        if not location_info.latitude or not location_info.longitude:\n",
    "            return True\n",
    "        \n",
    "        lat, lon = location_info.latitude, location_info.longitude\n",
    "        \n",
    "        # Basic coordinate range validation\n",
    "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
    "            return False\n",
    "        \n",
    "        # Generic geographic validation - flag obviously wrong coordinates\n",
    "        # If coordinates suggest North America but no clear indication it should be there\n",
    "        if (-130 < lon < -60) and (25 < lat < 50):  # North America range\n",
    "            logger.warning(f\"Coordinates for '{location_info.name}' appear to be in North America ({lat}, {lon}). \"\n",
    "                         f\"Please verify if this is correct for your historical context.\")\n",
    "            # Don't auto-correct, just warn - let the user/context decide\n",
    "        \n",
    "        # If coordinates suggest Australia/Oceania for what might be European/Mediterranean names\n",
    "        elif (110 < lon < 180) and (-45 < lat < -10):  # Australia/Oceania range\n",
    "            logger.warning(f\"Coordinates for '{location_info.name}' appear to be in Australia/Oceania ({lat}, {lon}). \"\n",
    "                         f\"Please verify if this is correct for your historical context.\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
    "        if location_name in self.location_cache:\n",
    "            cached = self.location_cache[location_name]\n",
    "            return LocationInfo(**cached) if cached else None\n",
    "        \n",
    "        location_info = None\n",
    "        \n",
    "        location_info = self.get_location_from_ontology(location_name)\n",
    "        \n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_wikidata(location_name)\n",
    "        \n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_dbpedia(location_name)\n",
    "        \n",
    "        if location_info:\n",
    "            self.location_cache[location_name] = {\n",
    "                'name': location_info.name,\n",
    "                'latitude': location_info.latitude,\n",
    "                'longitude': location_info.longitude,\n",
    "                'country': location_info.country,\n",
    "                'region': location_info.region,\n",
    "                'source': location_info.source,\n",
    "                'confidence': location_info.confidence,\n",
    "                'uri': location_info.uri\n",
    "            }\n",
    "        else:\n",
    "            self.location_cache[location_name] = None\n",
    "        \n",
    "        self._save_location_cache()\n",
    "        \n",
    "        if location_info:\n",
    "            self.validate_coordinates(location_info)\n",
    "        \n",
    "        return location_info\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"Handles text chunking to manage token limits\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"claude-3-5-sonnet-20241022\"):  # CHANGED: Default to Claude model\n",
    "        # Use GPT tokenizer as approximation for Claude tokens\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text (approximation for Claude)\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 25000) -> List[str]:  # CHANGED: Increased for Claude\n",
    "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class BaseKGConnector:\n",
    "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
    "        self.name = name\n",
    "        self.base_url = base_url\n",
    "        self.rate_limit = rate_limit\n",
    "        self.last_request_time = 0\n",
    "        self.request_count = 0\n",
    "        self.success_count = 0\n",
    "        \n",
    "    def _rate_limit_wait(self):\n",
    "        \"\"\"Enforce rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit:\n",
    "            time.sleep(self.rate_limit - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get connector statistics\"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'requests': self.request_count,\n",
    "            'successes': self.success_count,\n",
    "            'success_rate': self.success_count / max(1, self.request_count)\n",
    "        }\n",
    "    \n",
    "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class EnhancedWikidataConnector(BaseKGConnector):\n",
    "    \"\"\"Wikidata connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
    "              {{\n",
    "                ?subject ?label \"{entity}\"@en .\n",
    "              }} UNION {{\n",
    "                ?subject rdfs:label \"{entity}\"@en .\n",
    "              }}\n",
    "              \n",
    "              ?subject ?predicate ?object .\n",
    "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
    "              \n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
    "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
    "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
    "                        source=self.name,\n",
    "                        confidence=0.9,\n",
    "                        source_uri=binding.get('subject', {}).get('value')\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
    "                return facts\n",
    "            else:\n",
    "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedDBpediaConnector(BaseKGConnector):\n",
    "    \"\"\"DBpedia connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
    "              <{entity_uri}> ?predicate ?object .\n",
    "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
    "              FILTER(!isBlank(?object))\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    predicate = binding.get('predicate', {}).get('value', '')\n",
    "                    obj = binding.get('object', {}).get('value', '')\n",
    "                    \n",
    "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
    "                    \n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=entity,\n",
    "                        predicate=predicate_name,\n",
    "                        object=obj,\n",
    "                        source=self.name,\n",
    "                        confidence=0.85,\n",
    "                        source_uri=entity_uri\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
    "                return facts\n",
    "            else:\n",
    "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedConceptNetConnector(BaseKGConnector):\n",
    "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
    "        \n",
    "    def search_related_concepts(self, entity: str) -> List[str]:\n",
    "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
    "        try:\n",
    "            # Try search API first\n",
    "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
    "            response = requests.get(search_url, timeout=10)\n",
    "            \n",
    "            related_concepts = []\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                for edge in data.get('edges', []):\n",
    "                    start = edge.get('start', {}).get('label', '')\n",
    "                    end = edge.get('end', {}).get('label', '')\n",
    "                    \n",
    "                    # Extract concept paths and clean them\n",
    "                    for concept_path in [start, end]:\n",
    "                        if concept_path and '/c/en/' in concept_path:\n",
    "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
    "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
    "                                related_concepts.append(concept)\n",
    "            \n",
    "            return list(set(related_concepts))[:5]  # Return top 5 unique concepts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
    "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
    "        try:\n",
    "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
    "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data.get('edges', [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            all_facts = []\n",
    "            \n",
    "            # Strategy 1: Try direct query first\n",
    "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
    "            \n",
    "            # Strategy 2: Search for related concepts and query them\n",
    "            related_concepts = self.search_related_concepts(entity)\n",
    "            \n",
    "            # Process direct edges\n",
    "            for edge in direct_edges:\n",
    "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
    "                if fact:\n",
    "                    all_facts.append(fact)\n",
    "            \n",
    "            # Process related concept edges\n",
    "            for concept in related_concepts:\n",
    "                concept_edges = self.query_concept_directly(concept, 5)\n",
    "                for edge in concept_edges:\n",
    "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
    "                    if fact:\n",
    "                        all_facts.append(fact)\n",
    "            \n",
    "            if all_facts:\n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
    "                if related_concepts:\n",
    "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
    "            \n",
    "            return all_facts[:limit]\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
    "        try:\n",
    "            start = edge.get('start', {})\n",
    "            end = edge.get('end', {})\n",
    "            relation = edge.get('rel', {})\n",
    "            weight = edge.get('weight', 1.0)\n",
    "            \n",
    "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "            rel_label = relation.get('label', 'related_to')\n",
    "            \n",
    "            # Skip if labels are empty or too short\n",
    "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
    "                return None\n",
    "            \n",
    "            # Determine confidence based on discovery method\n",
    "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
    "            \n",
    "            return EnhancedKnowledgeFact(\n",
    "                subject=original_entity,\n",
    "                predicate=rel_label,\n",
    "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
    "                source=self.name,\n",
    "                confidence=min(weight * confidence_multiplier, 1.0),\n",
    "                context=f\"Discovered {discovery_method}\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
    "            return None\n",
    "\n",
    "class MultiKGCache:\n",
    "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "        \n",
    "    def _load_cache(self) -> Dict:\n",
    "        \"\"\"Load cache from file\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save cache: {e}\")\n",
    "    \n",
    "    def get_cache_key(self, source: str, entity: str) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
    "    \n",
    "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Get cached facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
    "        \"\"\"Cache facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        serializable_facts = []\n",
    "        for fact in facts:\n",
    "            serializable_facts.append({\n",
    "                'subject': fact.subject,\n",
    "                'predicate': fact.predicate,\n",
    "                'object': fact.object,\n",
    "                'source': fact.source,\n",
    "                'confidence': fact.confidence,\n",
    "                'context': fact.context,\n",
    "                'temporal': fact.temporal,\n",
    "                'spatial': fact.spatial,\n",
    "                'evidence_score': fact.evidence_score,\n",
    "                'source_uri': fact.source_uri\n",
    "            })\n",
    "        self.cache[key] = serializable_facts\n",
    "        self._save_cache()\n",
    "\n",
    "class EnhancedMultiKGRAGSystem:\n",
    "    \"\"\"Multi-Knowledge Graph RAG system with chunking and location extraction - CLAUDE 4 VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connectors = {\n",
    "            'wikidata': EnhancedWikidataConnector(),\n",
    "            'dbpedia': EnhancedDBpediaConnector(),\n",
    "            'conceptnet': EnhancedConceptNetConnector()\n",
    "        }\n",
    "        self.cache = MultiKGCache()\n",
    "        self.chunker = TextChunker()\n",
    "        self.location_extractor = LocationExtractor()\n",
    "        self.global_locations = {}\n",
    "        self.vectorstore = None  # For RAG retrieval\n",
    "        self.document_chunks = []  # Store processed chunks for RAG\n",
    "        self.stats = {\n",
    "            'queries_processed': 0,\n",
    "            'entities_extracted': 0,\n",
    "            'facts_retrieved': 0,\n",
    "            'cache_hits': 0,\n",
    "            'chunks_processed': 0,\n",
    "            'locations_found': 0,\n",
    "            'locations_with_coordinates': 0,\n",
    "            'location_duplicates_avoided': 0,\n",
    "            'rag_queries': 0\n",
    "        }\n",
    "        \n",
    "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entities from text\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
    "        matches = re.findall(pattern, text)\n",
    "        entities.extend(matches)\n",
    "        \n",
    "        stop_words = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
    "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
    "        }\n",
    "        \n",
    "        filtered_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.strip()\n",
    "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
    "                filtered_entities.append(entity)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for entity in filtered_entities:\n",
    "            if entity.lower() not in seen:\n",
    "                seen.add(entity.lower())\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities[:12]\n",
    "    \n",
    "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
    "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
    "        all_facts = {}\n",
    "        cache_hits = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for entity in entities:\n",
    "                for source_name, connector in self.connectors.items():\n",
    "                    # Check cache first\n",
    "                    cached_facts = self.cache.get(source_name, entity)\n",
    "                    if cached_facts:\n",
    "                        cache_hits += 1\n",
    "                        if entity not in all_facts:\n",
    "                            all_facts[entity] = []\n",
    "                        for fact_data in cached_facts:\n",
    "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
    "                            all_facts[entity].append(fact)\n",
    "                    else:\n",
    "                        future = executor.submit(connector.retrieve_facts, entity, 5)\n",
    "                        futures[future] = (entity, source_name)\n",
    "            \n",
    "            # Collect results with better timeout handling\n",
    "            completed = 0\n",
    "            total_futures = len(futures)\n",
    "            \n",
    "            try:\n",
    "                for future in as_completed(futures, timeout=45):  # Increased timeout\n",
    "                    entity, source_name = futures[future]\n",
    "                    completed += 1\n",
    "                    \n",
    "                    try:\n",
    "                        facts = future.result(timeout=5)  # Individual future timeout\n",
    "                        if facts:\n",
    "                            self.cache.set(source_name, entity, facts)\n",
    "                            \n",
    "                            if entity not in all_facts:\n",
    "                                all_facts[entity] = []\n",
    "                            all_facts[entity].extend(facts)\n",
    "                            \n",
    "                            self.stats['facts_retrieved'] += len(facts)\n",
    "                        \n",
    "                        logger.debug(f\"‚úÖ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ùå {source_name} failed for {entity}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except TimeoutError:\n",
    "                pending_count = total_futures - completed\n",
    "                logger.warning(f\"‚è∞ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
    "                \n",
    "                # Cancel remaining futures\n",
    "                for future in futures:\n",
    "                    if not future.done():\n",
    "                        future.cancel()\n",
    "        \n",
    "        self.stats['cache_hits'] += cache_hits\n",
    "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
    "        return all_facts\n",
    "    \n",
    "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
    "        \"\"\"Format KG facts into context string\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for entity, facts in kg_facts.items():\n",
    "            if facts:\n",
    "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
    "                \n",
    "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
    "                \n",
    "                by_source = {}\n",
    "                for fact in sorted_facts[:4]:\n",
    "                    if fact.source not in by_source:\n",
    "                        by_source[fact.source] = []\n",
    "                    by_source[fact.source].append(fact)\n",
    "                \n",
    "                for source, source_facts in by_source.items():\n",
    "                    context_parts.append(f\"\\nFrom {source}:\")\n",
    "                    for fact in source_facts[:2]:\n",
    "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
    "                        if fact.confidence < 0.8:\n",
    "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
    "                        context_parts.append(fact_str)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
    "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
    "        location_key = location_info.name.lower().strip()\n",
    "        \n",
    "        if location_key in self.global_locations:\n",
    "            existing = self.global_locations[location_key]\n",
    "            if (location_info.latitude and location_info.longitude and \n",
    "                (not existing.latitude or not existing.longitude)):\n",
    "                self.global_locations[location_key] = location_info\n",
    "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
    "            else:\n",
    "                self.stats['location_duplicates_avoided'] += 1\n",
    "                logger.debug(f\"Location {location_info.name} already registered\")\n",
    "        else:\n",
    "            self.global_locations[location_key] = location_info\n",
    "            logger.info(f\"Registered new location: {location_info.name}\")\n",
    "        \n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
    "        return f\"ste:Location_{clean_name}\"\n",
    "    \n",
    "    def process_chunk(self, chunk: str, chunk_num: int, claude_client) -> str:  # CHANGED: Parameter name\n",
    "        \"\"\"Process a single chunk of text with RAG-enhanced location extraction - CLAUDE 4 VERSION\"\"\"\n",
    "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars) - CLAUDE 4 WITH RAG\")\n",
    "        \n",
    "        # 1. RAG RETRIEVAL: Get relevant context from vector store\n",
    "        relevant_context = \"\"\n",
    "        if self.vectorstore:\n",
    "            try:\n",
    "                # Use the chunk as a query to retrieve similar/relevant text\n",
    "                relevant_docs = self.vectorstore.similarity_search(chunk, k=7)\n",
    "                retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
    "                relevant_context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "                logger.info(f\"Retrieved {len(retrieved_chunks)} relevant chunks via RAG for chunk {chunk_num}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"RAG retrieval failed for chunk {chunk_num}: {e}\")\n",
    "                relevant_context = \"\"\n",
    "        \n",
    "        # 2. Extract entities and locations\n",
    "        entities = self.extract_entities_advanced(chunk)\n",
    "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
    "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
    "        \n",
    "        # 3. Enrich locations with coordinates\n",
    "        enriched_locations = {}\n",
    "        for location_name in locations[:5]:\n",
    "            location_info = self.location_extractor.enrich_location(location_name)\n",
    "            if location_info:\n",
    "                self.register_global_location(location_info)\n",
    "                enriched_locations[location_name] = location_info\n",
    "                self.stats['locations_found'] += 1\n",
    "                if location_info.latitude and location_info.longitude:\n",
    "                    self.stats['locations_with_coordinates'] += 1\n",
    "        \n",
    "        if not entities and not enriched_locations:\n",
    "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
    "            return \"\"\n",
    "        \n",
    "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
    "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
    "        \n",
    "        # 4. Get KG facts for entities\n",
    "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
    "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
    "        location_context = self.format_location_context(enriched_locations)\n",
    "        \n",
    "        # 5. RAG-ENHANCED PROMPT: Use retrieved context + KG facts + locations - OPTIMIZED FOR CLAUDE 4\n",
    "        enhanced_prompt = f\"\"\"You are extracting historical events using RAG (Retrieval-Augmented Generation). Use ALL available context sources to enhance your extraction.\n",
    "\n",
    "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
    "{chunk}\n",
    "\n",
    "RAG RETRIEVED RELEVANT CONTEXT:\n",
    "{relevant_context if relevant_context else \"No relevant context retrieved.\"}\n",
    "\n",
    "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
    "{kg_context}\n",
    "\n",
    "LOCATION INFORMATION WITH COORDINATES:\n",
    "{location_context}\n",
    "\n",
    "TASK: Extract ONLY the events that are actually mentioned in the current text chunk. Use the RAG retrieved context, KG facts, and location coordinates to enhance details but stay faithful to what's actually in the current chunk.\n",
    "\n",
    "Requirements:\n",
    "1. Extract ONLY events mentioned in the CURRENT text chunk (not from retrieved context)\n",
    "2. Use RAG retrieved context to provide additional historical context and validation\n",
    "3. Use KG facts to enhance entity information\n",
    "4. Use location coordinates to provide precise geographical data\n",
    "5. Include ALL these properties for each event:\n",
    "   - ste:hasType (description of event)\n",
    "   - ste:hasAgent (who caused/led the event)\n",
    "   - ste:hasTime (when it happened)\n",
    "   - ste:hasLocation (location name from text)\n",
    "   - ste:hasLatitude (latitude coordinate if available)\n",
    "   - ste:hasLongitude (longitude coordinate if available)\n",
    "   - ste:hasCountry (country if available)\n",
    "   - ste:hasRegion (region if available)\n",
    "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
    "   - ste:hasResult (outcome/consequence)\n",
    "   - ste:hasRAGContext \"yes\" (to indicate this was RAG-enhanced)\n",
    "   - ste:hasLLM \"Claude4\" (to indicate Claude 4 was used)\n",
    "\n",
    "Output format (do not include prefixes, they will be added later):\n",
    "```turtle\n",
    "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
    "    ste:hasType \"specific description from current chunk\" ;\n",
    "    ste:hasAgent \"specific person from current chunk\" ;\n",
    "    ste:hasTime \"specific date from current chunk\" ;\n",
    "    ste:hasLocation \"specific location from current chunk\" ;\n",
    "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
    "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
    "    ste:hasCountry \"Italy\" ;\n",
    "    ste:hasRegion \"Sicily\" ;\n",
    "    ste:hasLocationSource \"wikidata\" ;\n",
    "    ste:hasResult \"specific outcome from current chunk\" ;\n",
    "    ste:hasRAGContext \"yes\" ;\n",
    "    ste:hasLLM \"Claude4\" .\n",
    "```\n",
    "\n",
    "IMPORTANT: \n",
    "- The PRIMARY source is the CURRENT text chunk - extract events from IT\n",
    "- Use RAG retrieved context to validate and enhance your understanding\n",
    "- Use KG facts to enrich entity details\n",
    "- Include precise coordinates from location sources\n",
    "- Mark all events with ste:hasRAGContext \"yes\" and ste:hasLLM \"Claude4\"\n",
    "- Only extract events explicitly mentioned in the current chunk\n",
    "- If no clear events are found in current chunk, return empty\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # CHANGED: Claude API call instead of OpenAI\n",
    "            response = claude_client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",  # Latest Claude model\n",
    "                max_tokens=4000,\n",
    "                temperature=0,\n",
    "                messages=[{\"role\": \"user\", \"content\": enhanced_prompt}]\n",
    "            )\n",
    "            \n",
    "            turtle_output = self.clean_turtle(response.content[0].text)\n",
    "            self.stats['chunks_processed'] += 1\n",
    "            self.stats['rag_queries'] += 1  # Count as RAG usage\n",
    "            logger.info(f\"Generated RAG-enhanced RDF for chunk {chunk_num} using Claude 4\")\n",
    "            return turtle_output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing chunk {chunk_num} with Claude RAG: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def format_location_ids(self, location_ids: Dict[str, str]) -> str:\n",
    "        \"\"\"Format location IDs for prompt\"\"\"\n",
    "        if not location_ids:\n",
    "            return \"No location IDs available.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for location_name, location_id in location_ids.items():\n",
    "            context_parts.append(f\"- {location_name} -> {location_id}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
    "        \"\"\"Format location information into context string\"\"\"\n",
    "        if not enriched_locations:\n",
    "            return \"No location coordinates available.\"\n",
    "        \n",
    "        context_parts = [\"\\n=== Location Information ===\"]\n",
    "        \n",
    "        for location_name, location_info in enriched_locations.items():\n",
    "            context_parts.append(f\"\\n{location_name}:\")\n",
    "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
    "            \n",
    "            if location_info.latitude and location_info.longitude:\n",
    "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
    "                if location_info.source == \"corrected\":\n",
    "                    context_parts.append(f\"  - NOTE: Coordinates were corrected for historical accuracy\")\n",
    "            else:\n",
    "                context_parts.append(\"  - Coordinates: Not available\")\n",
    "            \n",
    "            if location_info.country:\n",
    "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
    "            \n",
    "            if location_info.region:\n",
    "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
    "            \n",
    "            if location_info.uri:\n",
    "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_global_location_rdf(self) -> str:\n",
    "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
    "        if not self.global_locations:\n",
    "            return \"\"\n",
    "        \n",
    "        location_rdf_parts = []\n",
    "        \n",
    "        for location_key, location_info in self.global_locations.items():\n",
    "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
    "            location_id = f\"ste:Location_{clean_name}\"\n",
    "            \n",
    "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
    "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
    "            \n",
    "            if location_info.latitude and location_info.longitude:\n",
    "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
    "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
    "            \n",
    "            if location_info.country:\n",
    "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
    "            \n",
    "            if location_info.region:\n",
    "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
    "            \n",
    "            if location_info.source:\n",
    "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
    "            \n",
    "            if location_info.uri:\n",
    "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
    "            \n",
    "            if rdf_lines[-1].endswith(' ;'):\n",
    "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
    "            \n",
    "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
    "        \n",
    "        return '\\n\\n'.join(location_rdf_parts)\n",
    "    \n",
    "    def clean_turtle(self, raw_output: str) -> str:\n",
    "        \"\"\"Clean turtle output\"\"\"\n",
    "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "        \n",
    "        lines = raw_output.strip().split('\\n')\n",
    "        turtle_lines = []\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if (stripped.startswith('@') or stripped.startswith('<') or \n",
    "                stripped.startswith(':') or stripped.startswith('_') or \n",
    "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
    "                turtle_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(turtle_lines)\n",
    "    \n",
    "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
    "        \"\"\"Create vector store from text chunks for RAG retrieval\"\"\"\n",
    "        try:\n",
    "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            \n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "            )\n",
    "            \n",
    "            # Prepare chunks with metadata\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            \n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                if len(chunk.strip()) > 50:  # Only include substantial chunks\n",
    "                    documents.append(chunk)\n",
    "                    metadatas.append({\n",
    "                        'chunk_id': i,\n",
    "                        'length': len(chunk),\n",
    "                        'type': 'text_chunk'\n",
    "                    })\n",
    "            \n",
    "            if documents:\n",
    "                self.vectorstore = FAISS.from_texts(documents, embeddings, metadatas=metadatas)\n",
    "                self.document_chunks = documents\n",
    "                logger.info(f\"Created vector store with {len(documents)} chunks\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.warning(\"No suitable chunks found for vector store\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating vector store: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def rag_query(self, query: str, claude_client, k: int = 3) -> Dict[str, Any]:  # CHANGED: Parameter name\n",
    "        \"\"\"Perform RAG query: retrieve relevant chunks + KG facts, then generate response - CLAUDE 4 VERSION\"\"\"\n",
    "        self.stats['rag_queries'] += 1\n",
    "        logger.info(f\"Processing RAG query with Claude 4: '{query[:50]}...'\")\n",
    "        \n",
    "        if not self.vectorstore:\n",
    "            return {\"error\": \"Vector store not initialized. Call prepare_vectorstore() first.\"}\n",
    "        \n",
    "        try:\n",
    "            # 1. RETRIEVE: Get relevant text chunks\n",
    "            relevant_docs = self.vectorstore.similarity_search(query, k=k)\n",
    "            retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
    "            \n",
    "            # 2. EXTRACT: Get entities from query\n",
    "            query_entities = self.extract_entities_advanced(query)\n",
    "            \n",
    "            # 3. RETRIEVE: Get KG facts for entities\n",
    "            kg_facts = self.retrieve_kg_facts_enhanced(query_entities)\n",
    "            kg_context = self.format_kg_context_enhanced(kg_facts)\n",
    "            \n",
    "            # 4. EXTRACT & ENRICH: Get locations from query\n",
    "            query_locations = self.location_extractor.extract_locations_from_text(query)\n",
    "            enriched_locations = {}\n",
    "            for location_name in query_locations[:5]:\n",
    "                location_info = self.location_extractor.enrich_location(location_name)\n",
    "                if location_info:\n",
    "                    enriched_locations[location_name] = location_info\n",
    "            \n",
    "            location_context = self.format_location_context(enriched_locations)\n",
    "            \n",
    "            # 5. AUGMENT: Create enhanced context for generation\n",
    "            context_parts = [\n",
    "                f\"QUERY: {query}\",\n",
    "                f\"\\nRETRIEVED RELEVANT TEXT CHUNKS:\",\n",
    "                \"\\n\" + \"\\n---\\n\".join(retrieved_chunks),\n",
    "                f\"\\nKNOWLEDGE GRAPH CONTEXT:\",\n",
    "                kg_context,\n",
    "                f\"\\nLOCATION CONTEXT:\",\n",
    "                location_context\n",
    "            ]\n",
    "            \n",
    "            enhanced_context = \"\\n\".join(context_parts)\n",
    "            \n",
    "            # 6. GENERATE: Create comprehensive response - CLAUDE 4 OPTIMIZED\n",
    "            rag_prompt = f\"\"\"You are an expert historian with access to multiple knowledge sources. Answer the question using the provided context.\n",
    "\n",
    "{enhanced_context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the question comprehensively using ALL available context\n",
    "2. Cite specific information from the retrieved text chunks\n",
    "3. Incorporate relevant knowledge graph facts to enhance your answer\n",
    "4. Include precise location information and coordinates when relevant\n",
    "5. If information conflicts between sources, mention this\n",
    "6. Be specific about dates, places, people, and events\n",
    "7. If you cannot answer completely, explain what information is missing\n",
    "8. Structure your response clearly with specific details\n",
    "9. Use your historical knowledge to provide additional context when appropriate\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "            \n",
    "            # 7. GENERATE: Get Claude response\n",
    "            response = claude_client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=4000,\n",
    "                temperature=0,\n",
    "                messages=[{\"role\": \"user\", \"content\": rag_prompt}]\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"answer\": response.content[0].text,\n",
    "                \"retrieved_chunks\": retrieved_chunks,\n",
    "                \"entities_found\": query_entities,\n",
    "                \"kg_facts_count\": sum(len(facts) for facts in kg_facts.values()),\n",
    "                \"locations_found\": list(enriched_locations.keys()),\n",
    "                \"sources\": {\n",
    "                    \"text_chunks\": len(retrieved_chunks),\n",
    "                    \"kg_sources\": list(set(fact.source for facts in kg_facts.values() for fact in facts)),\n",
    "                    \"location_sources\": list(set(loc.source for loc in enriched_locations.values()))\n",
    "                },\n",
    "                \"llm_used\": \"Claude 4\"  # CHANGED: Add LLM info\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"RAG query failed with Claude: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def interactive_rag_session(self, claude_client):  # CHANGED: Parameter name\n",
    "        \"\"\"Start an interactive RAG session - CLAUDE 4 VERSION\"\"\"\n",
    "        print(\"\\nü§ñ Starting Interactive RAG Session with Claude 4\")\n",
    "        print(\"Ask questions about your text. Type 'quit' to exit.\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\n‚ùì Your question: \").strip()\n",
    "                \n",
    "                if query.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"üëã Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not query:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\nüîç Processing your question with Claude 4...\")\n",
    "                result = self.rag_query(query, claude_client)\n",
    "                \n",
    "                if \"error\" in result:\n",
    "                    print(f\"‚ùå Error: {result['error']}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nüìù **Answer (Claude 4):**\")\n",
    "                print(result['answer'])\n",
    "                \n",
    "                print(f\"\\nüìä **Sources Used:**\")\n",
    "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
    "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
    "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
    "                if result['locations_found']:\n",
    "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
    "                print(f\"   - LLM: {result['llm_used']}\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nüëã Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "                continue\n",
    "\n",
    "# Utility functions\n",
    "def load_api_key():\n",
    "    \"\"\"Load Anthropic API key\"\"\"  # CHANGED: Comment\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")  # CHANGED: Environment variable name\n",
    "    if not api_key:\n",
    "        print(\"Error: ANTHROPIC_API_KEY not found in environment variables\")  # CHANGED: Error message\n",
    "        print(\"Please set your Anthropic API key:\")\n",
    "        print(\"export ANTHROPIC_API_KEY='your-api-key-here'\")\n",
    "        return None\n",
    "    print(\"Anthropic API Key loaded successfully.\")  # CHANGED: Success message\n",
    "    return api_key\n",
    "\n",
    "def load_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Load text from file\"\"\"\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        print(f\"Loaded text from {filepath}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def initialize_claude_client(api_key: str):  # CHANGED: Function name and logic\n",
    "    \"\"\"Initialize Claude client\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    try:\n",
    "        client = anthropic.Anthropic(api_key=api_key)\n",
    "        print(\"Claude 4 client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Claude client: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
    "    \"\"\"Create vector store from text\"\"\"\n",
    "    try:\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        \n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        )\n",
    "        \n",
    "        # Split text into sentences for better retrieval\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        texts = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 20]\n",
    "        \n",
    "        if not texts:\n",
    "            return None\n",
    "        \n",
    "        vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "        multi_kg_system.vectorstore = vectorstore\n",
    "        multi_kg_system.document_chunks = texts\n",
    "        print(f\"üìö Vector store created with {len(texts)} text segments\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with chunking support and RAG capabilities - CLAUDE 4 VERSION\"\"\"\n",
    "    print(\"üöÄ Starting Multi-Knowledge Graph RAG System with Chunking - CLAUDE 4 VERSION\")\n",
    "    \n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        return\n",
    "    \n",
    "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
    "    if not domain_text:\n",
    "        print(\"‚ö†Ô∏è  No input file found, using sample text\")\n",
    "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC. \n",
    "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes. \n",
    "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
    "    else:\n",
    "        print(f\"üìÑ Using YOUR text from {INPUT_TEXT_FILE}\")\n",
    "        print(f\"üìù Text length: {len(domain_text)} characters\")\n",
    "    \n",
    "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
    "    claude_client = initialize_claude_client(api_key)  # CHANGED: Function call\n",
    "    \n",
    "    if not claude_client:  # CHANGED: Variable name\n",
    "        return\n",
    "    \n",
    "    # Prepare vector store for RAG FIRST\n",
    "    print(\"\\nüìö Setting up RAG vector store...\")\n",
    "    vectorstore = prepare_vectorstore_from_text(domain_text, multi_kg_system)\n",
    "    \n",
    "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
    "    print(f\"üî¢ Total tokens in text: {token_count:,}\")\n",
    "    \n",
    "    if token_count > 25000:  # CHANGED: Increased threshold for Claude\n",
    "        print(\"üìä Text is large, chunking into smaller pieces...\")\n",
    "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=25000)  # CHANGED: Increased\n",
    "        print(f\"üìÑ Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"üìÑ Text is small enough to process as single chunk\")\n",
    "        chunks = [domain_text]\n",
    "    \n",
    "    # Extract events and create RDF\n",
    "    all_turtle_outputs = []\n",
    "    all_entities = set()\n",
    "    \n",
    "    print(\"\\nüîÑ Processing chunks for event extraction with Claude 4 RAG...\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\nüîÑ Processing chunk {i}/{len(chunks)} with Claude 4...\")\n",
    "        \n",
    "        turtle_output = multi_kg_system.process_chunk(chunk, i, claude_client)  # CHANGED: Parameter\n",
    "        if turtle_output:\n",
    "            all_turtle_outputs.append(turtle_output)\n",
    "            \n",
    "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
    "        all_entities.update(chunk_entities)\n",
    "        \n",
    "        if i < len(chunks):\n",
    "            time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Save RDF output\n",
    "    if all_turtle_outputs:\n",
    "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
    "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
    "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        final_output = prefixes + \"# Historical Events with RAG-Enhanced Embedded Location Data (CLAUDE 4)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
    "        \n",
    "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_output)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Saved enhanced RDF to {OUTPUT_RAG_TTL}\")\n",
    "        print(f\"üìä Processing Statistics (Claude 4 with RAG):\")\n",
    "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
    "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
    "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
    "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
    "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
    "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
    "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
    "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
    "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
    "        print(f\"   - RAG queries for RDF generation: {multi_kg_system.stats['rag_queries']}\")\n",
    "        print(f\"   - LLM used: Claude 4 (Anthropic)\")  # CHANGED: Added LLM info\n",
    "        \n",
    "        print(f\"\\nüîó Knowledge Graph Connector Statistics:\")\n",
    "        for name, connector in multi_kg_system.connectors.items():\n",
    "            stats = connector.get_stats()\n",
    "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
    "        \n",
    "        if multi_kg_system.location_extractor.location_cache:\n",
    "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
    "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
    "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample of generated RDF:\")\n",
    "        print(\"=\"*60)\n",
    "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No events were extracted from any chunks\")\n",
    "    \n",
    "    # START RAG SESSION HERE! - CLAUDE 4 VERSION\n",
    "    if vectorstore:\n",
    "        print(f\"\\nü§ñ Claude 4 RAG System Ready!\")\n",
    "        \n",
    "        # Show example queries\n",
    "        print(f\"\\nüí° Try asking questions like:\")\n",
    "        print(f\"   - 'What battles happened in Sicily?'\")\n",
    "        print(f\"   - 'Who were the main leaders mentioned?'\")\n",
    "        print(f\"   - 'What events occurred in 415 BC?'\")\n",
    "        print(f\"   - 'Describe the naval engagements'\")\n",
    "        print(f\"   - 'What was the outcome of the siege?'\")\n",
    "        \n",
    "        # Ask if user wants to start interactive session\n",
    "        response = input(f\"\\n‚ùì Start interactive Claude 4 RAG session? (y/n): \").strip()\n",
    "        \n",
    "        # Check if user typed a question instead of y/n\n",
    "        if response.lower() not in ['y', 'yes', 'n', 'no', '']:\n",
    "            # User typed a question directly!\n",
    "            print(f\"\\nüîç Processing your question with Claude 4: '{response}'\")\n",
    "            result = multi_kg_system.rag_query(response, claude_client)\n",
    "            \n",
    "            if \"error\" not in result:\n",
    "                print(f\"\\nüìù **Answer (Claude 4):**\")\n",
    "                print(result['answer'])\n",
    "                \n",
    "                print(f\"\\nüìä **Sources Used:**\")\n",
    "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
    "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
    "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
    "                if result['locations_found']:\n",
    "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
    "                print(f\"   - LLM: {result['llm_used']}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {result['error']}\")\n",
    "            \n",
    "            # Ask if they want to continue with interactive session\n",
    "            continue_response = input(f\"\\n‚ùì Continue with interactive Claude 4 RAG session? (y/n): \").strip().lower()\n",
    "            if continue_response in ['y', 'yes', '']:\n",
    "                multi_kg_system.interactive_rag_session(claude_client)\n",
    "        \n",
    "        elif response.lower() in ['y', 'yes', '']:\n",
    "            multi_kg_system.interactive_rag_session(claude_client)\n",
    "        else:\n",
    "            print(f\"\\nüí° You can also query programmatically:\")\n",
    "            print(f\"   result = multi_kg_system.rag_query('your question', claude_client)\")\n",
    "            \n",
    "            # Offer a few sample queries\n",
    "            sample_queries = [\n",
    "                \"What are the main events mentioned in the text?\",\n",
    "                \"Which locations are mentioned?\",\n",
    "                \"Who are the key people involved?\"\n",
    "            ]\n",
    "            \n",
    "            print(f\"\\nüîç Running sample queries with Claude 4:\")\n",
    "            for query in sample_queries:\n",
    "                print(f\"\\n‚ùì Sample query: '{query}'\")\n",
    "                result = multi_kg_system.rag_query(query, claude_client)\n",
    "                if \"error\" not in result:\n",
    "                    print(f\"üìù Answer: {result['answer'][:200]}...\")\n",
    "                    print(f\"üìä Sources: {len(result['retrieved_chunks'])} chunks, {result['kg_facts_count']} KG facts\")\n",
    "                    print(f\"ü§ñ LLM: {result['llm_used']}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Error: {result['error']}\")\n",
    "                print(\"-\" * 40)\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not create vector store for RAG functionality\")\n",
    "    \n",
    "    print(f\"\\nüéâ Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
    "    print(f\"üìä System ran with Claude 4 + RAG + Knowledge Graph + Location enrichment.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
