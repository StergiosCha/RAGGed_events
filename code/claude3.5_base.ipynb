{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab0f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:26:58,735 - INFO - TRUE BASE GENERATION SYSTEM INITIALIZED\n",
      "2025-05-29 07:26:58,736 - INFO - ‚ùå Knowledge Graphs: DISABLED\n",
      "2025-05-29 07:26:58,736 - INFO - ‚ùå Location Enrichment: DISABLED\n",
      "2025-05-29 07:26:58,736 - INFO - ‚ùå External APIs: DISABLED\n",
      "2025-05-29 07:26:58,736 - INFO - ‚ùå RAG Text Retrieval: DISABLED\n",
      "2025-05-29 07:26:58,736 - INFO - ‚úÖ Pure LLM Generation: ENABLED\n",
      "2025-05-29 07:26:58,766 - INFO - Processing chunk 1 (59851 chars) - TRUE BASE GENERATION\n",
      "2025-05-29 07:26:58,770 - INFO - Found entities (text-only): ['The Project Gutenberg', 'The History', 'Peloponnesian War']...\n",
      "2025-05-29 07:26:58,770 - INFO - Found locations (text-only): ['Peloponnesian War', 'START', 'Achaeans']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting TRUE BASE GENERATION SYSTEM\n",
      "============================================================\n",
      "‚ùå Knowledge Graphs: DISABLED\n",
      "‚ùå Location Coordinate Lookup: DISABLED\n",
      "‚ùå External APIs: DISABLED\n",
      "‚ùå RAG Text Retrieval: DISABLED\n",
      "‚ùå All External Knowledge Sources: DISABLED\n",
      "‚úÖ Pure LLM Generation from Text Only: ENABLED\n",
      "============================================================\n",
      "‚úÖ Anthropic API Key loaded successfully.\n",
      "‚úÖ Loaded text from part_aa\n",
      "üìÑ Using text from part_aa\n",
      "üìù Text length: 398568 characters\n",
      "‚úÖ LLM initialized successfully.\n",
      "üî¢ Total tokens in text: 99,642\n",
      "üìä Text is large, chunking into smaller pieces...\n",
      "üìÑ Created 7 chunks\n",
      "\n",
      "üîÑ Processing chunks with TRUE BASE GENERATION...\n",
      "\n",
      "üîÑ Processing chunk 1/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:27:13,877 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:27:13,903 - INFO - Generated 12 events from chunk 1 (base generation)\n",
      "2025-05-29 07:27:14,409 - INFO - Processing chunk 2 (59939 chars) - TRUE BASE GENERATION\n",
      "2025-05-29 07:27:14,424 - INFO - Found entities (text-only): ['They', 'Chimerium', 'Thesprotis']...\n",
      "2025-05-29 07:27:14,425 - INFO - Found locations (text-only): ['Surely', 'Some', 'Peloponnesian War']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 2/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:27:32,504 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:27:32,509 - INFO - Generated 12 events from chunk 2 (base generation)\n",
      "2025-05-29 07:27:33,012 - INFO - Processing chunk 3 (59850 chars) - TRUE BASE GENERATION\n",
      "2025-05-29 07:27:33,029 - INFO - Found entities (text-only): ['Manifold', 'Hellenes', 'Sparta']...\n",
      "2025-05-29 07:27:33,030 - INFO - Found locations (text-only): ['Hellespont', 'Dorians', 'Ithome Most']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 3/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:27:48,132 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:27:48,135 - INFO - Generated 12 events from chunk 3 (base generation)\n",
      "2025-05-29 07:27:48,647 - INFO - Processing chunk 4 (59987 chars) - TRUE BASE GENERATION\n",
      "2025-05-29 07:27:48,657 - INFO - Found entities (text-only): ['Slow', 'Meanwhile', 'Again']...\n",
      "2025-05-29 07:27:48,658 - INFO - Found locations (text-only): ['Hellespont', 'Pronaeans Not', 'Zeuxis']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 4/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:28:08,212 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:28:08,220 - INFO - Generated 10 events from chunk 4 (base generation)\n",
      "2025-05-29 07:28:08,726 - INFO - Processing chunk 5 (59822 chars) - TRUE BASE GENERATION\n",
      "2025-05-29 07:28:08,738 - INFO - Found entities (text-only): ['Thus', 'Athenians You', 'Athens']...\n",
      "2025-05-29 07:28:08,739 - INFO - Found locations (text-only): ['Suddenly', 'Some', 'Chian']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 5/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:28:25,863 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:28:25,867 - INFO - Generated 12 events from chunk 5 (base generation)\n",
      "2025-05-29 07:28:26,370 - INFO - Processing chunk 6 (59994 chars) - TRUE BASE GENERATION\n",
      "2025-05-29 07:28:26,387 - INFO - Found entities (text-only): ['The Peloponnesians', 'The Athenians', 'Phormio']...\n",
      "2025-05-29 07:28:26,388 - INFO - Found locations (text-only): ['Hellespont', 'Acarnania', 'Scythians']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 6/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:28:42,655 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:28:42,659 - INFO - Generated 10 events from chunk 6 (base generation)\n",
      "2025-05-29 07:28:43,165 - INFO - Processing chunk 7 (36927 chars) - TRUE BASE GENERATION\n",
      "2025-05-29 07:28:43,177 - INFO - Found entities (text-only): ['Mitylenians', 'Peloponnese', 'Mitylene']...\n",
      "2025-05-29 07:28:43,178 - INFO - Found locations (text-only): ['Ephesus', 'Chians', 'Ionian']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 7/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:28:59,089 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:28:59,093 - INFO - Generated 10 events from chunk 7 (base generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved TRUE BASE GENERATION RDF to extracted_events_base_generation_ckaude.ttl\n",
      "üìä TRUE BASE GENERATION Statistics:\n",
      "   - Generation Mode: PURE BASE (No External Knowledge)\n",
      "   - Total chunks processed: 7\n",
      "   - Successful chunks: 7\n",
      "   - Events extracted: 78\n",
      "   - External API calls: 0 (should be 0)\n",
      "   - Knowledge sources used: 0 (should be 0)\n",
      "   - Knowledge Graph queries: 0 (DISABLED)\n",
      "   - Location coordinate lookups: 0 (DISABLED)\n",
      "   - RAG text retrievals: 0 (DISABLED)\n",
      "\n",
      "üìù Sample of TRUE BASE GENERATION RDF:\n",
      "============================================================\n",
      "@prefix ste: <http://www.example.org/ste#> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "# TRUE BASE GENERATION - No External Knowledge Sources\n",
      "ste:Event1_1 a ste:Event ;\n",
      "    ste:hasType \"Commencement of the Peloponnesian War between Athens and Sparta\" ;\n",
      "    ste:hasAgent \"Athenians and Peloponnesians led by their respective city-states\" ;\n",
      "    ste:hasTime \"431 BC\" ;\n",
      "    ste:hasLocation \"Greece\" ;\n",
      "    ste:hasLatitude \"38.0\" ;\n",
      "    ste:hasLongitude \"23.0\" ;\n",
      "    ste:hasCountry \"Greece\" ;\n",
      "    ste:hasRegion \"Hellas\" ;\n",
      "    ste:hasLocationSource \"inferred\" ;\n",
      "    ste:hasResult \"Beginning of the greatest war in Greek history up to that time\" .\n",
      "\n",
      "ste:Event1_2 a ste:Event ;\n",
      "    st...\n",
      "============================================================\n",
      "\n",
      "üéØ VERIFICATION:\n",
      "   ‚úÖ No external knowledge was used\n",
      "   ‚úÖ No API calls were made\n",
      "   ‚úÖ Only text-based pattern matching was used\n",
      "   ‚úÖ LLM used only information from the input text\n",
      "\n",
      "üéâ TRUE BASE GENERATION complete!\n",
      "üìÑ Output file: extracted_events_base_generation_ckaude.ttl\n",
      "üîç This output contains ONLY information from your text, no external knowledge\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TRUE BASE GENERATION SYSTEM\n",
    "No external knowledge sources - pure LLM generation from text only\n",
    "All external APIs, Knowledge Graphs, and Location enrichment DISABLED\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Only essential imports - no external knowledge sources\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Configuration\n",
    "INPUT_TEXT_FILE = \"part_aa\"\n",
    "OUTPUT_BASE_TTL = 'extracted_events_base_generation_ckaude.ttl'\n",
    "\n",
    "# TRUE BASE GENERATION FLAGS - All external knowledge DISABLED\n",
    "RAG_ENABLED = False\n",
    "KNOWLEDGE_GRAPHS_ENABLED = False\n",
    "LOCATION_ENRICHMENT_ENABLED = False\n",
    "EXTERNAL_APIS_ENABLED = False\n",
    "\n",
    "# Token limits\n",
    "MAX_TOKENS_PER_REQUEST = 100000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"Handles text chunking to manage token limits\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"claude-sonnet-4-20250514\"):\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Approximate token count for Claude (roughly 4 chars per token)\"\"\"\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 15000) -> List[str]:\n",
    "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class TrueBaseGenerationSystem:\n",
    "    \"\"\"TRUE Base Generation System - No external knowledge sources\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunker = TextChunker()\n",
    "        self.stats = {\n",
    "            'chunks_processed': 0,\n",
    "            'events_extracted': 0,\n",
    "            'external_api_calls': 0,  # This will stay 0\n",
    "            'knowledge_sources_used': 0,  # This will stay 0\n",
    "        }\n",
    "        logger.info(\"TRUE BASE GENERATION SYSTEM INITIALIZED\")\n",
    "        logger.info(\"‚ùå Knowledge Graphs: DISABLED\")\n",
    "        logger.info(\"‚ùå Location Enrichment: DISABLED\") \n",
    "        logger.info(\"‚ùå External APIs: DISABLED\")\n",
    "        logger.info(\"‚ùå RAG Text Retrieval: DISABLED\")\n",
    "        logger.info(\"‚úÖ Pure LLM Generation: ENABLED\")\n",
    "    \n",
    "    def extract_basic_entities_from_text_only(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entities using ONLY pattern matching - no external validation\"\"\"\n",
    "        # Simple pattern matching - no external knowledge validation\n",
    "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        # Basic stopword filtering - no external knowledge\n",
    "        stop_words = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', \n",
    "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', \n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During'\n",
    "        }\n",
    "        \n",
    "        filtered_entities = []\n",
    "        for entity in matches:\n",
    "            entity = entity.strip()\n",
    "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
    "                filtered_entities.append(entity)\n",
    "        \n",
    "        # Remove duplicates - keep first 10 to avoid overwhelming the prompt\n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for entity in filtered_entities:\n",
    "            if entity.lower() not in seen:\n",
    "                seen.add(entity.lower())\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities[:10]  # Limited to avoid prompt bloat\n",
    "    \n",
    "    def extract_basic_locations_from_text_only(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract locations using ONLY pattern matching - no coordinate lookup\"\"\"\n",
    "        # Simple pattern matching for potential locations - no external validation\n",
    "        location_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
    "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
    "        ]\n",
    "        \n",
    "        locations = []\n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            locations.extend(matches)\n",
    "        \n",
    "        # Basic filtering - no external knowledge\n",
    "        location_stopwords = {\n",
    "            'The', 'This', 'That', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
    "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
    "            'September', 'October', 'November', 'December'\n",
    "        }\n",
    "        \n",
    "        filtered_locations = []\n",
    "        for loc in locations:\n",
    "            loc = loc.strip()\n",
    "            if (loc not in location_stopwords and len(loc) > 2 and not loc.isdigit()):\n",
    "                filtered_locations.append(loc)\n",
    "        \n",
    "        return list(set(filtered_locations))[:5]  # Limited to avoid prompt bloat\n",
    "    \n",
    "    def process_chunk_true_base(self, chunk: str, chunk_num: int, llm) -> str:\n",
    "        \"\"\"Process chunk with TRUE base generation - no external knowledge\"\"\"\n",
    "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars) - TRUE BASE GENERATION\")\n",
    "        \n",
    "        # ONLY extract entities/locations from text patterns - NO external validation\n",
    "        entities = self.extract_basic_entities_from_text_only(chunk)\n",
    "        locations = self.extract_basic_locations_from_text_only(chunk)\n",
    "        \n",
    "        logger.info(f\"Found entities (text-only): {entities[:3]}...\")\n",
    "        logger.info(f\"Found locations (text-only): {locations[:3]}...\")\n",
    "        \n",
    "        if not entities and not locations:\n",
    "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
    "            return \"\"\n",
    "        \n",
    "        # TRUE BASE GENERATION WITH INFERENCE PROMPT - SAME FORMAT AS ENHANCED SYSTEMS\n",
    "        base_prompt = f\"\"\"You are extracting historical events from text using ONLY the information provided in the text chunk. Do not use external knowledge sources, but you CAN and SHOULD make reasonable inferences from the text.\n",
    "\n",
    "TEXT CHUNK {chunk_num} TO ANALYZE:\n",
    "{chunk}\n",
    "\n",
    "ENTITIES FOUND IN TEXT: {', '.join(entities) if entities else 'None'}\n",
    "LOCATIONS FOUND IN TEXT: {', '.join(locations) if locations else 'None'}\n",
    "\n",
    "TASK: Extract historical events mentioned in this text chunk using the text information and making REASONABLE INFERENCES.\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Extract ONLY events explicitly mentioned in the text chunk\n",
    "2. Use information directly stated in the text\n",
    "3. MAKE REASONABLE INFERENCES from context clues in the text\n",
    "4. If you can reasonably infer coordinates, countries, regions from textual context, DO IT\n",
    "5. Include ALL these properties for each event:\n",
    "   - ste:hasType (description of event, enhanced with context)\n",
    "   - ste:hasAgent (who caused/led the event, with inferred roles)\n",
    "   - ste:hasTime (when it happened, with inferred specificity)\n",
    "   - ste:hasLocation (location name from text)\n",
    "   - ste:hasLatitude (infer approximate coordinates if you can from text context)\n",
    "   - ste:hasLongitude (infer approximate coordinates if you can from text context)\n",
    "   - ste:hasCountry (infer country from textual geographic context)\n",
    "   - ste:hasRegion (infer region from textual geographic context)\n",
    "   - ste:hasLocationSource \"inferred\" (if you made geographic inferences)\n",
    "   - ste:hasResult (outcome, enhanced with contextual inference)\n",
    "\n",
    "INFERENCE GUIDELINES:\n",
    "- If text mentions \"Athens\", infer it's in Greece, approximate coordinates\n",
    "- If text mentions \"Sicily\", infer it's in Italy, Mediterranean coordinates  \n",
    "- If text mentions \"Sparta/Lacedaemon\", infer Peloponnese, Greece\n",
    "- If you know from context clues what geographic region events occurred in, infer coordinates\n",
    "- If someone is called \"King\", infer royal title\n",
    "- If text implies timeframes, infer more specific dates\n",
    "- If outcomes are implied, infer likely results\n",
    "\n",
    "Output format (do not include prefixes):\n",
    "```turtle\n",
    "ste:Event{chunk_num}_1 a ste:Event ;\n",
    "    ste:hasType \"specific event type inferred from context\" ;\n",
    "    ste:hasAgent \"person/group with inferred roles\" ;\n",
    "    ste:hasTime \"time period with inferred specificity\" ;\n",
    "    ste:hasLocation \"location name from text\" ;\n",
    "    ste:hasLatitude \"37.9838\" ;\n",
    "    ste:hasLongitude \"23.7275\" ;\n",
    "    ste:hasCountry \"Greece\" ;\n",
    "    ste:hasRegion \"Attica\" ;\n",
    "    ste:hasLocationSource \"inferred\" ;\n",
    "    ste:hasResult \"outcome inferred from context\" .\n",
    "```\n",
    "\n",
    "CRITICAL: \n",
    "- Use the SAME output format as enhanced systems for fair comparison\n",
    "- INFER coordinates, countries, regions if you can reasonably deduce them from text\n",
    "- Make the events as detailed and specific as possible through inference\n",
    "- If you truly cannot infer something, then use empty string \"\"\n",
    "- The goal is to extract maximum information through text analysis and inference\n",
    "\n",
    "If no clear historical events are mentioned in the text, return empty.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke([HumanMessage(content=base_prompt)])\n",
    "            turtle_output = self.clean_turtle(response.content)\n",
    "            \n",
    "            if turtle_output:\n",
    "                self.stats['chunks_processed'] += 1\n",
    "                # Count events by counting \"ste:Event\" occurrences\n",
    "                event_count = turtle_output.count('ste:Event')\n",
    "                self.stats['events_extracted'] += event_count\n",
    "                logger.info(f\"Generated {event_count} events from chunk {chunk_num} (base generation)\")\n",
    "            \n",
    "            return turtle_output\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing chunk {chunk_num}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def clean_turtle(self, raw_output: str) -> str:\n",
    "        \"\"\"Clean turtle output\"\"\"\n",
    "        # Extract turtle code block if present\n",
    "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "        \n",
    "        # If no code block, try to extract turtle-like lines\n",
    "        lines = raw_output.strip().split('\\n')\n",
    "        turtle_lines = []\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if (stripped.startswith('@') or stripped.startswith('<') or \n",
    "                stripped.startswith(':') or stripped.startswith('ste:') or \n",
    "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
    "                turtle_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(turtle_lines)\n",
    "\n",
    "# Utility functions\n",
    "def load_api_key():\n",
    "    \"\"\"Load Anthropic API key\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: ANTHROPIC_API_KEY not found\")\n",
    "        return None\n",
    "    print(\"‚úÖ Anthropic API Key loaded successfully.\")\n",
    "    return api_key\n",
    "\n",
    "def load_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Load text from file\"\"\"\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        print(f\"‚úÖ Loaded text from {filepath}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def initialize_llm(api_key: str):\n",
    "    \"\"\"Initialize LLM\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    try:\n",
    "        llm = ChatAnthropic(model=\"claude-sonnet-4-20250514\", temperature=0, anthropic_api_key=api_key)\n",
    "        print(\"‚úÖ LLM initialized successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function - TRUE BASE GENERATION ONLY\"\"\"\n",
    "    print(\"üöÄ Starting TRUE BASE GENERATION SYSTEM\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚ùå Knowledge Graphs: DISABLED\")\n",
    "    print(\"‚ùå Location Coordinate Lookup: DISABLED\") \n",
    "    print(\"‚ùå External APIs: DISABLED\")\n",
    "    print(\"‚ùå RAG Text Retrieval: DISABLED\")\n",
    "    print(\"‚ùå All External Knowledge Sources: DISABLED\")\n",
    "    print(\"‚úÖ Pure LLM Generation from Text Only: ENABLED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        return\n",
    "    \n",
    "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
    "    if not domain_text:\n",
    "        print(\"‚ö†Ô∏è  No input file found, using sample text\")\n",
    "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC. \n",
    "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes. \n",
    "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
    "    else:\n",
    "        print(f\"üìÑ Using text from {INPUT_TEXT_FILE}\")\n",
    "        print(f\"üìù Text length: {len(domain_text)} characters\")\n",
    "    \n",
    "    base_system = TrueBaseGenerationSystem()\n",
    "    llm = initialize_llm(api_key)\n",
    "    \n",
    "    if not llm:\n",
    "        return\n",
    "    \n",
    "    token_count = base_system.chunker.count_tokens(domain_text)\n",
    "    print(f\"üî¢ Total tokens in text: {token_count:,}\")\n",
    "    \n",
    "    if token_count > 15000:\n",
    "        print(\"üìä Text is large, chunking into smaller pieces...\")\n",
    "        chunks = base_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=15000)\n",
    "        print(f\"üìÑ Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"üìÑ Text is small enough to process as single chunk\")\n",
    "        chunks = [domain_text]\n",
    "    \n",
    "    # Process chunks with TRUE base generation\n",
    "    all_turtle_outputs = []\n",
    "    \n",
    "    print(\"\\nüîÑ Processing chunks with TRUE BASE GENERATION...\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\nüîÑ Processing chunk {i}/{len(chunks)}...\")\n",
    "        \n",
    "        turtle_output = base_system.process_chunk_true_base(chunk, i, llm)\n",
    "        if turtle_output:\n",
    "            all_turtle_outputs.append(turtle_output)\n",
    "        \n",
    "        if i < len(chunks):\n",
    "            time.sleep(0.5)  # Brief pause between chunks\n",
    "    \n",
    "    # Save RDF output\n",
    "    if all_turtle_outputs:\n",
    "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        final_output = prefixes + \"# TRUE BASE GENERATION - No External Knowledge Sources\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
    "        \n",
    "        with open(OUTPUT_BASE_TTL, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_output)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Saved TRUE BASE GENERATION RDF to {OUTPUT_BASE_TTL}\")\n",
    "        print(f\"üìä TRUE BASE GENERATION Statistics:\")\n",
    "        print(f\"   - Generation Mode: PURE BASE (No External Knowledge)\")\n",
    "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
    "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
    "        print(f\"   - Events extracted: {base_system.stats['events_extracted']}\")\n",
    "        print(f\"   - External API calls: {base_system.stats['external_api_calls']} (should be 0)\")\n",
    "        print(f\"   - Knowledge sources used: {base_system.stats['knowledge_sources_used']} (should be 0)\")\n",
    "        print(f\"   - Knowledge Graph queries: 0 (DISABLED)\")\n",
    "        print(f\"   - Location coordinate lookups: 0 (DISABLED)\")\n",
    "        print(f\"   - RAG text retrievals: 0 (DISABLED)\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample of TRUE BASE GENERATION RDF:\")\n",
    "        print(\"=\"*60)\n",
    "        print(final_output[:800] + \"...\" if len(final_output) > 800 else final_output)\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüéØ VERIFICATION:\")\n",
    "        print(f\"   ‚úÖ No external knowledge was used\")\n",
    "        print(f\"   ‚úÖ No API calls were made\")\n",
    "        print(f\"   ‚úÖ Only text-based pattern matching was used\")\n",
    "        print(f\"   ‚úÖ LLM used only information from the input text\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No events were extracted from any chunks\")\n",
    "        print(\"üí° This might be because:\")\n",
    "        print(\"   - The text doesn't contain clear historical events\")\n",
    "        print(\"   - The base generation approach is more conservative\")\n",
    "        print(\"   - Without external knowledge, fewer entities were recognized\")\n",
    "    \n",
    "    print(f\"\\nüéâ TRUE BASE GENERATION complete!\")\n",
    "    print(f\"üìÑ Output file: {OUTPUT_BASE_TTL}\")\n",
    "    print(f\"üîç This output contains ONLY information from your text, no external knowledge\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
