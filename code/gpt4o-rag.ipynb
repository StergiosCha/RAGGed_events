{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt-4o RAG\n",
    "\n",
    "\"\"\"\n",
    "Enhanced Multi-Knowledge Graph RAG System with Text Chunking\n",
    "Handles large texts by processing them in chunks to avoid token limits\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal\n",
    "from rdflib.namespace import XSD, SKOS\n",
    "\n",
    "# Configuration\n",
    "INPUT_TEXT_FILE = \"part_aa\"\n",
    "ONTOLOGY_PATH = \"wiki.owl\"\n",
    "LOCATION_ONTOLOGY_PATH = \"locations.owl\"\n",
    "OUTPUT_RAG_TTL = 'extracted_events_rag_with_multi_kg.ttl'\n",
    "OUTPUT_RAG_OWL = 'extracted_events_rag_with_multi_kg.owl'\n",
    "KG_CACHE_FILE = 'kg_cache.json'\n",
    "LOCATION_CACHE_FILE = 'location_cache.json'\n",
    "KG_ANALYSIS_REPORT = 'multi_kg_analysis_report.txt'\n",
    "\n",
    "# Token limits\n",
    "MAX_TOKENS_PER_REQUEST = 100000  # Conservative limit for GPT-4\n",
    "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
    "\n",
    "# Namespaces\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "STE = Namespace(\"http://www.example.org/ste#\")\n",
    "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
    "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
    "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
    "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
    "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
    "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Imports\n",
    "try:\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.schema import HumanMessage\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    print(\"pip install rdflib python-dotenv langchain langchain-openai langchain-community faiss-cpu sentence-transformers tiktoken requests\")\n",
    "    exit(1)\n",
    "\n",
    "@dataclass\n",
    "class LocationInfo:\n",
    "    \"\"\"Location information with coordinates\"\"\"\n",
    "    name: str\n",
    "    latitude: Optional[float] = None\n",
    "    longitude: Optional[float] = None\n",
    "    country: Optional[str] = None\n",
    "    region: Optional[str] = None\n",
    "    source: str = \"extracted\"\n",
    "    confidence: float = 1.0\n",
    "    uri: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class EnhancedKnowledgeFact:\n",
    "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    source: str\n",
    "    confidence: float = 1.0\n",
    "    context: Optional[str] = None\n",
    "    temporal: Optional[str] = None\n",
    "    spatial: Optional[str] = None\n",
    "    evidence_score: float = 1.0\n",
    "    source_uri: Optional[str] = None\n",
    "\n",
    "class LocationExtractor:\n",
    "    \"\"\"Extracts and enriches location information\"\"\"\n",
    "    \n",
    "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
    "        self.ontology_path = ontology_path\n",
    "        self.location_graph = None\n",
    "        self.location_cache = self._load_location_cache()\n",
    "        self.load_location_ontology()\n",
    "        \n",
    "    def _load_location_cache(self) -> Dict:\n",
    "        \"\"\"Load location cache\"\"\"\n",
    "        if os.path.exists(LOCATION_CACHE_FILE):\n",
    "            try:\n",
    "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load location cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_location_cache(self):\n",
    "        \"\"\"Save location cache\"\"\"\n",
    "        try:\n",
    "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save location cache: {e}\")\n",
    "    \n",
    "    def load_location_ontology(self):\n",
    "        \"\"\"Load locations.owl ontology\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.ontology_path):\n",
    "                self.location_graph = Graph()\n",
    "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
    "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
    "            else:\n",
    "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
    "                self.location_graph = None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading location ontology: {e}\")\n",
    "            self.location_graph = None\n",
    "    \n",
    "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract potential location names from text\"\"\"\n",
    "        # Pattern for location names (capitalized words, possibly with common location indicators)\n",
    "        location_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
    "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'  # General capitalized names\n",
    "        ]\n",
    "        \n",
    "        locations = []\n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            locations.extend(matches)\n",
    "        \n",
    "        # Filter and clean\n",
    "        location_stopwords = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', \n",
    "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
    "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', \n",
    "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday', \n",
    "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "        }\n",
    "        \n",
    "        filtered_locations = []\n",
    "        for loc in locations:\n",
    "            loc = loc.strip()\n",
    "            if (loc not in location_stopwords and len(loc) > 2 and \n",
    "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
    "                filtered_locations.append(loc)\n",
    "        \n",
    "        return list(set(filtered_locations))\n",
    "    \n",
    "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location info from local ontology\"\"\"\n",
    "        if not self.location_graph:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Query the ontology for location information\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
    "                ?location rdfs:label ?label .\n",
    "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
    "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
    "                OPTIONAL {{ ?location geo:long ?long }}\n",
    "                OPTIONAL {{ ?location dbp:country ?country }}\n",
    "                OPTIONAL {{ ?location dbp:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            results = self.location_graph.query(query)\n",
    "            for row in results:\n",
    "                return LocationInfo(\n",
    "                    name=location_name,\n",
    "                    latitude=float(row.lat) if row.lat else None,\n",
    "                    longitude=float(row.long) if row.long else None,\n",
    "                    country=str(row.country) if row.country else None,\n",
    "                    region=str(row.region) if row.region else None,\n",
    "                    source=\"local_ontology\",\n",
    "                    uri=str(row.location) if row.location else None\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            # Try to find the DBpedia resource\n",
    "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
    "                <{entity_uri}> geo:lat ?lat ;\n",
    "                               geo:long ?long .\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                bindings = data.get('results', {}).get('bindings', [])\n",
    "                \n",
    "                if bindings:\n",
    "                    binding = bindings[0]\n",
    "                    return LocationInfo(\n",
    "                        name=location_name,\n",
    "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
    "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
    "                        country=binding.get('country', {}).get('value', ''),\n",
    "                        region=binding.get('region', {}).get('value', ''),\n",
    "                        source=\"dbpedia\",\n",
    "                        uri=entity_uri\n",
    "                    )\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from Wikidata\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
    "              ?item rdfs:label \"{location_name}\"@en .\n",
    "              ?item wdt:P625 ?coord .\n",
    "              OPTIONAL {{ ?item wdt:P17 ?country }}\n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "            }}\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                bindings = data.get('results', {}).get('bindings', [])\n",
    "                \n",
    "                if bindings:\n",
    "                    binding = bindings[0]\n",
    "                    coord_str = binding.get('coord', {}).get('value', '')\n",
    "                    \n",
    "                    # Parse coordinate string (format: \"Point(longitude latitude)\")\n",
    "                    coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
    "                    if coord_match:\n",
    "                        longitude = float(coord_match.group(1))\n",
    "                        latitude = float(coord_match.group(2))\n",
    "                        \n",
    "                        return LocationInfo(\n",
    "                            name=location_name,\n",
    "                            latitude=latitude,\n",
    "                            longitude=longitude,\n",
    "                            country=binding.get('countryLabel', {}).get('value', ''),\n",
    "                            source=\"wikidata\",\n",
    "                            uri=binding.get('item', {}).get('value', '')\n",
    "                        )\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
    "        # Check cache first\n",
    "        if location_name in self.location_cache:\n",
    "            cached = self.location_cache[location_name]\n",
    "            return LocationInfo(**cached) if cached else None\n",
    "        \n",
    "        # Try different sources in order of preference\n",
    "        location_info = None\n",
    "        \n",
    "        # 1. Try local ontology first\n",
    "        location_info = self.get_location_from_ontology(location_name)\n",
    "        \n",
    "        # 2. Try Wikidata\n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_wikidata(location_name)\n",
    "        \n",
    "        # 3. Try DBpedia\n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_dbpedia(location_name)\n",
    "        \n",
    "        # Cache the result (even if None)\n",
    "        if location_info:\n",
    "            self.location_cache[location_name] = {\n",
    "                'name': location_info.name,\n",
    "                'latitude': location_info.latitude,\n",
    "                'longitude': location_info.longitude,\n",
    "                'country': location_info.country,\n",
    "                'region': location_info.region,\n",
    "                'source': location_info.source,\n",
    "                'confidence': location_info.confidence,\n",
    "                'uri': location_info.uri\n",
    "            }\n",
    "        else:\n",
    "            self.location_cache[location_name] = None\n",
    "        \n",
    "        self._save_location_cache()\n",
    "        return location_info\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"Handles text chunking to manage token limits\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-4\"):\n",
    "        self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_text_by_tokens(self, text: str, max_tokens: int = 15000, overlap_tokens: int = 200) -> List[str]:\n",
    "        \"\"\"Chunk text by token count with overlap\"\"\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        chunks = []\n",
    "        \n",
    "        start = 0\n",
    "        while start < len(tokens):\n",
    "            end = min(start + max_tokens, len(tokens))\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "            if end >= len(tokens):\n",
    "                break\n",
    "            \n",
    "            # Move start back by overlap amount\n",
    "            start = end - overlap_tokens\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 15000) -> List[str]:\n",
    "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class BaseKGConnector:\n",
    "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
    "        self.name = name\n",
    "        self.base_url = base_url\n",
    "        self.rate_limit = rate_limit\n",
    "        self.last_request_time = 0\n",
    "        self.request_count = 0\n",
    "        self.success_count = 0\n",
    "        \n",
    "    def _rate_limit_wait(self):\n",
    "        \"\"\"Enforce rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit:\n",
    "            time.sleep(self.rate_limit - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get connector statistics\"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'requests': self.request_count,\n",
    "            'successes': self.success_count,\n",
    "            'success_rate': self.success_count / max(1, self.request_count)\n",
    "        }\n",
    "    \n",
    "    def retrieve_facts(self, entity: str, limit: int = 50) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class EnhancedWikidataConnector(BaseKGConnector):\n",
    "    \"\"\"Wikidata connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 50) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from Wikidata\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
    "              {{\n",
    "                ?subject ?label \"{entity}\"@en .\n",
    "              }} UNION {{\n",
    "                ?subject rdfs:label \"{entity}\"@en .\n",
    "              }}\n",
    "              \n",
    "              ?subject ?predicate ?object .\n",
    "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
    "              \n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
    "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
    "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
    "                        source=self.name,\n",
    "                        confidence=0.9,\n",
    "                        source_uri=binding.get('subject', {}).get('value')\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
    "                return facts\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Wikidata query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedDBpediaConnector(BaseKGConnector):\n",
    "    \"\"\"DBpedia connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 50) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from DBpedia\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
    "              <{entity_uri}> ?predicate ?object .\n",
    "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
    "              FILTER(!isBlank(?object))\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    predicate = binding.get('predicate', {}).get('value', '')\n",
    "                    obj = binding.get('object', {}).get('value', '')\n",
    "                    \n",
    "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
    "                    \n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=entity,\n",
    "                        predicate=predicate_name,\n",
    "                        object=obj,\n",
    "                        source=self.name,\n",
    "                        confidence=0.85,\n",
    "                        source_uri=entity_uri\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
    "                return facts\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"DBpedia query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedConceptNetConnector(BaseKGConnector):\n",
    "    \"\"\"ConceptNet connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 50) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from ConceptNet\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            concept = f\"/c/en/{entity.lower().replace(' ', '_')}\"\n",
    "            url = f\"{self.base_url}{concept}?limit={limit}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for edge in data.get('edges', []):\n",
    "                    start = edge.get('start', {})\n",
    "                    end = edge.get('end', {})\n",
    "                    relation = edge.get('rel', {})\n",
    "                    weight = edge.get('weight', 1.0)\n",
    "                    \n",
    "                    start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "                    end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "                    rel_label = relation.get('label', 'related_to')\n",
    "                    \n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=start_label,\n",
    "                        predicate=rel_label,\n",
    "                        object=end_label,\n",
    "                        source=self.name,\n",
    "                        confidence=min(weight, 1.0)\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from ConceptNet for '{entity}'\")\n",
    "                return facts\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class MultiKGCache:\n",
    "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "        \n",
    "    def _load_cache(self) -> Dict:\n",
    "        \"\"\"Load cache from file\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save cache: {e}\")\n",
    "    \n",
    "    def get_cache_key(self, source: str, entity: str) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
    "    \n",
    "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Get cached facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
    "        \"\"\"Cache facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        serializable_facts = []\n",
    "        for fact in facts:\n",
    "            serializable_facts.append({\n",
    "                'subject': fact.subject,\n",
    "                'predicate': fact.predicate,\n",
    "                'object': fact.object,\n",
    "                'source': fact.source,\n",
    "                'confidence': fact.confidence,\n",
    "                'context': fact.context,\n",
    "                'temporal': fact.temporal,\n",
    "                'spatial': fact.spatial,\n",
    "                'evidence_score': fact.evidence_score,\n",
    "                'source_uri': fact.source_uri\n",
    "            })\n",
    "        self.cache[key] = serializable_facts\n",
    "        self._save_cache()\n",
    "\n",
    "class EnhancedMultiKGRAGSystem:\n",
    "    \"\"\"Multi-Knowledge Graph RAG system with chunking and location extraction\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connectors = {\n",
    "            'wikidata': EnhancedWikidataConnector(),\n",
    "            'dbpedia': EnhancedDBpediaConnector(),\n",
    "            'conceptnet': EnhancedConceptNetConnector()\n",
    "        }\n",
    "        self.cache = MultiKGCache()\n",
    "        self.chunker = TextChunker()\n",
    "        self.location_extractor = LocationExtractor()\n",
    "        self.stats = {\n",
    "            'queries_processed': 0,\n",
    "            'entities_extracted': 0,\n",
    "            'facts_retrieved': 0,\n",
    "            'cache_hits': 0,\n",
    "            'chunks_processed': 0,\n",
    "            'locations_found': 0,\n",
    "            'locations_with_coordinates': 0\n",
    "        }\n",
    "        \n",
    "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entities from text\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Pattern for capitalized names\n",
    "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
    "        matches = re.findall(pattern, text)\n",
    "        entities.extend(matches)\n",
    "        \n",
    "        # Filter out common stop words\n",
    "        stop_words = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
    "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
    "        }\n",
    "        \n",
    "        filtered_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.strip()\n",
    "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
    "                filtered_entities.append(entity)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for entity in filtered_entities:\n",
    "            if entity.lower() not in seen:\n",
    "                seen.add(entity.lower())\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities[:15]  # Limit to prevent too many API calls\n",
    "    \n",
    "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
    "        \"\"\"Retrieve facts from knowledge graphs\"\"\"\n",
    "        all_facts = {}\n",
    "        cache_hits = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for entity in entities:\n",
    "                for source_name, connector in self.connectors.items():\n",
    "                    # Check cache first\n",
    "                    cached_facts = self.cache.get(source_name, entity)\n",
    "                    if cached_facts:\n",
    "                        cache_hits += 1\n",
    "                        if entity not in all_facts:\n",
    "                            all_facts[entity] = []\n",
    "                        for fact_data in cached_facts:\n",
    "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
    "                            all_facts[entity].append(fact)\n",
    "                    else:\n",
    "                        future = executor.submit(connector.retrieve_facts, entity, 3)\n",
    "                        futures[future] = (entity, source_name)\n",
    "            \n",
    "            # Collect results\n",
    "            for future in as_completed(futures, timeout=30):\n",
    "                entity, source_name = futures[future]\n",
    "                try:\n",
    "                    facts = future.result()\n",
    "                    if facts:\n",
    "                        self.cache.set(source_name, entity, facts)\n",
    "                        \n",
    "                        if entity not in all_facts:\n",
    "                            all_facts[entity] = []\n",
    "                        all_facts[entity].extend(facts)\n",
    "                        \n",
    "                        self.stats['facts_retrieved'] += len(facts)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to retrieve facts from {source_name} for {entity}: {e}\")\n",
    "        \n",
    "        self.stats['cache_hits'] += cache_hits\n",
    "        return all_facts\n",
    "    \n",
    "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
    "        \"\"\"Format KG facts into context string\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for entity, facts in kg_facts.items():\n",
    "            if facts:\n",
    "                # Sort facts by confidence\n",
    "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
    "                \n",
    "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
    "                \n",
    "                # Group facts by source\n",
    "                by_source = {}\n",
    "                for fact in sorted_facts[:3]:  # Limit facts per entity\n",
    "                    if fact.source not in by_source:\n",
    "                        by_source[fact.source] = []\n",
    "                    by_source[fact.source].append(fact)\n",
    "                \n",
    "                for source, source_facts in by_source.items():\n",
    "                    context_parts.append(f\"\\nFrom {source}:\")\n",
    "                    for fact in source_facts[:2]:  # Limit facts per source\n",
    "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
    "                        if fact.confidence < 0.8:\n",
    "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
    "                        context_parts.append(fact_str)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def process_chunk(self, chunk: str, chunk_num: int, llm) -> str:\n",
    "        \"\"\"Process a single chunk of text with location extraction\"\"\"\n",
    "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars)\")\n",
    "        \n",
    "        # Extract entities from this chunk\n",
    "        entities = self.extract_entities_advanced(chunk)\n",
    "        \n",
    "        # Extract locations from this chunk\n",
    "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
    "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
    "        \n",
    "        # Enrich locations with coordinates\n",
    "        enriched_locations = {}\n",
    "        for location_name in locations[:10]:  # Limit to prevent too many API calls\n",
    "            location_info = self.location_extractor.enrich_location(location_name)\n",
    "            if location_info:\n",
    "                enriched_locations[location_name] = location_info\n",
    "                self.stats['locations_found'] += 1\n",
    "                if location_info.latitude and location_info.longitude:\n",
    "                    self.stats['locations_with_coordinates'] += 1\n",
    "        \n",
    "        if not entities and not enriched_locations:\n",
    "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
    "            return \"\"\n",
    "        \n",
    "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
    "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
    "        \n",
    "        # Get KG facts for entities in this chunk\n",
    "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
    "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
    "        \n",
    "        # Format location context\n",
    "        location_context = self.format_location_context(enriched_locations)\n",
    "        \n",
    "        # Create prompt for this chunk\n",
    "        enhanced_prompt = f\"\"\"You are extracting historical events from this specific text chunk. Use the knowledge graph facts and location information to enhance your extraction.\n",
    "\n",
    "TEXT CHUNK {chunk_num}:\n",
    "{chunk}\n",
    "\n",
    "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
    "{kg_context}\n",
    "\n",
    "LOCATION INFORMATION WITH COORDINATES:\n",
    "{location_context}\n",
    "\n",
    "TASK: Extract ONLY the events that are actually mentioned in this text chunk. Use the KG facts and location coordinates to enhance details but stay faithful to what's actually in the text.\n",
    "\n",
    "Requirements:\n",
    "1. Extract ONLY events mentioned in this text chunk\n",
    "2. Use proper RDF/Turtle format\n",
    "3. Include these properties for each event:\n",
    "   - ste:hasType (description of event)\n",
    "   - ste:hasAgent (who caused/led the event)\n",
    "   - ste:hasTime (when it happened)\n",
    "   - ste:hasLocation (location name from text)\n",
    "   - ste:hasLatitude (latitude coordinate if available)\n",
    "   - ste:hasLongitude (longitude coordinate if available)\n",
    "   - ste:hasCountry (country if available)\n",
    "   - ste:hasResult (outcome/consequence)\n",
    "\n",
    "Output format (do not include prefixes, they will be added later):\n",
    "```turtle\n",
    "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
    "    ste:hasType \"specific description from text\" ;\n",
    "    ste:hasAgent \"specific person from text\" ;\n",
    "    ste:hasTime \"specific date from text\" ;\n",
    "    ste:hasLocation \"specific location from text\" ;\n",
    "    ste:hasLatitude \"latitude_value\"^^xsd:double ;\n",
    "    ste:hasLongitude \"longitude_value\"^^xsd:double ;\n",
    "    ste:hasCountry \"country_name\" ;\n",
    "    ste:hasResult \"specific outcome from text\" .\n",
    "\n",
    "ste:Location{chunk_num}_1 a ste:Location ;\n",
    "    rdfs:label \"location_name\" ;\n",
    "    geo:lat \"latitude_value\"^^xsd:double ;\n",
    "    geo:long \"longitude_value\"^^xsd:double ;\n",
    "    ste:hasCountry \"country_name\" .\n",
    "```\n",
    "\n",
    "IMPORTANT: \n",
    "- Only extract events that are explicitly mentioned in this chunk\n",
    "- Use the exact coordinates provided in the location information\n",
    "- Only include coordinate properties if coordinates are available\n",
    "- If no clear events are found, return empty\n",
    "- Create separate location entities for places mentioned in events\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke([HumanMessage(content=enhanced_prompt)])\n",
    "            turtle_output = self.clean_turtle(response.content)\n",
    "            self.stats['chunks_processed'] += 1\n",
    "            return turtle_output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing chunk {chunk_num}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
    "        \"\"\"Format location information into context string\"\"\"\n",
    "        if not enriched_locations:\n",
    "            return \"No location coordinates available.\"\n",
    "        \n",
    "        context_parts = [\"\\n=== Location Information ===\"]\n",
    "        \n",
    "        for location_name, location_info in enriched_locations.items():\n",
    "            context_parts.append(f\"\\n{location_name}:\")\n",
    "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
    "            \n",
    "            if location_info.latitude and location_info.longitude:\n",
    "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
    "            else:\n",
    "                context_parts.append(\"  - Coordinates: Not available\")\n",
    "            \n",
    "            if location_info.country:\n",
    "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
    "            \n",
    "            if location_info.region:\n",
    "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
    "            \n",
    "            if location_info.uri:\n",
    "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def clean_turtle(self, raw_output: str) -> str:\n",
    "        \"\"\"Clean turtle output\"\"\"\n",
    "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "        \n",
    "        lines = raw_output.strip().split('\\n')\n",
    "        turtle_lines = []\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if (stripped.startswith('@') or stripped.startswith('<') or \n",
    "                stripped.startswith(':') or stripped.startswith('_') or \n",
    "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
    "                turtle_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(turtle_lines).strip() if turtle_lines else raw_output.strip()\n",
    "\n",
    "def load_api_key():\n",
    "    \"\"\"Load OpenAI API key\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: OPENAI_API_KEY not found\")\n",
    "        return None\n",
    "    print(\"OpenAI API Key loaded successfully.\")\n",
    "    return api_key\n",
    "\n",
    "def load_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Load text from file\"\"\"\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        print(f\"Loaded text from {filepath}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def initialize_llm(api_key: str):\n",
    "    \"\"\"Initialize LLM\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    try:\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, openai_api_key=api_key)\n",
    "        print(\"LLM initialized successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with chunking support\"\"\"\n",
    "    print(\"ðŸš€ Starting Multi-Knowledge Graph RAG System with Chunking\")\n",
    "    \n",
    "    # Load API key\n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        return\n",
    "    \n",
    "    # Load YOUR actual text\n",
    "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
    "    if not domain_text:\n",
    "        print(\"âš ï¸  No input file found, using sample text\")\n",
    "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC. \n",
    "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes. \n",
    "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
    "    else:\n",
    "        print(f\"ðŸ“„ Using YOUR text from {INPUT_TEXT_FILE}\")\n",
    "        print(f\"ðŸ“ Text length: {len(domain_text)} characters\")\n",
    "    \n",
    "    # Initialize systems\n",
    "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
    "    llm = initialize_llm(api_key)\n",
    "    \n",
    "    if not llm:\n",
    "        return\n",
    "    \n",
    "    # Check if text needs chunking\n",
    "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
    "    print(f\"ðŸ”¢ Total tokens in text: {token_count:,}\")\n",
    "    \n",
    "    if token_count > 15000:  # Conservative chunking threshold\n",
    "        print(\"ðŸ“Š Text is large, chunking into smaller pieces...\")\n",
    "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=15000)\n",
    "        print(f\"ðŸ“„ Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"ðŸ“„ Text is small enough to process as single chunk\")\n",
    "        chunks = [domain_text]\n",
    "    \n",
    "    # Process each chunk\n",
    "    all_turtle_outputs = []\n",
    "    all_entities = set()\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\nðŸ”„ Processing chunk {i}/{len(chunks)}...\")\n",
    "        \n",
    "        turtle_output = multi_kg_system.process_chunk(chunk, i, llm)\n",
    "        if turtle_output:\n",
    "            all_turtle_outputs.append(turtle_output)\n",
    "            \n",
    "        # Extract entities for statistics\n",
    "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
    "        all_entities.update(chunk_entities)\n",
    "        \n",
    "        # Small delay to be respectful to APIs\n",
    "        if i < len(chunks):\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Combine all outputs\n",
    "    if all_turtle_outputs:\n",
    "        # Add prefixes\n",
    "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
    "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
    "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        final_output = prefixes + \"\\n\\n\".join(all_turtle_outputs)\n",
    "        \n",
    "        # Save output\n",
    "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_output)\n",
    "        \n",
    "        print(f\"\\nâœ… Saved enhanced RDF to {OUTPUT_RAG_TTL}\")\n",
    "        print(f\"ðŸ“Š Processing Statistics:\")\n",
    "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
    "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
    "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
    "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
    "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
    "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
    "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
    "        \n",
    "        # Show connector statistics\n",
    "        print(f\"\\nðŸ”— Knowledge Graph Connector Statistics:\")\n",
    "        for name, connector in multi_kg_system.connectors.items():\n",
    "            stats = connector.get_stats()\n",
    "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
    "        \n",
    "        # Show location extraction statistics\n",
    "        if multi_kg_system.location_extractor.location_cache:\n",
    "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
    "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
    "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
    "        \n",
    "        # Show sample of output\n",
    "        print(f\"\\nðŸ“ Sample of generated RDF:\")\n",
    "        print(\"=\"*60)\n",
    "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No events were extracted from any chunks\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Process complete! Check {OUTPUT_RAG_TTL} for results based on YOUR input text.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
