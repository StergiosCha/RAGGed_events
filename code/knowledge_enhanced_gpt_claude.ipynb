{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ea10cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 22:59:09,972 - INFO - Loaded location ontology from locations.owl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Multi-Knowledge Graph System with Chunking (RAG DEACTIVATED)\n",
      "OpenAI API Key loaded successfully.\n",
      "Loaded text from grover3.txt\n",
      "üìÑ Using YOUR text from grover3.txt\n",
      "üìù Text length: 57806 characters\n",
      "LLM initialized successfully.\n",
      "\n",
      "‚ùå RAG vector store setup SKIPPED (RAG is DEACTIVATED)\n",
      "üî¢ Total tokens in text: 22,227\n",
      "üìä Text is large, chunking into smaller pieces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 22:59:10,577 - INFO - Processing chunk 1 (35923 chars) - RAG DISABLED\n",
      "2025-05-27 22:59:10,579 - INFO - Found potential locations in chunk 1: ['Adidas', 'Giorgo Agamben', 'Exhibition', 'Pictures', 'Mussorgsky', 'Lutrinae']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Created 2 chunks\n",
      "\n",
      "üîÑ Processing chunks for event extraction (without RAG)...\n",
      "\n",
      "üîÑ Processing chunk 1/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 22:59:17,094 - INFO - Registered new location: Exhibition\n",
      "2025-05-27 22:59:24,291 - INFO - Found entities in chunk 1: ['Giorgo Agamben', 'Pictures', 'Exhibition', 'Mussorgsky', 'Lutrinae']...\n",
      "2025-05-27 22:59:24,292 - INFO - Enriched 1 locations with coordinates\n",
      "2025-05-27 22:59:24,526 - INFO - Retrieved 0 facts from DBpedia for 'Giorgo Agamben'\n",
      "2025-05-27 22:59:24,588 - INFO - Retrieved 0 facts from Wikidata for 'Giorgo Agamben'\n",
      "2025-05-27 22:59:34,747 - INFO - KG retrieval completed: 3/3 successful, 15 cache hits\n",
      "2025-05-27 23:00:01,854 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-27 23:00:01,872 - INFO - Generated RDF for chunk 1 (without RAG)\n",
      "2025-05-27 23:00:02,878 - INFO - Processing chunk 2 (21542 chars) - RAG DISABLED\n",
      "2025-05-27 23:00:02,885 - INFO - Found potential locations in chunk 2: ['Seven', 'Tsiolis']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 2/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 23:00:03,961 - INFO - Registered new location: Seven\n",
      "2025-05-27 23:00:05,801 - INFO - Found entities in chunk 2: ['Seven', 'Tsiolis']...\n",
      "2025-05-27 23:00:05,803 - INFO - Enriched 1 locations with coordinates\n",
      "2025-05-27 23:00:06,031 - INFO - Retrieved 0 facts from DBpedia for 'Tsiolis'\n",
      "2025-05-27 23:00:06,093 - INFO - Retrieved 0 facts from Wikidata for 'Tsiolis'\n",
      "2025-05-27 23:00:15,784 - INFO - KG retrieval completed: 3/3 successful, 3 cache hits\n",
      "2025-05-27 23:00:33,529 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-27 23:00:33,566 - INFO - Generated RDF for chunk 2 (without RAG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved RDF to extracted_events_norag_with_multi_kg_G.ttl\n",
      "üìä Processing Statistics:\n",
      "   - Total chunks processed: 2\n",
      "   - Successful chunks: 2\n",
      "   - Unique entities found: 8\n",
      "   - Total KG facts retrieved: 0\n",
      "   - Cache hits: 18\n",
      "   - Locations found: 2\n",
      "   - Locations with coordinates: 2\n",
      "   - Location duplicates avoided: 0\n",
      "   - Unique global locations: 2\n",
      "   - RAG status: DEACTIVATED\n",
      "\n",
      "üîó Knowledge Graph Connector Statistics:\n",
      "   - Wikidata: 2/2 requests (100.0% success)\n",
      "   - DBpedia: 2/2 requests (100.0% success)\n",
      "   - ConceptNet: 0/2 requests (0.0% success)\n",
      "   - Location enrichment: 59/225 locations enriched (26.2% success)\n",
      "\n",
      "üìù Sample of generated RDF:\n",
      "============================================================\n",
      "@prefix ste: <http://www.example.org/ste#> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
      "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
      "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
      "\n",
      "# Historical Events with Knowledge Graph Enhanced Location Data (RAG DEACTIVATED)\n",
      "ste:Event1_1 a ste:Event ;\n",
      "    ste:hasType \"Return to Greece to fulfill military obligations\" ;\n",
      "    ste:hasAgent \"Unnamed narrator\" ;\n",
      "    ste:hasTime \"Late October 2020\" ;\n",
      "    ste:hasLocation \"Greece\" ;\n",
      "    ste:hasResult \"Preparation to serve as a substitute professor at the University of Thessaloniki\" .\n",
      "\n",
      "ste:Event1_2 a ste:Event ;\n",
      "    ste:hasType \"Travel from Sweden to Greece\" ;\n",
      "    ste:hasAgent \"Unnamed narrator\" ;\n",
      "    ste:hasTime \"Late October 2020\" ;\n",
      "    ste:hasLocation \"Thessaloniki\" ;\n",
      "    ste:hasLatitude \"40.6401\"^^xsd:double ;\n",
      "    ste:hasLo...\n",
      "============================================================\n",
      "\n",
      "‚ùå RAG System is DEACTIVATED\n",
      "üí° To enable RAG functionality:\n",
      "   1. Set RAG_ENABLED = True at the top of the script\n",
      "   2. Ensure langchain dependencies are installed\n",
      "   3. Re-run the script\n",
      "\n",
      "üéâ Process complete! Check extracted_events_norag_with_multi_kg_G.ttl for RDF results.\n",
      "üìä System ran in NON-RAG mode - only Knowledge Graph and Location enrichment was used.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Multi-Knowledge Graph RAG System with Text Chunking\n",
    "Handles large texts by processing them in chunks to avoid token limits\n",
    "RAG FUNCTIONALITY DEACTIVATED\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal\n",
    "from rdflib.namespace import XSD, SKOS\n",
    "\n",
    "# Configuration\n",
    "INPUT_TEXT_FILE = \"grover3.txt\"\n",
    "ONTOLOGY_PATH = \"wiki.owl\"\n",
    "LOCATION_ONTOLOGY_PATH = \"locations.owl\"\n",
    "OUTPUT_RAG_TTL = 'extracted_events_norag_with_multi_kg_G.ttl'\n",
    "OUTPUT_RAG_OWL = 'extracted_events_norag_with_multi_kg_G.owl'\n",
    "KG_CACHE_FILE = 'kg_cache.json'\n",
    "LOCATION_CACHE_FILE = 'location_cache.json'\n",
    "KG_ANALYSIS_REPORT = 'multi_kg_analysis_report.txt'\n",
    "\n",
    "# Token limits\n",
    "MAX_TOKENS_PER_REQUEST = 100000  # Conservative limit for GPT-4\n",
    "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
    "\n",
    "# RAG DEACTIVATION FLAG\n",
    "RAG_ENABLED = False  # Set to False to deactivate RAG\n",
    "\n",
    "# Namespaces\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "STE = Namespace(\"http://www.example.org/ste#\")\n",
    "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
    "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
    "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
    "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
    "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
    "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Imports\n",
    "try:\n",
    "    if RAG_ENABLED:\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.schema import HumanMessage\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    print(\"pip install rdflib python-dotenv langchain langchain-openai langchain-community faiss-cpu sentence-transformers tiktoken requests\")\n",
    "    exit(1)\n",
    "\n",
    "@dataclass\n",
    "class LocationInfo:\n",
    "    \"\"\"Location information with coordinates\"\"\"\n",
    "    name: str\n",
    "    latitude: Optional[float] = None\n",
    "    longitude: Optional[float] = None\n",
    "    country: Optional[str] = None\n",
    "    region: Optional[str] = None\n",
    "    source: str = \"extracted\"\n",
    "    confidence: float = 1.0\n",
    "    uri: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class EnhancedKnowledgeFact:\n",
    "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    source: str\n",
    "    confidence: float = 1.0\n",
    "    context: Optional[str] = None\n",
    "    temporal: Optional[str] = None\n",
    "    spatial: Optional[str] = None\n",
    "    evidence_score: float = 1.0\n",
    "    source_uri: Optional[str] = None\n",
    "\n",
    "class LocationExtractor:\n",
    "    \"\"\"Extracts and enriches location information\"\"\"\n",
    "    \n",
    "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
    "        self.ontology_path = ontology_path\n",
    "        self.location_graph = None\n",
    "        self.location_cache = self._load_location_cache()\n",
    "        self.load_location_ontology()\n",
    "        \n",
    "    def _load_location_cache(self) -> Dict:\n",
    "        \"\"\"Load location cache\"\"\"\n",
    "        if os.path.exists(LOCATION_CACHE_FILE):\n",
    "            try:\n",
    "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load location cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_location_cache(self):\n",
    "        \"\"\"Save location cache\"\"\"\n",
    "        try:\n",
    "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save location cache: {e}\")\n",
    "    \n",
    "    def load_location_ontology(self):\n",
    "        \"\"\"Load locations.owl ontology\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.ontology_path):\n",
    "                self.location_graph = Graph()\n",
    "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
    "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
    "            else:\n",
    "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
    "                self.location_graph = None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading location ontology: {e}\")\n",
    "            self.location_graph = None\n",
    "    \n",
    "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract potential location names from text\"\"\"\n",
    "        location_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
    "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
    "        ]\n",
    "        \n",
    "        locations = []\n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            locations.extend(matches)\n",
    "        \n",
    "        location_stopwords = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', \n",
    "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
    "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', \n",
    "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday', \n",
    "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "        }\n",
    "        \n",
    "        filtered_locations = []\n",
    "        for loc in locations:\n",
    "            loc = loc.strip()\n",
    "            if (loc not in location_stopwords and len(loc) > 2 and \n",
    "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
    "                filtered_locations.append(loc)\n",
    "        \n",
    "        return list(set(filtered_locations))\n",
    "    \n",
    "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location info from local ontology\"\"\"\n",
    "        if not self.location_graph:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
    "                ?location rdfs:label ?label .\n",
    "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
    "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
    "                OPTIONAL {{ ?location geo:long ?long }}\n",
    "                OPTIONAL {{ ?location dbp:country ?country }}\n",
    "                OPTIONAL {{ ?location dbp:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            results = self.location_graph.query(query)\n",
    "            for row in results:\n",
    "                return LocationInfo(\n",
    "                    name=location_name,\n",
    "                    latitude=float(row.lat) if row.lat else None,\n",
    "                    longitude=float(row.long) if row.long else None,\n",
    "                    country=str(row.country) if row.country else None,\n",
    "                    region=str(row.region) if row.region else None,\n",
    "                    source=\"local_ontology\",\n",
    "                    uri=str(row.location) if row.location else None\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
    "                <{entity_uri}> geo:lat ?lat ;\n",
    "                               geo:long ?long .\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                bindings = data.get('results', {}).get('bindings', [])\n",
    "                \n",
    "                if bindings:\n",
    "                    binding = bindings[0]\n",
    "                    return LocationInfo(\n",
    "                        name=location_name,\n",
    "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
    "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
    "                        country=binding.get('country', {}).get('value', ''),\n",
    "                        region=binding.get('region', {}).get('value', ''),\n",
    "                        source=\"dbpedia\",\n",
    "                        uri=entity_uri\n",
    "                    )\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Try multiple query strategies to get the right location\n",
    "            queries = [\n",
    "                # Try exact label match first\n",
    "                f\"\"\"\n",
    "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
    "                  ?item rdfs:label \"{location_name}\"@en .\n",
    "                  ?item wdt:P625 ?coord .\n",
    "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
    "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}\n",
    "                LIMIT 5\n",
    "                \"\"\",\n",
    "                # Try with additional filters for places/locations\n",
    "                f\"\"\"\n",
    "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
    "                  ?item rdfs:label \"{location_name}\"@en .\n",
    "                  ?item wdt:P625 ?coord .\n",
    "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
    "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
    "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}\n",
    "                LIMIT 5\n",
    "                \"\"\"\n",
    "            ]\n",
    "            \n",
    "            for query in queries:\n",
    "                params = {'query': query, 'format': 'json'}\n",
    "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    bindings = data.get('results', {}).get('bindings', [])\n",
    "                    \n",
    "                    if bindings:\n",
    "                        # Prefer results with country information\n",
    "                        best_binding = None\n",
    "                        for binding in bindings:\n",
    "                            if binding.get('country'):\n",
    "                                best_binding = binding\n",
    "                                break\n",
    "                        \n",
    "                        if not best_binding:\n",
    "                            best_binding = bindings[0]\n",
    "                        \n",
    "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
    "                        \n",
    "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
    "                        if coord_match:\n",
    "                            longitude = float(coord_match.group(1))\n",
    "                            latitude = float(coord_match.group(2))\n",
    "                            \n",
    "                            return LocationInfo(\n",
    "                                name=location_name,\n",
    "                                latitude=latitude,\n",
    "                                longitude=longitude,\n",
    "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
    "                                source=\"wikidata\",\n",
    "                                uri=best_binding.get('item', {}).get('value', '')\n",
    "                            )\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
    "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
    "        if not location_info.latitude or not location_info.longitude:\n",
    "            return True\n",
    "        \n",
    "        lat, lon = location_info.latitude, location_info.longitude\n",
    "        \n",
    "        # Basic coordinate range validation\n",
    "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
    "            return False\n",
    "        \n",
    "        # Generic geographic validation - flag obviously wrong coordinates\n",
    "        # If coordinates suggest North America but no clear indication it should be there\n",
    "        if (-130 < lon < -60) and (25 < lat < 50):  # North America range\n",
    "            logger.warning(f\"Coordinates for '{location_info.name}' appear to be in North America ({lat}, {lon}). \"\n",
    "                         f\"Please verify if this is correct for your historical context.\")\n",
    "            # Don't auto-correct, just warn - let the user/context decide\n",
    "        \n",
    "        # If coordinates suggest Australia/Oceania for what might be European/Mediterranean names\n",
    "        elif (110 < lon < 180) and (-45 < lat < -10):  # Australia/Oceania range\n",
    "            logger.warning(f\"Coordinates for '{location_info.name}' appear to be in Australia/Oceania ({lat}, {lon}). \"\n",
    "                         f\"Please verify if this is correct for your historical context.\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
    "        if location_name in self.location_cache:\n",
    "            cached = self.location_cache[location_name]\n",
    "            return LocationInfo(**cached) if cached else None\n",
    "        \n",
    "        location_info = None\n",
    "        \n",
    "        location_info = self.get_location_from_ontology(location_name)\n",
    "        \n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_wikidata(location_name)\n",
    "        \n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_dbpedia(location_name)\n",
    "        \n",
    "        if location_info:\n",
    "            self.location_cache[location_name] = {\n",
    "                'name': location_info.name,\n",
    "                'latitude': location_info.latitude,\n",
    "                'longitude': location_info.longitude,\n",
    "                'country': location_info.country,\n",
    "                'region': location_info.region,\n",
    "                'source': location_info.source,\n",
    "                'confidence': location_info.confidence,\n",
    "                'uri': location_info.uri\n",
    "            }\n",
    "        else:\n",
    "            self.location_cache[location_name] = None\n",
    "        \n",
    "        self._save_location_cache()\n",
    "        \n",
    "        if location_info:\n",
    "            self.validate_coordinates(location_info)\n",
    "        \n",
    "        return location_info\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"Handles text chunking to manage token limits\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-4o\"):\n",
    "        self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 15000) -> List[str]:\n",
    "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class BaseKGConnector:\n",
    "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
    "        self.name = name\n",
    "        self.base_url = base_url\n",
    "        self.rate_limit = rate_limit\n",
    "        self.last_request_time = 0\n",
    "        self.request_count = 0\n",
    "        self.success_count = 0\n",
    "        \n",
    "    def _rate_limit_wait(self):\n",
    "        \"\"\"Enforce rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit:\n",
    "            time.sleep(self.rate_limit - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get connector statistics\"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'requests': self.request_count,\n",
    "            'successes': self.success_count,\n",
    "            'success_rate': self.success_count / max(1, self.request_count)\n",
    "        }\n",
    "    \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class EnhancedWikidataConnector(BaseKGConnector):\n",
    "    \"\"\"Wikidata connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
    "              {{\n",
    "                ?subject ?label \"{entity}\"@en .\n",
    "              }} UNION {{\n",
    "                ?subject rdfs:label \"{entity}\"@en .\n",
    "              }}\n",
    "              \n",
    "              ?subject ?predicate ?object .\n",
    "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
    "              \n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
    "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
    "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
    "                        source=self.name,\n",
    "                        confidence=0.9,\n",
    "                        source_uri=binding.get('subject', {}).get('value')\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
    "                return facts\n",
    "            else:\n",
    "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedDBpediaConnector(BaseKGConnector):\n",
    "    \"\"\"DBpedia connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
    "              <{entity_uri}> ?predicate ?object .\n",
    "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
    "              FILTER(!isBlank(?object))\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    predicate = binding.get('predicate', {}).get('value', '')\n",
    "                    obj = binding.get('object', {}).get('value', '')\n",
    "                    \n",
    "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
    "                    \n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=entity,\n",
    "                        predicate=predicate_name,\n",
    "                        object=obj,\n",
    "                        source=self.name,\n",
    "                        confidence=0.85,\n",
    "                        source_uri=entity_uri\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
    "                return facts\n",
    "            else:\n",
    "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedConceptNetConnector(BaseKGConnector):\n",
    "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
    "        \n",
    "    def search_related_concepts(self, entity: str) -> List[str]:\n",
    "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
    "        try:\n",
    "            # Try search API first\n",
    "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
    "            response = requests.get(search_url, timeout=10)\n",
    "            \n",
    "            related_concepts = []\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                for edge in data.get('edges', []):\n",
    "                    start = edge.get('start', {}).get('label', '')\n",
    "                    end = edge.get('end', {}).get('label', '')\n",
    "                    \n",
    "                    # Extract concept paths and clean them\n",
    "                    for concept_path in [start, end]:\n",
    "                        if concept_path and '/c/en/' in concept_path:\n",
    "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
    "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
    "                                related_concepts.append(concept)\n",
    "            \n",
    "            return list(set(related_concepts))[:5]  # Return top 5 unique concepts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
    "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
    "        try:\n",
    "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
    "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data.get('edges', [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            all_facts = []\n",
    "            \n",
    "            # Strategy 1: Try direct query first\n",
    "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
    "            \n",
    "            # Strategy 2: Search for related concepts and query them\n",
    "            related_concepts = self.search_related_concepts(entity)\n",
    "            \n",
    "            # Process direct edges\n",
    "            for edge in direct_edges:\n",
    "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
    "                if fact:\n",
    "                    all_facts.append(fact)\n",
    "            \n",
    "            # Process related concept edges\n",
    "            for concept in related_concepts:\n",
    "                concept_edges = self.query_concept_directly(concept, 5)\n",
    "                for edge in concept_edges:\n",
    "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
    "                    if fact:\n",
    "                        all_facts.append(fact)\n",
    "            \n",
    "            if all_facts:\n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
    "                if related_concepts:\n",
    "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
    "            \n",
    "            return all_facts[:limit]\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
    "        try:\n",
    "            start = edge.get('start', {})\n",
    "            end = edge.get('end', {})\n",
    "            relation = edge.get('rel', {})\n",
    "            weight = edge.get('weight', 1.0)\n",
    "            \n",
    "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "            rel_label = relation.get('label', 'related_to')\n",
    "            \n",
    "            # Skip if labels are empty or too short\n",
    "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
    "                return None\n",
    "            \n",
    "            # Determine confidence based on discovery method\n",
    "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
    "            \n",
    "            return EnhancedKnowledgeFact(\n",
    "                subject=original_entity,\n",
    "                predicate=rel_label,\n",
    "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
    "                source=self.name,\n",
    "                confidence=min(weight * confidence_multiplier, 1.0),\n",
    "                context=f\"Discovered {discovery_method}\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
    "            return None\n",
    "\n",
    "class MultiKGCache:\n",
    "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "        \n",
    "    def _load_cache(self) -> Dict:\n",
    "        \"\"\"Load cache from file\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save cache: {e}\")\n",
    "    \n",
    "    def get_cache_key(self, source: str, entity: str) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
    "    \n",
    "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Get cached facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
    "        \"\"\"Cache facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        serializable_facts = []\n",
    "        for fact in facts:\n",
    "            serializable_facts.append({\n",
    "                'subject': fact.subject,\n",
    "                'predicate': fact.predicate,\n",
    "                'object': fact.object,\n",
    "                'source': fact.source,\n",
    "                'confidence': fact.confidence,\n",
    "                'context': fact.context,\n",
    "                'temporal': fact.temporal,\n",
    "                'spatial': fact.spatial,\n",
    "                'evidence_score': fact.evidence_score,\n",
    "                'source_uri': fact.source_uri\n",
    "            })\n",
    "        self.cache[key] = serializable_facts\n",
    "        self._save_cache()\n",
    "\n",
    "class EnhancedMultiKGRAGSystem:\n",
    "    \"\"\"Multi-Knowledge Graph system with RAG functionality DEACTIVATED\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connectors = {\n",
    "            'wikidata': EnhancedWikidataConnector(),\n",
    "            'dbpedia': EnhancedDBpediaConnector(),\n",
    "            'conceptnet': EnhancedConceptNetConnector()\n",
    "        }\n",
    "        self.cache = MultiKGCache()\n",
    "        self.chunker = TextChunker()\n",
    "        self.location_extractor = LocationExtractor()\n",
    "        self.global_locations = {}\n",
    "        # RAG components deactivated\n",
    "        self.vectorstore = None  \n",
    "        self.document_chunks = []  \n",
    "        self.stats = {\n",
    "            'queries_processed': 0,\n",
    "            'entities_extracted': 0,\n",
    "            'facts_retrieved': 0,\n",
    "            'cache_hits': 0,\n",
    "            'chunks_processed': 0,\n",
    "            'locations_found': 0,\n",
    "            'locations_with_coordinates': 0,\n",
    "            'location_duplicates_avoided': 0,\n",
    "            'rag_queries': 0\n",
    "        }\n",
    "        \n",
    "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entities from text\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
    "        matches = re.findall(pattern, text)\n",
    "        entities.extend(matches)\n",
    "        \n",
    "        stop_words = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
    "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
    "        }\n",
    "        \n",
    "        filtered_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.strip()\n",
    "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
    "                filtered_entities.append(entity)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for entity in filtered_entities:\n",
    "            if entity.lower() not in seen:\n",
    "                seen.add(entity.lower())\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities[:15]\n",
    "    \n",
    "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
    "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
    "        all_facts = {}\n",
    "        cache_hits = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for entity in entities:\n",
    "                for source_name, connector in self.connectors.items():\n",
    "                    # Check cache first\n",
    "                    cached_facts = self.cache.get(source_name, entity)\n",
    "                    if cached_facts:\n",
    "                        cache_hits += 1\n",
    "                        if entity not in all_facts:\n",
    "                            all_facts[entity] = []\n",
    "                        for fact_data in cached_facts:\n",
    "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
    "                            all_facts[entity].append(fact)\n",
    "                    else:\n",
    "                        future = executor.submit(connector.retrieve_facts, entity, 3)\n",
    "                        futures[future] = (entity, source_name)\n",
    "            \n",
    "            # Collect results with better timeout handling\n",
    "            completed = 0\n",
    "            total_futures = len(futures)\n",
    "            \n",
    "            try:\n",
    "                for future in as_completed(futures, timeout=45):  # Increased timeout\n",
    "                    entity, source_name = futures[future]\n",
    "                    completed += 1\n",
    "                    \n",
    "                    try:\n",
    "                        facts = future.result(timeout=5)  # Individual future timeout\n",
    "                        if facts:\n",
    "                            self.cache.set(source_name, entity, facts)\n",
    "                            \n",
    "                            if entity not in all_facts:\n",
    "                                all_facts[entity] = []\n",
    "                            all_facts[entity].extend(facts)\n",
    "                            \n",
    "                            self.stats['facts_retrieved'] += len(facts)\n",
    "                        \n",
    "                        logger.debug(f\"‚úÖ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ùå {source_name} failed for {entity}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except TimeoutError:\n",
    "                pending_count = total_futures - completed\n",
    "                logger.warning(f\"‚è∞ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
    "                \n",
    "                # Cancel remaining futures\n",
    "                for future in futures:\n",
    "                    if not future.done():\n",
    "                        future.cancel()\n",
    "        \n",
    "        self.stats['cache_hits'] += cache_hits\n",
    "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
    "        return all_facts\n",
    "    \n",
    "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
    "        \"\"\"Format KG facts into context string\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for entity, facts in kg_facts.items():\n",
    "            if facts:\n",
    "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
    "                \n",
    "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
    "                \n",
    "                by_source = {}\n",
    "                for fact in sorted_facts[:3]:\n",
    "                    if fact.source not in by_source:\n",
    "                        by_source[fact.source] = []\n",
    "                    by_source[fact.source].append(fact)\n",
    "                \n",
    "                for source, source_facts in by_source.items():\n",
    "                    context_parts.append(f\"\\nFrom {source}:\")\n",
    "                    for fact in source_facts[:2]:\n",
    "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
    "                        if fact.confidence < 0.8:\n",
    "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
    "                        context_parts.append(fact_str)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
    "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
    "        location_key = location_info.name.lower().strip()\n",
    "        \n",
    "        if location_key in self.global_locations:\n",
    "            existing = self.global_locations[location_key]\n",
    "            if (location_info.latitude and location_info.longitude and \n",
    "                (not existing.latitude or not existing.longitude)):\n",
    "                self.global_locations[location_key] = location_info\n",
    "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
    "            else:\n",
    "                self.stats['location_duplicates_avoided'] += 1\n",
    "                logger.debug(f\"Location {location_info.name} already registered\")\n",
    "        else:\n",
    "            self.global_locations[location_key] = location_info\n",
    "            logger.info(f\"Registered new location: {location_info.name}\")\n",
    "        \n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
    "        return f\"ste:Location_{clean_name}\"\n",
    "    \n",
    "    def process_chunk(self, chunk: str, chunk_num: int, llm) -> str:\n",
    "        \"\"\"Process a single chunk of text WITHOUT RAG (RAG DEACTIVATED)\"\"\"\n",
    "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars) - RAG DISABLED\")\n",
    "        \n",
    "        # RAG retrieval DEACTIVATED - skip this step\n",
    "        # relevant_context = \"\"\n",
    "        \n",
    "        # Extract entities and locations (this remains the same)\n",
    "        entities = self.extract_entities_advanced(chunk)\n",
    "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
    "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
    "        \n",
    "        # Enrich locations with coordinates\n",
    "        enriched_locations = {}\n",
    "        for location_name in locations[:10]:\n",
    "            location_info = self.location_extractor.enrich_location(location_name)\n",
    "            if location_info:\n",
    "                self.register_global_location(location_info)\n",
    "                enriched_locations[location_name] = location_info\n",
    "                self.stats['locations_found'] += 1\n",
    "                if location_info.latitude and location_info.longitude:\n",
    "                    self.stats['locations_with_coordinates'] += 1\n",
    "        \n",
    "        if not entities and not enriched_locations:\n",
    "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
    "            return \"\"\n",
    "        \n",
    "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
    "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
    "        \n",
    "        # Get KG facts for entities (this remains the same)\n",
    "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
    "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
    "        location_context = self.format_location_context(enriched_locations)\n",
    "        \n",
    "        # SIMPLIFIED PROMPT WITHOUT RAG\n",
    "        simplified_prompt = f\"\"\"You are extracting historical events from text chunks. Use knowledge graph facts and location coordinates to enhance your extraction.\n",
    "\n",
    "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
    "{chunk}\n",
    "\n",
    "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
    "{kg_context}\n",
    "\n",
    "LOCATION INFORMATION WITH COORDINATES:\n",
    "{location_context}\n",
    "\n",
    "TASK: Extract ONLY the events that are actually mentioned in the current text chunk.\n",
    "\n",
    "Requirements:\n",
    "1. Extract ONLY events mentioned in the CURRENT text chunk\n",
    "2. Use KG facts to enhance entity information\n",
    "3. Use location coordinates to provide precise geographical data\n",
    "4. Include ALL these properties for each event:\n",
    "   - ste:hasType (description of event)\n",
    "   - ste:hasAgent (who caused/led the event)\n",
    "   - ste:hasTime (when it happened)\n",
    "   - ste:hasLocation (location name from text)\n",
    "   - ste:hasLatitude (latitude coordinate if available)\n",
    "   - ste:hasLongitude (longitude coordinate if available)\n",
    "   - ste:hasCountry (country if available)\n",
    "   - ste:hasRegion (region if available)\n",
    "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
    "   - ste:hasResult (outcome/consequence)\n",
    "\n",
    "Output format (do not include prefixes, they will be added later):\n",
    "```turtle\n",
    "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
    "    ste:hasType \"specific description from current chunk\" ;\n",
    "    ste:hasAgent \"specific person from current chunk\" ;\n",
    "    ste:hasTime \"specific date from current chunk\" ;\n",
    "    ste:hasLocation \"specific location from current chunk\" ;\n",
    "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
    "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
    "    ste:hasCountry \"Italy\" ;\n",
    "    ste:hasRegion \"Sicily\" ;\n",
    "    ste:hasLocationSource \"wikidata\" ;\n",
    "    ste:hasResult \"specific outcome from current chunk\" .\n",
    "```\n",
    "\n",
    "IMPORTANT: \n",
    "- Extract events ONLY from the CURRENT text chunk\n",
    "- Use KG facts to enrich entity details\n",
    "- Include precise coordinates from location sources\n",
    "- Only extract events explicitly mentioned in the current chunk\n",
    "- If no clear events are found in current chunk, return empty\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke([HumanMessage(content=simplified_prompt)])\n",
    "            turtle_output = self.clean_turtle(response.content)\n",
    "            self.stats['chunks_processed'] += 1\n",
    "            logger.info(f\"Generated RDF for chunk {chunk_num} (without RAG)\")\n",
    "            return turtle_output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing chunk {chunk_num}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
    "        \"\"\"Format location information into context string\"\"\"\n",
    "        if not enriched_locations:\n",
    "            return \"No location coordinates available.\"\n",
    "        \n",
    "        context_parts = [\"\\n=== Location Information ===\"]\n",
    "        \n",
    "        for location_name, location_info in enriched_locations.items():\n",
    "            context_parts.append(f\"\\n{location_name}:\")\n",
    "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
    "            \n",
    "            if location_info.latitude and location_info.longitude:\n",
    "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
    "                if location_info.source == \"corrected\":\n",
    "                    context_parts.append(f\"  - NOTE: Coordinates were corrected for historical accuracy\")\n",
    "            else:\n",
    "                context_parts.append(\"  - Coordinates: Not available\")\n",
    "            \n",
    "            if location_info.country:\n",
    "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
    "            \n",
    "            if location_info.region:\n",
    "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
    "            \n",
    "            if location_info.uri:\n",
    "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_global_location_rdf(self) -> str:\n",
    "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
    "        if not self.global_locations:\n",
    "            return \"\"\n",
    "        \n",
    "        location_rdf_parts = []\n",
    "        \n",
    "        for location_key, location_info in self.global_locations.items():\n",
    "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
    "            location_id = f\"ste:Location_{clean_name}\"\n",
    "            \n",
    "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
    "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
    "            \n",
    "            if location_info.latitude and location_info.longitude:\n",
    "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
    "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
    "            \n",
    "            if location_info.country:\n",
    "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
    "            \n",
    "            if location_info.region:\n",
    "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
    "            \n",
    "            if location_info.source:\n",
    "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
    "            \n",
    "            if location_info.uri:\n",
    "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
    "            \n",
    "            if rdf_lines[-1].endswith(' ;'):\n",
    "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
    "            \n",
    "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
    "        \n",
    "        return '\\n\\n'.join(location_rdf_parts)\n",
    "    \n",
    "    def clean_turtle(self, raw_output: str) -> str:\n",
    "        \"\"\"Clean turtle output\"\"\"\n",
    "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "        \n",
    "        lines = raw_output.strip().split('\\n')\n",
    "        turtle_lines = []\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if (stripped.startswith('@') or stripped.startswith('<') or \n",
    "                stripped.startswith(':') or stripped.startswith('_') or \n",
    "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
    "                turtle_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(turtle_lines)\n",
    "\n",
    "    # RAG METHODS DEACTIVATED\n",
    "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
    "        \"\"\"RAG DEACTIVATED: Vector store preparation disabled\"\"\"\n",
    "        logger.info(\"RAG functionality is DEACTIVATED - vectorstore not created\")\n",
    "        return False\n",
    "    \n",
    "    def rag_query(self, query: str, llm, k: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"RAG DEACTIVATED: RAG queries disabled\"\"\"\n",
    "        return {\"error\": \"RAG functionality is DEACTIVATED. Set RAG_ENABLED=True to enable RAG features.\"}\n",
    "    \n",
    "    def interactive_rag_session(self, llm):\n",
    "        \"\"\"RAG DEACTIVATED: Interactive RAG session disabled\"\"\"\n",
    "        print(\"\\n‚ùå RAG functionality is DEACTIVATED\")\n",
    "        print(\"To enable RAG, set RAG_ENABLED=True at the top of the script\")\n",
    "\n",
    "# Utility functions\n",
    "def load_api_key():\n",
    "    \"\"\"Load OpenAI API key\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: OPENAI_API_KEY not found\")\n",
    "        return None\n",
    "    print(\"OpenAI API Key loaded successfully.\")\n",
    "    return api_key\n",
    "\n",
    "def load_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Load text from file\"\"\"\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        print(f\"Loaded text from {filepath}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def initialize_llm(api_key: str):\n",
    "    \"\"\"Initialize LLM\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    try:\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, openai_api_key=api_key)\n",
    "        print(\"LLM initialized successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
    "    \"\"\"RAG DEACTIVATED: Vector store creation disabled\"\"\"\n",
    "    if not RAG_ENABLED:\n",
    "        logger.info(\"RAG functionality is DEACTIVATED - vectorstore not created\")\n",
    "        return None\n",
    "    \n",
    "    # Original code would go here if RAG_ENABLED was True\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with chunking support (RAG DEACTIVATED)\"\"\"\n",
    "    print(\"üöÄ Starting Multi-Knowledge Graph System with Chunking (RAG DEACTIVATED)\")\n",
    "    \n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        return\n",
    "    \n",
    "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
    "    if not domain_text:\n",
    "        print(\"‚ö†Ô∏è  No input file found, using sample text\")\n",
    "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC. \n",
    "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes. \n",
    "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
    "    else:\n",
    "        print(f\"üìÑ Using YOUR text from {INPUT_TEXT_FILE}\")\n",
    "        print(f\"üìù Text length: {len(domain_text)} characters\")\n",
    "    \n",
    "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
    "    llm = initialize_llm(api_key)\n",
    "    \n",
    "    if not llm:\n",
    "        return\n",
    "    \n",
    "    # Vector store preparation SKIPPED (RAG deactivated)\n",
    "    print(\"\\n‚ùå RAG vector store setup SKIPPED (RAG is DEACTIVATED)\")\n",
    "    \n",
    "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
    "    print(f\"üî¢ Total tokens in text: {token_count:,}\")\n",
    "    \n",
    "    if token_count > 15000:\n",
    "        print(\"üìä Text is large, chunking into smaller pieces...\")\n",
    "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=15000)\n",
    "        print(f\"üìÑ Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"üìÑ Text is small enough to process as single chunk\")\n",
    "        chunks = [domain_text]\n",
    "    \n",
    "    # Extract events and create RDF (without RAG)\n",
    "    all_turtle_outputs = []\n",
    "    all_entities = set()\n",
    "    \n",
    "    print(\"\\nüîÑ Processing chunks for event extraction (without RAG)...\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\nüîÑ Processing chunk {i}/{len(chunks)}...\")\n",
    "        \n",
    "        turtle_output = multi_kg_system.process_chunk(chunk, i, llm)\n",
    "        if turtle_output:\n",
    "            all_turtle_outputs.append(turtle_output)\n",
    "            \n",
    "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
    "        all_entities.update(chunk_entities)\n",
    "        \n",
    "        if i < len(chunks):\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Save RDF output\n",
    "    if all_turtle_outputs:\n",
    "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
    "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
    "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        final_output = prefixes + \"# Historical Events with Knowledge Graph Enhanced Location Data (RAG DEACTIVATED)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
    "        \n",
    "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_output)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Saved RDF to {OUTPUT_RAG_TTL}\")\n",
    "        print(f\"üìä Processing Statistics:\")\n",
    "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
    "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
    "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
    "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
    "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
    "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
    "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
    "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
    "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
    "        print(f\"   - RAG status: DEACTIVATED\")\n",
    "        \n",
    "        print(f\"\\nüîó Knowledge Graph Connector Statistics:\")\n",
    "        for name, connector in multi_kg_system.connectors.items():\n",
    "            stats = connector.get_stats()\n",
    "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
    "        \n",
    "        if multi_kg_system.location_extractor.location_cache:\n",
    "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
    "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
    "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample of generated RDF:\")\n",
    "        print(\"=\"*60)\n",
    "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No events were extracted from any chunks\")\n",
    "    \n",
    "    # RAG SESSION DEACTIVATED\n",
    "    print(f\"\\n‚ùå RAG System is DEACTIVATED\")\n",
    "    print(f\"üí° To enable RAG functionality:\")\n",
    "    print(f\"   1. Set RAG_ENABLED = True at the top of the script\")\n",
    "    print(f\"   2. Ensure langchain dependencies are installed\")\n",
    "    print(f\"   3. Re-run the script\")\n",
    "    \n",
    "    print(f\"\\nüéâ Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
    "    print(f\"üìä System ran in NON-RAG mode - only Knowledge Graph and Location enrichment was used.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f112597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:51:02,083 - INFO - Loaded location ontology from locations.owl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Multi-Knowledge Graph System with Chunking (RAG DEACTIVATED) - CLAUDE 4 VERSION\n",
      "Anthropic API Key loaded successfully.\n",
      "Loaded text from part_aa\n",
      "üìÑ Using YOUR text from part_aa\n",
      "üìù Text length: 398568 characters\n",
      "Claude 4 client initialized successfully.\n",
      "\n",
      "‚ùå RAG vector store setup SKIPPED (RAG is DEACTIVATED)\n",
      "üî¢ Total tokens in text: 86,945\n",
      "üìä Text is large, chunking into smaller pieces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:51:07,312 - INFO - Processing chunk 1 (117220 chars) - RAG DISABLED - USING CLAUDE 4\n",
      "2025-05-29 07:51:07,321 - INFO - Found potential locations in chunk 1: ['Corinthians', 'Ambraciots', 'Chimerium', 'Ephyre', 'King Cyrus', 'BOOK', 'Immediately', 'Cambyses', 'Pydna Accordingly', 'Strepsa', 'The Lacedaemonians', 'Pisistratus', 'Lacedaemonius', 'Seeing', 'Thyamis', 'Phocaeans', 'Atreus', 'Hence', 'Various', 'Achaeans', 'The Translator', 'Ambracia', 'Megara', 'Lacedaemon\\n\\nThe Athenians', 'Ozolian Locrians', 'Thrace', 'Bottiaeans', 'Old', 'Sybota Thus', 'Peloponnesian War', 'Troy', 'Actium', 'Zacynthus', 'Could', 'Corcyraeans', 'Athenians Thus', 'Trojan', 'Hellenes', 'Alive', 'Corinth', 'Euboea', 'Chrysippus', 'Assuredly', 'Sicily With', 'Meikiades', 'Epidamnus', 'Eurybatus', 'Corinth But', 'Corinth For', 'Eurystheus', 'THE PELOPONNESIAN WAR', 'Thesprotis', 'They', 'START', 'Danaans', 'Asopius Arrived', 'Glaucon', 'CHAPTER', 'Pale', 'Aware', 'Subsequently', 'Thessalians', 'Persian', 'Advancing', 'The Peloponnesian War', 'But Archidamus', 'Here', 'Peloponnesian', 'Leucas', 'Gigonus', 'Peloponnesian War\\n\\nThucydides', 'Samians', 'The Progress', 'Tyndareus', 'Thessaly', 'Corinthians After', 'Translated', 'There', 'While Callias', 'CONNOP THIRLWALL\\nHistorian', 'The Thebans', 'David Widger', 'Take', 'Halys', 'Philoctetes', 'Hellas Never', 'Persians', 'Corcyraean', 'Corcyraeans But', 'Aphytis', 'Lycomedes', 'Everywhere', 'Our', 'With', 'Corcyraeans The', 'Samos Again', 'Peloponnese But', 'Men', 'While', 'Athenians', 'Their', 'Phalius', 'Argives', 'Aegina', 'CHAPTER XVIII\\nCHAPTER XIX\\nCHAPTER', 'Athens They', 'United States', 'Sicyon', 'Archetimus', 'Sailing', 'Having', 'Failing', 'Epicles Their', 'Darius', 'CHAPTER VII\\nCHAPTER VIII\\n\\nBOOK III\\nCHAPTER', 'Phthiotis', 'Thus', 'The Median War', 'Upon', 'Anactorium', 'Strombichus', 'Samian', 'Leukimme Neither', 'Chalcis', 'Perhaps', 'Croesus', 'Mede Still', 'Marathon', 'CHAPTER III\\nCongress', 'CHAPTER III\\nCHAPTER', 'Title', 'Sthenelaidas', 'Samos', 'Callias', 'Polycrates', 'Ionia', 'Gulf', 'Others', 'Confidence', 'Leukimme', 'Richard Crawley\\nWith Permission', 'Athenians Ten', 'Indeed', 'Mycale', 'Are', 'Causes', 'Remaining', 'Boeotian', 'Returning', 'Asia', 'Isarchus When', 'Acheron', 'Peloponnese With', 'Ilium', 'Eleans', 'Spartan', 'Trojans', 'Eurystheus And', 'Taulantians', 'Eight', 'Philip', 'King', 'Well', 'Macedonia', 'Kestrine', 'Lake Bolbe', 'However', 'Troy What', 'You', 'Engaged', 'Ameinocles', 'Summoning', 'God', 'Hellas All', 'Carthaginians', 'Commencement', 'Panathenaic', 'THE HISTORY', 'Phoenicians', 'Cyclades', 'Defeated', 'Whenever', 'Thebes', 'Times', 'Further', 'Argos', 'Peloponnese Complaints', 'Homer', 'Thracian', 'Aetolians', 'Beroea', 'Meanwhile', 'Cyrus', 'Gods', 'Produced', 'THE PROJECT GUTENBERG EBOOK THE HISTORY', 'Lacedaemonian', 'Median War', 'Epidamnian', 'Delos', 'Hermione', 'Eratocleides', 'Lysimachus', 'Remember', 'Aeginetans', 'Pitane', 'Pydna', 'Boeotia', 'Almost', 'Hellas Moreover', 'Calliades', 'The Corinthian', 'Agamemnon', 'Being', 'Such', 'Macedonians', 'Dorians', 'Euthycles', 'Greece\\nThis Translation', 'Sicily', 'Eretria', 'Achilles', 'Richard Crawley\\nRelease Date', 'UTF', 'Peloponnesian Confederacy', 'Language', 'Salamis This', 'Accordingly', 'Ionia There', 'These Corcyraeans', 'Chalcidice', 'Timanor', 'Cephallonia', 'Mygdonia', 'Secondly', 'Concerning', 'Believing', 'Neither', 'Unable', 'Peloponnese And', 'English\\nCharacter', 'Receiving', 'Lacedaemonians', 'Without', 'Arcadia', 'Apollonia', 'Leagrus', 'Vote', 'CONTENTS\\nBOOK', 'Again', 'Thirdly', 'Bottica', 'Troezen', 'Pausanias With', 'The Mede', 'Hellen', 'Lysicles', 'Medes They', 'Troy But', 'Epidaurus', 'Perseus', 'Sestos', 'Macedonian', 'Instantly', 'Hellas', 'Instead', 'Leos', 'Now', 'Sparta', 'Aristides', 'Formerly', 'Supremacy', 'Corcyra They', 'Cimon', 'Derdas', 'Eurytimus', 'Respectfully Inscribed', 'Trojan War', 'Hellas The', 'Homer Born', 'Abronichus', 'Medes After', 'Diotimus', 'Hipparchus', 'Difficulty', 'Planted', 'Hellas For', 'Turning', 'Olynthus', 'Callicrates', 'Surely', 'For', 'Ionian', 'Thessalus', 'Aisimides', 'Abstinence', 'Each', 'Phaeacians This', 'Even', 'Acarnanians', 'Pallene', 'Illyrians Sitting', 'Whereas', 'Delian Apollo About', 'Xerxes', 'Ilium Twenty', 'Did', 'Samos And', 'CHAPTER XVI\\nCHAPTER XVII\\n\\nBOOK', 'Corcyra Not', 'Corinthians When', 'Perdiccas', 'Any', 'Accordingly Attica', 'Ionians', 'Italy', 'Megarian', 'War', 'Point Leukimme', 'Lesbos Both', 'Invited', 'Hippias', 'Heraclids Atreus', 'Corcyraeans After', 'Isarchidas', 'Acherusian', 'Samians Dating', 'Timanthes', 'Peloponnesians', 'Dorian', 'Cadmeis', 'Leucadians', 'Adimantus', 'Marseilles', 'Epidamnus Advertisement', 'Salamis', 'Illyrian', 'Corcyra', 'The Project Gutenberg', 'Athenians After', 'Phliasians', 'She', 'Empire\\n\\nThe', 'Peloponnesian War\\nAuthor', 'Time', 'Mede', 'Hera', 'Archidamus Last', 'Athens', 'Median', 'Thucydides', 'Attica', 'And Perdiccas', 'BOOK VII\\nCHAPTER XXI\\nCHAPTER XXII\\nCHAPTER XXIII\\n\\nBOOK VIII\\nCHAPTER XXIV\\nCHAPTER XXV\\nCHAPTER XXVI\\nBOOK', 'Hellenic', 'Thesprotis This', 'Themistocles', 'According', 'Both', 'Mede Not', 'Euboea The Lacedaemonians', 'Medes', 'Arcadians', 'Boeotians', 'The History', 'Alexander', 'Chalcidians', 'Sicilian', 'Europe', 'Weigh', 'Corinthians There', 'Harmodius', 'Sermylians', 'Lacedaemonians Their', 'Chios', 'Sybota', 'Ambracian', 'Pallene So', 'Project Gutenberg License', 'Aristogiton', 'Sixty', 'Heraclids', 'Besides', 'Minos', 'Still', 'Arne', 'Pelops', 'Corinth Great', 'Proteas', 'Elis', 'His\\nGreat Predecessor', 'Phormio', 'Andocides', 'Leotychides', 'Greece', 'Iolaus', 'Carians', 'Delphi', 'Athenians That', 'Ambraciot', 'Lacedaemon', 'Albert Imrie', 'Now Agamemnon', 'Pellichas', 'Not', 'Athenian', 'Leogoras', 'From', 'Therme', 'Chersonese', 'CHAPTER XII\\nCHAPTER XIII\\nCHAPTER XIV\\n\\nBOOK', 'Deucalion', 'Epidamnians', 'Hellespont', 'The Corinthians', 'Wars', 'All Lacedaemonians', 'Thucydides\\nThis', 'The Athenians', 'Elean', 'Apollo', 'Olympic', 'Cyllene', 'Rhenea', 'The Corcyraeans', 'Megarians', 'Thucydides\\nTranslator', 'The State', 'Corinthian', 'Perceiving', 'Arrived', 'Hellas And', 'Aristeus', 'Hellenes There', 'Phoenician', 'Peloponnese', 'Pelasgian', 'Work', 'Nor', 'Archestratus', 'Persia', 'Ionic Gulf Its', 'Xenoclides', 'The Affair']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Created 4 chunks\n",
      "\n",
      "üîÑ Processing chunks for event extraction (without RAG, using Claude 4)...\n",
      "\n",
      "üîÑ Processing chunk 1/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:51:19,599 - INFO - Registered new location: Cambyses\n",
      "2025-05-29 07:51:22,321 - INFO - Registered new location: Strepsa\n",
      "2025-05-29 07:51:22,322 - INFO - Found entities in chunk 1: ['The Project Gutenberg', 'The History', 'Peloponnesian War', 'Thucydides\\nThis', 'United States']...\n",
      "2025-05-29 07:51:22,323 - INFO - Enriched 2 locations with coordinates\n",
      "2025-05-29 07:51:22,589 - INFO - Retrieved 0 facts from Wikidata for 'The Project Gutenberg'\n",
      "2025-05-29 07:51:23,571 - WARNING - Wikidata returned status 400 for Thucydides\n",
      "This\n",
      "2025-05-29 07:51:23,844 - WARNING - DBpedia returned status 400 for Thucydides\n",
      "This\n",
      "2025-05-29 07:51:24,597 - INFO - Retrieved 0 facts from Wikidata for 'Project Gutenberg License'\n",
      "2025-05-29 07:51:24,794 - INFO - Retrieved 0 facts from DBpedia for 'Project Gutenberg License'\n",
      "2025-05-29 07:51:25,597 - WARNING - Wikidata returned status 400 for Peloponnesian War\n",
      "Author\n",
      "2025-05-29 07:51:25,800 - WARNING - DBpedia returned status 400 for Peloponnesian War\n",
      "Author\n",
      "2025-05-29 07:51:26,499 - WARNING - Wikidata returned status 429 for Thucydides\n",
      "Translator\n",
      "2025-05-29 07:51:26,801 - WARNING - DBpedia returned status 400 for Thucydides\n",
      "Translator\n",
      "2025-05-29 07:51:33,557 - WARNING - Wikidata returned status 400 for Richard Crawley\n",
      "Release Date\n",
      "2025-05-29 07:51:33,782 - WARNING - DBpedia returned status 400 for Richard Crawley\n",
      "Release Date\n",
      "2025-05-29 07:51:41,401 - WARNING - Wikidata returned status 400 for English\n",
      "Character\n",
      "2025-05-29 07:51:41,621 - WARNING - DBpedia returned status 400 for English\n",
      "Character\n",
      "2025-05-29 07:51:59,875 - INFO - KG retrieval completed: 21/21 successful, 24 cache hits\n",
      "2025-05-29 07:52:13,858 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:52:13,864 - INFO - Generated RDF for chunk 1 (Claude 4, without RAG)\n",
      "2025-05-29 07:52:14,871 - INFO - Processing chunk 2 (115641 chars) - RAG DISABLED - USING CLAUDE 4\n",
      "2025-05-29 07:52:14,899 - INFO - Found potential locations in chunk 2: ['Corinthians', 'Delay', 'Ambraciots', 'Myronides After', 'BOOK', 'Immediately', 'Haliae', 'Medism', 'Cylon', 'Lacedaemonians The', 'Suppose', 'Miltiades Next', 'Antigenes This', 'Naucleides', 'Capital', 'Procne', 'Lampsacus', 'Except', 'Locris', 'Sardis', 'Synoecia', 'Achaeans', 'Digging', 'Plataean', 'Megara', 'Thrace', 'Cecruphalia', 'Urged', 'Athens After', 'Cropia', 'Peloponnesian War', 'Anticles', 'Peloponnese After', 'Ozolian Locrians The Athenians', 'The Phaleric', 'Zacynthus', 'Corcyraeans', 'Cranonians', 'Treasurers', 'Hellanicus', 'Twice', 'Hellenes', 'Euphamidas', 'Corinth', 'Fairwater', 'His', 'Slow', 'Euboea', 'Egypt After', 'Yet', 'Gyrtonians', 'Acharnians', 'Pellene', 'Evarchus', 'Aegean', 'Epidamnus', 'Lesbos', 'Thrace Tereus', 'Eurymachus', 'Opuntian Locrians', 'The King', 'Munychia', 'Enneacrounos', 'Crissaean Gulf', 'King Pleistarchus', 'Byzantium', 'They', 'Caunus', 'CHAPTER', 'Euboean', 'Sitalces', 'Subsequently', 'Thessalians', 'Geraneia', 'Persian', 'Orestes', 'Peloponnesian', 'For Themistocles', 'Graea', 'Leucas', 'Under Cecrops', 'Attic', 'Nile', 'Samos Their', 'Samians', 'Xanthippus Landing', 'Pausanias Matter', 'Thessaly', 'Nisaea Atalanta', 'There', 'Socrates', 'The Thebans', 'Timoxenus', 'Secret', 'Till', 'Brooks', 'Feast', 'Cyprus', 'Eumachus', 'Persians', 'Rheiti', 'Corcyraean', 'Anactorians', 'Beginning', 'Everywhere', 'Chians', 'Brilessus While', 'Stesagoras', 'Farmers', 'Our', 'With', 'Athens Pericles', 'Acropolis This', 'Helot', 'While', 'Athenians', 'Amyrtaeus', 'Cilicians', 'Their', 'Laconia', 'Themistocles\\n\\nAfter', 'Argives', 'Aegina', 'Athens They', 'Nicomedes', 'Oenophyta', 'Sicyon', 'But Themistocles', 'Corinthians The Lacedaemonians', 'Peloponnese Such', 'Helots The', 'Corcyra Still', 'Sailing', 'Preparations', 'Encamping', 'Athens With', 'Olympian Zeus', 'Ithome', 'Union', 'Earth', 'Cleopompus', 'Delphians Immediately', 'Admetus', 'Argolis', 'Thus', 'Lesbos After', 'Making', 'Hystaspes', 'Upon', 'Taenarus', 'Samian', 'Anthesterion', 'Athens Therefore', 'Methone', 'Chalcis', 'Marshes', 'Marathon', 'Pheia', 'Sicyonians', 'Ancient', 'Samos', 'Sadocus', 'Considering', 'Brazen House', 'Nisaea', 'Released', 'Tragia', 'Aegina But', 'Ennea Hodoi', 'Egypt Meanwhile Orestes', 'Ionia', 'Diplomatic Skirmishes', 'Soon', 'Eretrian', 'Others', 'Epidaurians Meanwhile', 'Astacus', 'Aristonymus', 'Scyros', 'Admitting', 'About', 'Athenians These', 'Gongylus', 'Epidaurians', 'Colonae', 'Boeotarchs', 'Boeotia After', 'Athens Not', 'Indeed', 'Brasidas', 'Disease', 'Spartans', 'Poseidon', 'Thyrea', 'Diemporus', 'Piraeus', 'Boeotian', 'Athens But Athens', 'Thriasian', 'Athens The', 'Zeal', 'Thera', 'Make', 'Pegae For', 'Peloponnese With', 'Pharnaces', 'Dismissing', 'Melesippus', 'Cleomenes', 'Athens Meanwhile', 'Xenotimus', 'Phalerum', 'Eleans', 'Lesbians', 'Spartan', 'Athens Indeed', 'The Byzantines', 'Egyptians', 'Eurymedon', 'King', 'None', 'Second Congress', 'Well', 'Nymphodorus', 'Chrysis', 'Dionysia', 'Consider', 'Marea', 'Libyans', 'Magnesia', 'However', 'Naupactus', 'Out', 'Setting', 'You', 'Then', 'Corinth The Athenians', 'Stroebus Upon', 'God', 'Thessalian', 'Acharnae', 'Pericles\\n\\nThe', 'Phoenicians', 'Acarnania Failing', 'Tolmides', 'Between', 'Attica Without', 'Cyclades', 'Boeum', 'Apart', 'Thebes', 'Dionysus', 'Piraeus Meanwhile', 'Should', 'Mount Aegaleus', 'Thronium', 'Clinias', 'Pheia The', 'Edonians', 'Further', 'Argos', 'Samos Sixteen', 'Thracian', 'Acarnanian', 'Egypt Finding', 'Xanthippus', 'Meanwhile', 'Polymedes', 'Isthmus', 'Leontiades', 'Pythes', 'Caria', 'Pissuthnes', 'Odrysians Again', 'Pharsalians', 'Theseus', 'Histiaea', 'Pheraeans The Larisaean', 'Lacedaemonian', 'Median War', 'Delos', 'Menon', 'Hermione', 'Beautiful', 'Perioeci', 'Coronea', 'Eleusinians', 'Thebans', 'Aeginetans', 'Thria', 'Dolopian', 'Eleusis', 'Aethaeans', 'Pydna', 'Boeotia', 'Megabates', 'Alarmed', 'Eumolpus', 'Memphis', 'Plataeans While', 'Dorkis', 'Athens Meanwhile Inaros', 'Confident', 'Melos', 'Destroying', 'Eion', 'Phrygia', 'Being', 'Ramphias', 'Pericles', 'Such', 'Long Walls', 'Macedonians', 'Dorians', 'Coasting', 'Sicily', 'Larisa', 'Oenoe During', 'Manifold', 'Thuriats', 'Point Ichthys', 'Troad', 'Thracians Teres', 'Pandion', 'Daskylion', 'Megara Now', 'Three', 'Opus', 'Phocians', 'Accordingly', 'Epicles', 'Now Plataea', 'Zeus Accordingly', 'Cranian', 'Alope', 'Locrians', 'Phocians Some', 'Cyprians', 'Sitting', 'Outside Peloponnese', 'Palaira', 'Pegae', 'Besieged', 'Other', 'Mount Parnes', 'Oropians', 'Asopus', 'Helots', 'Egyptians Inaros', 'Thebes For Plataea', 'Woe', 'Peloponnese And', 'Funeral Oration', 'Plataeans', 'Libyan', 'Lacedaemonians', 'Tellis', 'Athenians The', 'Powers', 'Cleombrotus', 'Pausanias For', 'Drabescus', 'Eleusinian Demeter', 'Again', 'Ithome Most', 'Troezen', 'Whether', 'The Mede', 'Pausanias', 'Long Walls Meanwhile', 'Thrius', 'Molossian', 'Accordingly Cylon', 'Chaeronea', 'Pellenians', 'Hellas', 'Kitinium', 'Oracles', 'Now', 'Sparta', 'Itys', 'Megabazus', 'Aegina Above', 'Cimon', 'Crete', 'Meanwhile Pericles', 'Acropolis The Athenians', 'Tanagra', 'Hellas The', 'Myos', 'Athenians Reinforced', 'Nisaea The Megarians', 'Prosopitis', 'Zopyrus', 'Diasia', 'For', 'Thasos', 'Egyptian', 'Archidamus', 'Ionian', 'Abderite', 'Cranians', 'Strymon', 'Zeus', 'Even', 'Pythian', 'Acarnanians', 'Perdiccas Coming', 'Tanagraeans', 'Artabazus', 'Xerxes', 'Did', 'Mendesian', 'Cephallenia', 'Cimon Sixty', 'Onetorides', 'Its', 'Callirhoe', 'Euboea Such', 'First Invasion', 'Daulis', 'Leave', 'Perdiccas', 'Asiatic Magnesia', 'Cyprian', 'Megarid', 'Any', 'Ionians', 'Italy', 'Timocrates', 'Aristonus', 'Touching', 'Aenesias', 'Orchomenus', 'Achaia', 'Megarian', 'Oenoe', 'Carcinus', 'Euboea After', 'War', 'Acarnanian Evarchus', 'Olympia', 'Daulian', 'Xanthippus The Athenians', 'Kaiadas', 'Cyprus These', 'Nine Ways', 'Miltiades', 'Peloponnesians', 'Miletus The Byzantines', 'Dorian', 'Meanwhile Cylon', 'Leucadians', 'Phocis', 'Pythangelus', 'The Plataeans', 'Paleans', 'Tolmaeus', 'Ionia Going', 'Among', 'Cimon The', 'Naxos', 'Teres', 'Amphipolis They', 'Leonidas', 'Erechtheus', 'Plataea', 'Histiaeans', 'Salamis', 'Athens Towards', 'Pythian Apollo', 'Tereus', 'Corcyra', 'Taenarus The Lacedaemonians', 'King Pleistoanax', 'Afterwards', 'Mede', 'Anxious', 'Odrysians', 'Ephesus After', 'Attacked', 'Athens', 'Argilus', 'Agesander Not', 'Median', 'Tlepolemus', 'Thucydides', 'Familiarity', 'Attica', 'Foremost', 'Nay', 'Hellenic', 'Themistocles', 'Megabuzus', 'Thasians', 'Medes', 'Athene', 'Miletus Victory', 'Your', 'Boeotians', 'Pleistoanax', 'Corinthians They', 'Gracious', 'Leocrates', 'Alexander', 'One', 'Pyrasians', 'Places', 'Pythodorus', 'Pausanias The Athenians', 'Mede Meanwhile Pausanias', 'Kitium', 'Thessalians Meanwhile', 'Chios', 'For Pausanias', 'Egypt Arriving', 'Chalcidians Thus Sitalces', 'White Castle Within', 'Zeuxis', 'Pharos', 'Attica The', 'Locrians The', 'Abandoning', 'Sixty', 'Besides', 'Lemnos', 'Still', 'Athens This', 'Phyleides', 'Oeniadae', 'Zeus Meilichios', 'Psammetichus', 'Individuals', 'Proteas', 'Elis', 'Phormio', 'Echecratidas', 'Taking', 'Tolmaeus They', 'Long Wall', 'Pericles The Athenians', 'Diacritus', 'King Xerxes', 'Archidamus Even', 'Delphi', 'Fellow', 'This Teres', 'Full', 'Lacedaemon', 'Hagnon', 'Pharsalian', 'Pronaeans Not', 'Not', 'Libya', 'Athenian', 'From', 'Therme', 'After Pausanias', 'Carystus', 'Achaean', 'Phocis They', 'Hellespont', 'Messenians', 'Egypt They', 'Cyrene', 'Thracians', 'Pursued', 'Oropus', 'Egypt', 'The Athenians', 'Hearing', 'Fresh', 'Olympic', 'Chian', 'Let', 'Acarnania', 'Megarians', 'Lesbian', 'Priene Worsted', 'Zacynthians', 'Doris', 'Lastly', 'Nine Pipes', 'King Artaxerxes', 'Acropolis', 'Phaleric Then', 'Deep', 'Erineum They', 'Perceiving', 'Arrived', 'Hellas And', 'Larisaeans', 'Hellene', 'Athens Personally', 'Myronides', 'Sollium', 'Opuntian', 'Milesians', 'Pharsalus', 'Theban', 'Altogether', 'Thessaly They', 'Sallying', 'Hellenes This', 'Phoenician', 'Peloponnese', 'Pelasgian', 'Knots', 'Theagenes', 'Samaeans', 'Attica This', 'The Lacedaemonians']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 2/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:52:21,048 - WARNING - Coordinates for 'Cylon' appear to be in North America (45.122222222, -92.353055555). Please verify if this is correct for your historical context.\n",
      "2025-05-29 07:52:21,049 - INFO - Registered new location: Cylon\n",
      "2025-05-29 07:52:21,050 - INFO - Found entities in chunk 2: ['Themistocles', 'Piraeus', 'Athenians', 'For', 'Between']...\n",
      "2025-05-29 07:52:21,051 - INFO - Enriched 1 locations with coordinates\n",
      "2025-05-29 07:52:21,303 - INFO - Retrieved 3 facts from DBpedia for 'About'\n",
      "2025-05-29 07:52:21,511 - INFO - Retrieved 3 facts from Wikidata for 'About'\n",
      "2025-05-29 07:52:22,277 - INFO - Retrieved 3 facts from DBpedia for 'His'\n",
      "2025-05-29 07:52:22,352 - INFO - Retrieved 3 facts from Wikidata for 'His'\n",
      "2025-05-29 07:52:23,339 - INFO - Retrieved 3 facts from Wikidata for 'Thus'\n",
      "2025-05-29 07:52:23,565 - INFO - Retrieved 3 facts from DBpedia for 'Thus'\n",
      "2025-05-29 07:52:30,445 - INFO - Retrieved 1 facts from ConceptNet for 'About'\n",
      "2025-05-29 07:52:30,708 - INFO - Retrieved 0 facts from Wikidata for 'Mede Meanwhile Pausanias'\n",
      "2025-05-29 07:52:30,927 - INFO - Retrieved 0 facts from DBpedia for 'Mede Meanwhile Pausanias'\n",
      "2025-05-29 07:52:34,188 - INFO - Retrieved 1 facts from ConceptNet for 'His'\n",
      "2025-05-29 07:52:45,237 - INFO - Retrieved 0 facts from Wikidata for 'Peloponnese With'\n",
      "2025-05-29 07:52:45,463 - INFO - Retrieved 0 facts from DBpedia for 'Peloponnese With'\n",
      "2025-05-29 07:52:49,354 - INFO - Retrieved 3 facts from Wikidata for 'Cyprus'\n",
      "2025-05-29 07:52:49,364 - INFO - Retrieved 3 facts from DBpedia for 'Cyprus'\n",
      "2025-05-29 07:53:06,062 - WARNING - ‚è∞ Timeout: 2/19 KG queries still pending, continuing with available results\n",
      "2025-05-29 07:53:10,543 - INFO - Retrieved 1 facts from ConceptNet for 'Cyprus'\n",
      "2025-05-29 07:53:10,544 - INFO - KG retrieval completed: 17/19 successful, 26 cache hits\n",
      "2025-05-29 07:53:22,775 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:53:22,780 - INFO - Generated RDF for chunk 2 (Claude 4, without RAG)\n",
      "2025-05-29 07:53:23,790 - INFO - Processing chunk 3 (115922 chars) - RAG DISABLED - USING CLAUDE 4\n",
      "2025-05-29 07:53:23,814 - INFO - Found potential locations in chunk 3: ['Sintians', 'Corinthians', 'Ambraciots', 'Pierians', 'Molycrian Rhium', 'Cydonia', 'Amphiaraus Dissatisfied', 'Immediately', 'Athenians The Chalcidians', 'Athenians Besides', 'Suddenly', 'Cercine', 'Megara This', 'Trust', 'Achaeans', 'Theolytus', 'Edonians From Eordia', 'Plataea The', 'Plataean', 'Ambracia', 'Megara', 'Athenians The Lesbians', 'Thrace', 'Bottiaeans', 'Athenians You', 'Amphilochian', 'Lemnians', 'Stratus', 'Zacynthus', 'Stratus The Lacedaemonians', 'Gortynia', 'Hellenes', 'Anthemus', 'Corinth', 'His', 'Fifth Years', 'Euboea', 'Yet', 'Achelous', 'Philemon', 'Chalcidians The', 'Pericles When', 'Lesbos', 'Crissaean Gulf', 'Byzantium', 'They', 'King But', 'Abdera', 'The Peloponnesian', 'Rhodian Dorieus', 'Inland', 'This Rhium', 'Sitalces', 'Thessalians', 'Laurium', 'Agraeans', 'Fear', 'Amphilochian Argos', 'Mount Pangaeus', 'Stopped', 'Advancing', 'Sitalces\\n\\nThe', 'Persian', 'Euxine', 'Peloponnesian', 'The Chalcidian', 'Leucas', 'Nicolaus', 'Against', 'Thermopylae', 'Evenus', 'Methymnians', 'Mitylene', 'Maedians', 'There', 'Pierian Gulf', 'Crissaean', 'Messenians After', 'Since', 'Methymna', 'Archidamus The Plataeans', 'Twenty', 'More', 'Supplications', 'Laeaeans', 'Anactorians', 'Beginning', 'Acarnanians The', 'Chians', 'Orestians', 'Attica During', 'Our', 'With', 'Men', 'While', 'Athenians', 'Athenian And', 'CHAPTER VII\\nSecond Year', 'Their', 'Naupactus The Lacedaemonians', 'Laconia', 'Argives', 'Machaon', 'Thracian Irruption', 'Axius', 'Sicyon', 'Alcmaeon', 'Failing', 'Lesbos However', 'Idomene', 'Justice', 'Olympian Zeus', 'Oroedus', 'Cleopompus', 'Paeonians', 'Danube', 'Thus', 'Chaonians', 'Thrace These', 'Tegean', 'Upon', 'Anthemus The Macedonians', 'Pontus', 'Polichnitans', 'Anactorium', 'Meanwhile Phormio', 'Rhium', 'Seuthes', 'Coronta', 'Chalcis', 'Respect', 'Laeaean Paeonians', 'Crusis', 'Bottice', 'Odrysian', 'Elated', 'Agatharchidas The Peloponnesians', 'Sadocus', 'Nisaea', 'Mount Scombrus', 'The Chalcidians', 'Others', 'Confidence', 'Astacus', 'Phagres', 'The Peloponnesians', 'About', 'Isocrates', 'Phanomachus', 'Indeed', 'Brasidas', 'Dyme', 'Disease', 'Xenophon', 'Agrianes', 'Poseidon', 'Corinth Passing', 'Pentacosiomedimni', 'Paralian', 'Spardacus', 'Leucadian', 'Piraeus', 'Naupactus For', 'Almopians', 'Aristocleides', 'Asia', 'Athens The', 'Elimiots', 'Stratodemus', 'Magnetes', 'Make', 'Photys', 'Athenians Their', 'Naupactus The Athenians', 'Lesbians', 'Arriving', 'The Hellenic', 'Athens Indeed', 'Spartan', 'Corinthian Aristeus', 'Melesander', 'Geraestus', 'CHAPTER VIII\\nThird Year', 'Philip', 'King', 'Patrae', 'Crissaean Gulf Six', 'Macedonia', 'The Mitylenians', 'Pollis', 'Hestiodorus', 'Almopia These Macedonians', 'Zeuxidamus', 'However', 'Naupactus', 'Physca', 'You', 'Achaea', 'Corinth The Athenians', 'Fort Budorum', 'Cyrrhus', 'Lycia', 'Dersaeans', 'Lacedaemonian Timocrates', 'Fall', 'Between', 'Whenever', 'Should', 'Clinias', 'Further', 'Argos', 'Rather', 'Homer', 'Thracian', 'Acarnanian', 'Malea', 'Meanwhile', 'Isthmus', 'Ionian Gulf', 'Cyrus', 'Revolt', 'Caria', 'Europus', 'Leucas The Corinthians', 'Lacedaemonian', 'Asopius', 'Median War', 'Malean Apollo', 'Peloponnesians During', 'Hermione', 'Thebans', 'Meleas', 'Remember', 'Amphilochia', 'Lyncestae', 'Perseverance', 'Pericles', 'Such', 'Meanwhile Sitalces', 'Atintanians', 'Macedonians', 'Sicily', 'Paeonia', 'Eleian', 'BOOK III\\nCHAPTER', 'The Hellenes', 'Mitylenians After', 'Paravaeans', 'Comfort', 'Bisaltia', 'Euxine The', 'Amphiraus', 'Born', 'Cynes', 'Arcturus', 'Triballi', 'Chalcidice', 'Pangaeus', 'For Athens', 'Prasiai', 'Mygdonia', 'Perplexed', 'Neither', 'Naupactus Thus', 'Sitalces Laying', 'Bradidas', 'Oroedus There', 'Unable', 'Atalanta', 'The Athenian', 'Athens There', 'Plataeans', 'Externally', 'Settling', 'Hebrus', 'Lacedaemonians', 'Phoenicia', 'Athenians The', 'Cleombrotus', 'Aneristus', 'Armed', 'Again', 'Treres', 'Strong', 'Troezen', 'Naval Victories', 'Doberus', 'King Tharyps', 'Archelaus', 'Pausanias', 'Ethiopia', 'Temenids', 'Crestonia', 'Epidaurus', 'Hellas', 'Now', 'Sparta', 'Investment', 'Bottiaea', 'Amyntas', 'Nericus', 'Crete', 'Trojan War', 'The Acarnanians', 'Callimachus The', 'Laconian', 'Molossians', 'Halieis', 'Turning', 'Olynthus', 'Dolopia', 'For', 'Stratonice', 'Archidamus', 'Winter', 'This Argos', 'Each', 'Strymon', 'Zeus', 'The Plataean', 'Spartolus', 'Putting', 'Acarnanians', 'King Antichus', 'Provisions', 'Athens The Mitylenians', 'Pharnabazus', 'Cephallenia', 'Nicanor', 'The Plague', 'Liberator', 'Sinking', 'Mount Rhodope', 'Perdiccas', 'Timocrates', 'Olympiad', 'Piraeus There', 'Preparation', 'War', 'Gortys', 'Cydonians', 'Eordians', 'Hatred', 'Mitylene\\n\\nThe', 'Olympia', 'Hermaeondas', 'Learchus', 'Rhodope The', 'Nicias', 'Discovering', 'Haemus', 'Droi', 'Peloponnesians', 'Dorian', 'Leucadians', 'Acarnan Such', 'Timagoras', 'Among', 'Argos This', 'Teres', 'Scythians', 'Pella', 'Panaeans', 'Cease', 'Ambracian Gulf', 'Plataea', 'Messenian', 'Tilataeans', 'Salamis', 'Piraeus The Peloponnesians', 'Steersmen', 'Mount Pindus', 'Mitylenians', 'Graciously', 'Numberless', 'Amphilochians', 'Afterwards', 'Molycrium', 'Mede', 'Lower Macedonia', 'Odrysians', 'Cretan', 'Athens', 'Bottia', 'Thucydides', 'Attica', 'Nestus', 'Andros But Pericles', 'Hellenic', 'Rhia', 'According', 'Getae The', 'Crete For Nicias', 'Medes', 'Your', 'Boeotians', 'Opposite', 'Great', 'Plataeans Others', 'Alexander', 'One', 'Chalcidians', 'Sabylinthus', 'Cleippides', 'Sicilian', 'Europe', 'Lycophron', 'Position', 'Mounts Haemus', 'Tenedians', 'This Seuthes', 'Owing', 'Fourth', 'Rhodope', 'Attica The', 'Anapus', 'Cithaeron', 'Corinth Not', 'Besides', 'Phormio The Peloponnesians', 'Lemnos', 'Still', 'Cnemus', 'Oeniadae', 'Elis', 'Had', 'Phormio', 'Ameiniades', 'Echinades', 'Deinias', 'Bordering', 'Achaea The Athenians', 'Odomanti', 'Alcidas', 'Ambraciot', 'Lacedaemon', 'Hagnon', 'Pieria', 'Not', 'Libya', 'Athenian', 'Stratians', 'From', 'Passing', 'Sicilians', 'Assembling', 'Hellespont', 'Messenians', 'These Macedonians', 'Callimachus', 'Amphilochus', 'Thracians', 'Egypt', 'Getae', 'The Athenians', 'Phaselis', 'Panormus', 'Paeonian', 'King Archidamus', 'Peloponnese Indeed', 'Apollo', 'Chian', 'Cyllene', 'Achaean Rhium', 'Acarnania', 'Limnaea', 'Megarians', 'Lesbian', 'Corinthian', 'Under', 'Euripides', 'Arrived', 'Alcmaeon The Athenians', 'Policy', 'Aristeus', 'Thesprotians', 'Theban', 'Athens Their', 'Peloponnese', 'Nor', 'Oskius This', 'Imbrians', 'Chalcidian', 'The Plataeans', 'Dii']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 3/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:53:30,027 - INFO - Registered new location: Cydonia\n",
      "2025-05-29 07:53:35,449 - INFO - Found entities in chunk 3: ['There', 'Against', 'Further', 'Athenian', 'Athens']...\n",
      "2025-05-29 07:53:35,451 - INFO - Enriched 1 locations with coordinates\n",
      "2025-05-29 07:53:35,677 - INFO - Retrieved 3 facts from DBpedia for 'Our'\n",
      "2025-05-29 07:53:35,778 - INFO - Retrieved 3 facts from Wikidata for 'Our'\n",
      "2025-05-29 07:53:36,721 - INFO - Retrieved 3 facts from DBpedia for 'Nor'\n",
      "2025-05-29 07:53:36,787 - INFO - Retrieved 3 facts from Wikidata for 'Nor'\n",
      "2025-05-29 07:53:37,788 - INFO - Retrieved 3 facts from Wikidata for 'Again'\n",
      "2025-05-29 07:53:38,013 - INFO - Retrieved 3 facts from DBpedia for 'Again'\n",
      "2025-05-29 07:53:44,384 - INFO - Retrieved 1 facts from ConceptNet for 'Our'\n",
      "2025-05-29 07:53:44,656 - INFO - Retrieved 0 facts from Wikidata for 'Athenian And'\n",
      "2025-05-29 07:53:44,875 - INFO - Retrieved 0 facts from DBpedia for 'Athenian And'\n",
      "2025-05-29 07:53:52,575 - INFO - Retrieved 1 facts from ConceptNet for 'Nor'\n",
      "2025-05-29 07:53:52,842 - INFO - Retrieved 0 facts from Wikidata for 'For Athens'\n",
      "2025-05-29 07:53:53,061 - INFO - Retrieved 0 facts from DBpedia for 'For Athens'\n",
      "2025-05-29 07:53:55,876 - INFO - Retrieved 1 facts from ConceptNet for 'Again'\n",
      "2025-05-29 07:53:56,214 - INFO - Retrieved 3 facts from Wikidata for 'Rather'\n",
      "2025-05-29 07:53:56,434 - INFO - Retrieved 3 facts from DBpedia for 'Rather'\n",
      "2025-05-29 07:54:17,988 - INFO - KG retrieval completed: 18/18 successful, 27 cache hits\n",
      "2025-05-29 07:54:31,997 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:54:32,002 - INFO - Generated RDF for chunk 3 (Claude 4, without RAG)\n",
      "2025-05-29 07:54:33,015 - INFO - Processing chunk 4 (47590 chars) - RAG DISABLED - USING CLAUDE 4\n",
      "2025-05-29 07:54:33,030 - INFO - Found potential locations in chunk 4: ['Myconus', 'Ammias', 'Plataea', 'Cyme', 'Salamis', 'Mitylene Going', 'Mitylenians', 'Cleon', 'Accordingly', 'Daimachus', 'Afterwards', 'Attica However', 'Diodotus', 'Lacon', 'Mede', 'Minoa', 'Confess', 'Embatum', 'Nisaea', 'Hysiae', 'Cleon After', 'Myus', 'Fortune', 'Fifth Year', 'Athens', 'Ionia', 'Sandius', 'Median', 'Plataean', 'Megara', 'Thucydides', 'Attica', 'Clarus', 'Budorum', 'Helots', 'The Peloponnesians', 'Salaminian', 'About', 'Were', 'Hellenic', 'Mitylenian', 'Towards', 'Plataeans', 'Colophon', 'Indeed', 'Antissa', 'Arcadians', 'Boeotians', 'Peloponnese Meanwhile', 'Artemisium', 'Mitylenians Their', 'Corcyraean Revolution\\n\\nDuring', 'Lacedaemonians', 'Erythrae', 'Pleistoanax', 'Eupompides', 'Hellenes', 'Arcadian', 'Athenians The', 'Niceratus', 'Another', 'Execution', 'Tenedos', 'Ionia From Ephesus Alcidas', 'Again', 'Paralian', 'Aeimnestus', 'Hellas Although', 'Only', 'Lesbos', 'Though', 'Pausanias', 'Meander', 'Asopolaus', 'Diodotus The', 'Lysicles', 'Androcrates', 'Make', 'They', 'Cleaenetus', 'Hellas', 'Epicurus', 'Now', 'Antissians', 'Cleomenes', 'Sparta', 'Lesbians', 'Paches', 'Cithaeron', 'Theaenetus', 'Besides', 'Word', 'Still', 'Ionia This', 'Salaethus', 'Hope', 'Here', 'Peloponnesian', 'The Mitylenians', 'Teutiaplus', 'Either', 'Consider', 'Athens Such', 'Starting', 'Colophonians', 'Samians', 'Methymnians', 'Mitylene', 'However', 'Mitylene The', 'Carians', 'Crossing', 'Astymachus', 'Ionian', 'For', 'Eucrates', 'Alcidas', 'Lacedaemon', 'Not', 'Athenian', 'Plain', 'Druoskephalai', 'Tolmides', 'Thebes', 'Lesbos CHAPTER', 'Coroebus', 'Patmos', 'Megara Accordingly', 'Punish', 'Trial', 'The Lacedaemonian', 'Mitylene Wishing', 'Chians', 'Aeolic', 'Meanwhile', 'Icarus', 'Our', 'Notium', 'With', 'The Athenians', 'Caria', 'Elean', 'Pissuthnes', 'Eresus', 'Fire', 'Athenians', 'Wine', 'Mitylenians The', 'Nevertheless', 'Ephesus', 'Luckily', 'Lacedaemonian', 'Their', 'Itamenes', 'Mitylene Fears', 'Let', 'Delos', 'Compassion', 'Anaia', 'King Pausanias', 'Megarians', 'War', 'Hippias', 'Oakheads After', 'Thebans', 'Pyrrha', 'Teian', 'Nicias', 'Arrived', 'Myonnesus', 'Ithome', 'Peloponnesians', 'Theban', 'Such', 'Ladders', 'Peloponnese', 'Erythraeid', 'Upon', 'The Plataeans']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing chunk 4/4 with Claude 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 07:54:35,690 - INFO - Registered new location: Plataea\n",
      "2025-05-29 07:54:36,477 - INFO - Registered new location: Cyme\n",
      "2025-05-29 07:54:36,478 - INFO - Registered new location: Salamis\n",
      "2025-05-29 07:54:45,537 - INFO - Found entities in chunk 4: ['Antissa', 'Pyrrha', 'Eresus', 'Methymnians', 'Antissians']...\n",
      "2025-05-29 07:54:45,538 - INFO - Enriched 3 locations with coordinates\n",
      "2025-05-29 07:54:45,765 - INFO - Retrieved 0 facts from DBpedia for 'Methymnians'\n",
      "2025-05-29 07:54:45,809 - INFO - Retrieved 0 facts from Wikidata for 'Methymnians'\n",
      "2025-05-29 07:54:46,815 - INFO - Retrieved 0 facts from Wikidata for 'Antissians'\n",
      "2025-05-29 07:54:47,079 - INFO - Retrieved 0 facts from DBpedia for 'Antissians'\n",
      "2025-05-29 07:54:47,806 - INFO - Retrieved 0 facts from Wikidata for 'Mitylenians'\n",
      "2025-05-29 07:54:48,036 - INFO - Retrieved 0 facts from DBpedia for 'Mitylenians'\n",
      "2025-05-29 07:55:05,644 - INFO - Retrieved 0 facts from Wikidata for 'The Athenians'\n",
      "2025-05-29 07:55:05,862 - INFO - Retrieved 0 facts from DBpedia for 'The Athenians'\n",
      "2025-05-29 07:55:27,197 - INFO - KG retrieval completed: 16/16 successful, 29 cache hits\n",
      "2025-05-29 07:55:39,376 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-05-29 07:55:39,379 - INFO - Generated RDF for chunk 4 (Claude 4, without RAG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved RDF to extracted_events_norag_with_multi_kg_Claude.ttl\n",
      "üìä Processing Statistics (Claude 4):\n",
      "   - Total chunks processed: 4\n",
      "   - Successful chunks: 4\n",
      "   - Unique entities found: 56\n",
      "   - Total KG facts retrieved: 53\n",
      "   - Cache hits: 106\n",
      "   - Locations found: 7\n",
      "   - Locations with coordinates: 7\n",
      "   - Location duplicates avoided: 0\n",
      "   - Unique global locations: 7\n",
      "   - LLM used: Claude 4 (Anthropic)\n",
      "   - RAG status: DEACTIVATED\n",
      "\n",
      "üîó Knowledge Graph Connector Statistics:\n",
      "   - Wikidata: 18/23 requests (78.3% success)\n",
      "   - DBpedia: 17/22 requests (77.3% success)\n",
      "   - ConceptNet: 6/29 requests (20.7% success)\n",
      "   - Location enrichment: 40/157 locations enriched (25.5% success)\n",
      "\n",
      "üìù Sample of generated RDF:\n",
      "============================================================\n",
      "@prefix ste: <http://www.example.org/ste#> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
      "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
      "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
      "\n",
      "# Historical Events with Knowledge Graph Enhanced Location Data (RAG DEACTIVATED, CLAUDE 4)\n",
      "ste:Event1 a ste:Event, dbp:Translation ;\n",
      "    ste:hasType \"Translation of Thucydides' History of Peloponnesian War\" ;\n",
      "    ste:hasAgent \"Richard Crawley\" ;\n",
      "    ste:hasTime \"431 BC\" ;\n",
      "    ste:hasLocation \"Athens\" ;\n",
      "    ste:hasResult \"English translation of the historical text\" .\n",
      "\n",
      "ste:Event2 a ste:Event, dbp:MilitaryConflict ;\n",
      "    ste:hasType \"Battle between Corinthians and Corcyraeans at Strepsa\" ;\n",
      "    ste:hasAgent \"Corinthians and Corcyraeans\" ;\n",
      "    ste:hasTime \"Before Peloponnesian War\" ;\n",
      "    ste:hasLocation \"Strepsa\" ;\n",
      "    ste:hasLatitu...\n",
      "============================================================\n",
      "\n",
      "‚ùå RAG System is DEACTIVATED\n",
      "üí° To enable RAG functionality:\n",
      "   1. Set RAG_ENABLED = True at the top of the script\n",
      "   2. Ensure langchain dependencies are installed\n",
      "   3. Re-run the script\n",
      "\n",
      "üéâ Process complete! Check extracted_events_norag_with_multi_kg_Claude.ttl for RDF results.\n",
      "üìä System ran in NON-RAG mode with Claude 4 - only Knowledge Graph and Location enrichment was used.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Multi-Knowledge Graph RAG System with Text Chunking - CLAUDE 4 VERSION\n",
    "Handles large texts by processing them in chunks to avoid token limits\n",
    "RAG FUNCTIONALITY DEACTIVATED\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal\n",
    "from rdflib.namespace import XSD, SKOS\n",
    "\n",
    "# Configuration\n",
    "INPUT_TEXT_FILE = \"part_aa\"\n",
    "ONTOLOGY_PATH = \"wiki.owl\"\n",
    "LOCATION_ONTOLOGY_PATH = \"locations.owl\"\n",
    "OUTPUT_RAG_TTL = 'extracted_events_norag_with_multi_kg_Claude.ttl'\n",
    "OUTPUT_RAG_OWL = 'extracted_events_norag_with_multi_kg_Claude.owl'\n",
    "KG_CACHE_FILE = 'kg_cache.json'\n",
    "LOCATION_CACHE_FILE = 'location_cache.json'\n",
    "KG_ANALYSIS_REPORT = 'multi_kg_analysis_report.txt'\n",
    "\n",
    "# Token limits - UPDATED FOR CLAUDE 4\n",
    "MAX_TOKENS_PER_REQUEST = 150000  # Claude 4 has higher token limits\n",
    "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
    "\n",
    "# RAG DEACTIVATION FLAG\n",
    "RAG_ENABLED = False  # Set to False to deactivate RAG\n",
    "\n",
    "# Namespaces\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "STE = Namespace(\"http://www.example.org/ste#\")\n",
    "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
    "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
    "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
    "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
    "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
    "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Imports - UPDATED FOR CLAUDE\n",
    "try:\n",
    "    if RAG_ENABLED:\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "    # CHANGED: Import Anthropic instead of OpenAI\n",
    "    import anthropic\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    print(\"pip install rdflib python-dotenv anthropic langchain langchain-community faiss-cpu sentence-transformers tiktoken requests\")\n",
    "    exit(1)\n",
    "\n",
    "@dataclass\n",
    "class LocationInfo:\n",
    "    \"\"\"Location information with coordinates\"\"\"\n",
    "    name: str\n",
    "    latitude: Optional[float] = None\n",
    "    longitude: Optional[float] = None\n",
    "    country: Optional[str] = None\n",
    "    region: Optional[str] = None\n",
    "    source: str = \"extracted\"\n",
    "    confidence: float = 1.0\n",
    "    uri: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class EnhancedKnowledgeFact:\n",
    "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    source: str\n",
    "    confidence: float = 1.0\n",
    "    context: Optional[str] = None\n",
    "    temporal: Optional[str] = None\n",
    "    spatial: Optional[str] = None\n",
    "    evidence_score: float = 1.0\n",
    "    source_uri: Optional[str] = None\n",
    "\n",
    "class LocationExtractor:\n",
    "    \"\"\"Extracts and enriches location information\"\"\"\n",
    "    \n",
    "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
    "        self.ontology_path = ontology_path\n",
    "        self.location_graph = None\n",
    "        self.location_cache = self._load_location_cache()\n",
    "        self.load_location_ontology()\n",
    "        \n",
    "    def _load_location_cache(self) -> Dict:\n",
    "        \"\"\"Load location cache\"\"\"\n",
    "        if os.path.exists(LOCATION_CACHE_FILE):\n",
    "            try:\n",
    "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load location cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_location_cache(self):\n",
    "        \"\"\"Save location cache\"\"\"\n",
    "        try:\n",
    "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save location cache: {e}\")\n",
    "    \n",
    "    def load_location_ontology(self):\n",
    "        \"\"\"Load locations.owl ontology\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.ontology_path):\n",
    "                self.location_graph = Graph()\n",
    "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
    "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
    "            else:\n",
    "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
    "                self.location_graph = None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading location ontology: {e}\")\n",
    "            self.location_graph = None\n",
    "    \n",
    "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract potential location names from text\"\"\"\n",
    "        location_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
    "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
    "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
    "        ]\n",
    "        \n",
    "        locations = []\n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            locations.extend(matches)\n",
    "        \n",
    "        location_stopwords = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', \n",
    "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
    "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', \n",
    "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday', \n",
    "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "        }\n",
    "        \n",
    "        filtered_locations = []\n",
    "        for loc in locations:\n",
    "            loc = loc.strip()\n",
    "            if (loc not in location_stopwords and len(loc) > 2 and \n",
    "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
    "                filtered_locations.append(loc)\n",
    "        \n",
    "        return list(set(filtered_locations))\n",
    "    \n",
    "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location info from local ontology\"\"\"\n",
    "        if not self.location_graph:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
    "                ?location rdfs:label ?label .\n",
    "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
    "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
    "                OPTIONAL {{ ?location geo:long ?long }}\n",
    "                OPTIONAL {{ ?location dbp:country ?country }}\n",
    "                OPTIONAL {{ ?location dbp:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            results = self.location_graph.query(query)\n",
    "            for row in results:\n",
    "                return LocationInfo(\n",
    "                    name=location_name,\n",
    "                    latitude=float(row.lat) if row.lat else None,\n",
    "                    longitude=float(row.long) if row.long else None,\n",
    "                    country=str(row.country) if row.country else None,\n",
    "                    region=str(row.region) if row.region else None,\n",
    "                    source=\"local_ontology\",\n",
    "                    uri=str(row.location) if row.location else None\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
    "                <{entity_uri}> geo:lat ?lat ;\n",
    "                               geo:long ?long .\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
    "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                bindings = data.get('results', {}).get('bindings', [])\n",
    "                \n",
    "                if bindings:\n",
    "                    binding = bindings[0]\n",
    "                    return LocationInfo(\n",
    "                        name=location_name,\n",
    "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
    "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
    "                        country=binding.get('country', {}).get('value', ''),\n",
    "                        region=binding.get('region', {}).get('value', ''),\n",
    "                        source=\"dbpedia\",\n",
    "                        uri=entity_uri\n",
    "                    )\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Try multiple query strategies to get the right location\n",
    "            queries = [\n",
    "                # Try exact label match first\n",
    "                f\"\"\"\n",
    "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
    "                  ?item rdfs:label \"{location_name}\"@en .\n",
    "                  ?item wdt:P625 ?coord .\n",
    "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
    "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}\n",
    "                LIMIT 5\n",
    "                \"\"\",\n",
    "                # Try with additional filters for places/locations\n",
    "                f\"\"\"\n",
    "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
    "                  ?item rdfs:label \"{location_name}\"@en .\n",
    "                  ?item wdt:P625 ?coord .\n",
    "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
    "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
    "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "                }}\n",
    "                LIMIT 5\n",
    "                \"\"\"\n",
    "            ]\n",
    "            \n",
    "            for query in queries:\n",
    "                params = {'query': query, 'format': 'json'}\n",
    "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    bindings = data.get('results', {}).get('bindings', [])\n",
    "                    \n",
    "                    if bindings:\n",
    "                        # Prefer results with country information\n",
    "                        best_binding = None\n",
    "                        for binding in bindings:\n",
    "                            if binding.get('country'):\n",
    "                                best_binding = binding\n",
    "                                break\n",
    "                        \n",
    "                        if not best_binding:\n",
    "                            best_binding = bindings[0]\n",
    "                        \n",
    "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
    "                        \n",
    "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
    "                        if coord_match:\n",
    "                            longitude = float(coord_match.group(1))\n",
    "                            latitude = float(coord_match.group(2))\n",
    "                            \n",
    "                            return LocationInfo(\n",
    "                                name=location_name,\n",
    "                                latitude=latitude,\n",
    "                                longitude=longitude,\n",
    "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
    "                                source=\"wikidata\",\n",
    "                                uri=best_binding.get('item', {}).get('value', '')\n",
    "                            )\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
    "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
    "        if not location_info.latitude or not location_info.longitude:\n",
    "            return True\n",
    "        \n",
    "        lat, lon = location_info.latitude, location_info.longitude\n",
    "        \n",
    "        # Basic coordinate range validation\n",
    "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
    "            return False\n",
    "        \n",
    "        # Generic geographic validation - flag obviously wrong coordinates\n",
    "        # If coordinates suggest North America but no clear indication it should be there\n",
    "        if (-130 < lon < -60) and (25 < lat < 50):  # North America range\n",
    "            logger.warning(f\"Coordinates for '{location_info.name}' appear to be in North America ({lat}, {lon}). \"\n",
    "                         f\"Please verify if this is correct for your historical context.\")\n",
    "            # Don't auto-correct, just warn - let the user/context decide\n",
    "        \n",
    "        # If coordinates suggest Australia/Oceania for what might be European/Mediterranean names\n",
    "        elif (110 < lon < 180) and (-45 < lat < -10):  # Australia/Oceania range\n",
    "            logger.warning(f\"Coordinates for '{location_info.name}' appear to be in Australia/Oceania ({lat}, {lon}). \"\n",
    "                         f\"Please verify if this is correct for your historical context.\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
    "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
    "        if location_name in self.location_cache:\n",
    "            cached = self.location_cache[location_name]\n",
    "            return LocationInfo(**cached) if cached else None\n",
    "        \n",
    "        location_info = None\n",
    "        \n",
    "        location_info = self.get_location_from_ontology(location_name)\n",
    "        \n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_wikidata(location_name)\n",
    "        \n",
    "        if not location_info:\n",
    "            location_info = self.get_location_from_dbpedia(location_name)\n",
    "        \n",
    "        if location_info:\n",
    "            self.location_cache[location_name] = {\n",
    "                'name': location_info.name,\n",
    "                'latitude': location_info.latitude,\n",
    "                'longitude': location_info.longitude,\n",
    "                'country': location_info.country,\n",
    "                'region': location_info.region,\n",
    "                'source': location_info.source,\n",
    "                'confidence': location_info.confidence,\n",
    "                'uri': location_info.uri\n",
    "            }\n",
    "        else:\n",
    "            self.location_cache[location_name] = None\n",
    "        \n",
    "        self._save_location_cache()\n",
    "        \n",
    "        if location_info:\n",
    "            self.validate_coordinates(location_info)\n",
    "        \n",
    "        return location_info\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"Handles text chunking to manage token limits\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"claude-3-5-sonnet-20241022\"):  # CHANGED: Default to Claude model\n",
    "        # Use GPT tokenizer as approximation for Claude tokens\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text (approximation for Claude)\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 25000) -> List[str]:  # CHANGED: Increased for Claude\n",
    "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class BaseKGConnector:\n",
    "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
    "        self.name = name\n",
    "        self.base_url = base_url\n",
    "        self.rate_limit = rate_limit\n",
    "        self.last_request_time = 0\n",
    "        self.request_count = 0\n",
    "        self.success_count = 0\n",
    "        \n",
    "    def _rate_limit_wait(self):\n",
    "        \"\"\"Enforce rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < self.rate_limit:\n",
    "            time.sleep(self.rate_limit - time_since_last)\n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get connector statistics\"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'requests': self.request_count,\n",
    "            'successes': self.success_count,\n",
    "            'success_rate': self.success_count / max(1, self.request_count)\n",
    "        }\n",
    "    \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class EnhancedWikidataConnector(BaseKGConnector):\n",
    "    \"\"\"Wikidata connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
    "              {{\n",
    "                ?subject ?label \"{entity}\"@en .\n",
    "              }} UNION {{\n",
    "                ?subject rdfs:label \"{entity}\"@en .\n",
    "              }}\n",
    "              \n",
    "              ?subject ?predicate ?object .\n",
    "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
    "              \n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
    "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
    "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
    "                        source=self.name,\n",
    "                        confidence=0.9,\n",
    "                        source_uri=binding.get('subject', {}).get('value')\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
    "                return facts\n",
    "            else:\n",
    "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedDBpediaConnector(BaseKGConnector):\n",
    "    \"\"\"DBpedia connector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            \n",
    "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
    "            \n",
    "            sparql_query = f\"\"\"\n",
    "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
    "              <{entity_uri}> ?predicate ?object .\n",
    "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
    "              FILTER(!isBlank(?object))\n",
    "            }}\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "            \n",
    "            params = {'query': sparql_query, 'format': 'json'}\n",
    "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                facts = []\n",
    "                \n",
    "                for binding in data.get('results', {}).get('bindings', []):\n",
    "                    predicate = binding.get('predicate', {}).get('value', '')\n",
    "                    obj = binding.get('object', {}).get('value', '')\n",
    "                    \n",
    "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
    "                    \n",
    "                    fact = EnhancedKnowledgeFact(\n",
    "                        subject=entity,\n",
    "                        predicate=predicate_name,\n",
    "                        object=obj,\n",
    "                        source=self.name,\n",
    "                        confidence=0.85,\n",
    "                        source_uri=entity_uri\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                \n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
    "                return facts\n",
    "            else:\n",
    "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "class EnhancedConceptNetConnector(BaseKGConnector):\n",
    "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
    "        \n",
    "    def search_related_concepts(self, entity: str) -> List[str]:\n",
    "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
    "        try:\n",
    "            # Try search API first\n",
    "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
    "            response = requests.get(search_url, timeout=10)\n",
    "            \n",
    "            related_concepts = []\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                for edge in data.get('edges', []):\n",
    "                    start = edge.get('start', {}).get('label', '')\n",
    "                    end = edge.get('end', {}).get('label', '')\n",
    "                    \n",
    "                    # Extract concept paths and clean them\n",
    "                    for concept_path in [start, end]:\n",
    "                        if concept_path and '/c/en/' in concept_path:\n",
    "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
    "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
    "                                related_concepts.append(concept)\n",
    "            \n",
    "            return list(set(related_concepts))[:5]  # Return top 5 unique concepts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
    "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
    "        try:\n",
    "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
    "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
    "            \n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data.get('edges', [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
    "        try:\n",
    "            self._rate_limit_wait()\n",
    "            all_facts = []\n",
    "            \n",
    "            # Strategy 1: Try direct query first\n",
    "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
    "            \n",
    "            # Strategy 2: Search for related concepts and query them\n",
    "            related_concepts = self.search_related_concepts(entity)\n",
    "            \n",
    "            # Process direct edges\n",
    "            for edge in direct_edges:\n",
    "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
    "                if fact:\n",
    "                    all_facts.append(fact)\n",
    "            \n",
    "            # Process related concept edges\n",
    "            for concept in related_concepts:\n",
    "                concept_edges = self.query_concept_directly(concept, 5)\n",
    "                for edge in concept_edges:\n",
    "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
    "                    if fact:\n",
    "                        all_facts.append(fact)\n",
    "            \n",
    "            if all_facts:\n",
    "                self.success_count += 1\n",
    "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
    "                if related_concepts:\n",
    "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
    "            \n",
    "            return all_facts[:limit]\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
    "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
    "        try:\n",
    "            start = edge.get('start', {})\n",
    "            end = edge.get('end', {})\n",
    "            relation = edge.get('rel', {})\n",
    "            weight = edge.get('weight', 1.0)\n",
    "            \n",
    "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
    "            rel_label = relation.get('label', 'related_to')\n",
    "            \n",
    "            # Skip if labels are empty or too short\n",
    "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
    "                return None\n",
    "            \n",
    "            # Determine confidence based on discovery method\n",
    "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
    "            \n",
    "            return EnhancedKnowledgeFact(\n",
    "                subject=original_entity,\n",
    "                predicate=rel_label,\n",
    "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
    "                source=self.name,\n",
    "                confidence=min(weight * confidence_multiplier, 1.0),\n",
    "                context=f\"Discovered {discovery_method}\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
    "            return None\n",
    "\n",
    "class MultiKGCache:\n",
    "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "        \n",
    "    def _load_cache(self) -> Dict:\n",
    "        \"\"\"Load cache from file\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load cache: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save cache: {e}\")\n",
    "    \n",
    "    def get_cache_key(self, source: str, entity: str) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
    "    \n",
    "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Get cached facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
    "        \"\"\"Cache facts\"\"\"\n",
    "        key = self.get_cache_key(source, entity)\n",
    "        serializable_facts = []\n",
    "        for fact in facts:\n",
    "            serializable_facts.append({\n",
    "                'subject': fact.subject,\n",
    "                'predicate': fact.predicate,\n",
    "                'object': fact.object,\n",
    "                'source': fact.source,\n",
    "                'confidence': fact.confidence,\n",
    "                'context': fact.context,\n",
    "                'temporal': fact.temporal,\n",
    "                'spatial': fact.spatial,\n",
    "                'evidence_score': fact.evidence_score,\n",
    "                'source_uri': fact.source_uri\n",
    "            })\n",
    "        self.cache[key] = serializable_facts\n",
    "        self._save_cache()\n",
    "\n",
    "class EnhancedMultiKGRAGSystem:\n",
    "    \"\"\"Multi-Knowledge Graph system with RAG functionality DEACTIVATED - CLAUDE 4 VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connectors = {\n",
    "            'wikidata': EnhancedWikidataConnector(),\n",
    "            'dbpedia': EnhancedDBpediaConnector(),\n",
    "            'conceptnet': EnhancedConceptNetConnector()\n",
    "        }\n",
    "        self.cache = MultiKGCache()\n",
    "        self.chunker = TextChunker()\n",
    "        self.location_extractor = LocationExtractor()\n",
    "        self.global_locations = {}\n",
    "        # RAG components deactivated\n",
    "        self.vectorstore = None  \n",
    "        self.document_chunks = []  \n",
    "        self.stats = {\n",
    "            'queries_processed': 0,\n",
    "            'entities_extracted': 0,\n",
    "            'facts_retrieved': 0,\n",
    "            'cache_hits': 0,\n",
    "            'chunks_processed': 0,\n",
    "            'locations_found': 0,\n",
    "            'locations_with_coordinates': 0,\n",
    "            'location_duplicates_avoided': 0,\n",
    "            'rag_queries': 0\n",
    "        }\n",
    "        \n",
    "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entities from text\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
    "        matches = re.findall(pattern, text)\n",
    "        entities.extend(matches)\n",
    "        \n",
    "        stop_words = {\n",
    "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
    "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
    "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
    "        }\n",
    "        \n",
    "        filtered_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.strip()\n",
    "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
    "                filtered_entities.append(entity)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for entity in filtered_entities:\n",
    "            if entity.lower() not in seen:\n",
    "                seen.add(entity.lower())\n",
    "                unique_entities.append(entity)\n",
    "        \n",
    "        return unique_entities[:15]\n",
    "    \n",
    "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
    "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
    "        all_facts = {}\n",
    "        cache_hits = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for entity in entities:\n",
    "                for source_name, connector in self.connectors.items():\n",
    "                    # Check cache first\n",
    "                    cached_facts = self.cache.get(source_name, entity)\n",
    "                    if cached_facts:\n",
    "                        cache_hits += 1\n",
    "                        if entity not in all_facts:\n",
    "                            all_facts[entity] = []\n",
    "                        for fact_data in cached_facts:\n",
    "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
    "                            all_facts[entity].append(fact)\n",
    "                    else:\n",
    "                        future = executor.submit(connector.retrieve_facts, entity, 3)\n",
    "                        futures[future] = (entity, source_name)\n",
    "            \n",
    "            # Collect results with better timeout handling\n",
    "            completed = 0\n",
    "            total_futures = len(futures)\n",
    "            \n",
    "            try:\n",
    "                for future in as_completed(futures, timeout=45):  # Increased timeout\n",
    "                    entity, source_name = futures[future]\n",
    "                    completed += 1\n",
    "                    \n",
    "                    try:\n",
    "                        facts = future.result(timeout=5)  # Individual future timeout\n",
    "                        if facts:\n",
    "                            self.cache.set(source_name, entity, facts)\n",
    "                            \n",
    "                            if entity not in all_facts:\n",
    "                                all_facts[entity] = []\n",
    "                            all_facts[entity].extend(facts)\n",
    "                            \n",
    "                            self.stats['facts_retrieved'] += len(facts)\n",
    "                        \n",
    "                        logger.debug(f\"‚úÖ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ùå {source_name} failed for {entity}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except TimeoutError:\n",
    "                pending_count = total_futures - completed\n",
    "                logger.warning(f\"‚è∞ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
    "                \n",
    "                # Cancel remaining futures\n",
    "                for future in futures:\n",
    "                    if not future.done():\n",
    "                        future.cancel()\n",
    "        \n",
    "        self.stats['cache_hits'] += cache_hits\n",
    "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
    "        return all_facts\n",
    "    \n",
    "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
    "        \"\"\"Format KG facts into context string\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for entity, facts in kg_facts.items():\n",
    "            if facts:\n",
    "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
    "                \n",
    "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
    "                \n",
    "                by_source = {}\n",
    "                for fact in sorted_facts[:3]:\n",
    "                    if fact.source not in by_source:\n",
    "                        by_source[fact.source] = []\n",
    "                    by_source[fact.source].append(fact)\n",
    "                \n",
    "                for source, source_facts in by_source.items():\n",
    "                    context_parts.append(f\"\\nFrom {source}:\")\n",
    "                    for fact in source_facts[:2]:\n",
    "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
    "                        if fact.confidence < 0.8:\n",
    "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
    "                        context_parts.append(fact_str)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
    "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
    "        location_key = location_info.name.lower().strip()\n",
    "        \n",
    "        if location_key in self.global_locations:\n",
    "            existing = self.global_locations[location_key]\n",
    "            if (location_info.latitude and location_info.longitude and \n",
    "                (not existing.latitude or not existing.longitude)):\n",
    "                self.global_locations[location_key] = location_info\n",
    "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
    "            else:\n",
    "                self.stats['location_duplicates_avoided'] += 1\n",
    "                logger.debug(f\"Location {location_info.name} already registered\")\n",
    "        else:\n",
    "            self.global_locations[location_key] = location_info\n",
    "            logger.info(f\"Registered new location: {location_info.name}\")\n",
    "        \n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
    "        return f\"ste:Location_{clean_name}\"\n",
    "    \n",
    "    def process_chunk(self, chunk: str, chunk_num: int, claude_client) -> str:  # CHANGED: Parameter name\n",
    "        \"\"\"Process a single chunk of text WITHOUT RAG (RAG DEACTIVATED) - CLAUDE 4 VERSION\"\"\"\n",
    "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars) - RAG DISABLED - USING CLAUDE 4\")\n",
    "        \n",
    "        # RAG retrieval DEACTIVATED - skip this step\n",
    "        # relevant_context = \"\"\n",
    "        \n",
    "        # Extract entities and locations (this remains the same)\n",
    "        entities = self.extract_entities_advanced(chunk)\n",
    "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
    "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
    "        \n",
    "        # Enrich locations with coordinates\n",
    "        enriched_locations = {}\n",
    "        for location_name in locations[:10]:\n",
    "            location_info = self.location_extractor.enrich_location(location_name)\n",
    "            if location_info:\n",
    "                self.register_global_location(location_info)\n",
    "                enriched_locations[location_name] = location_info\n",
    "                self.stats['locations_found'] += 1\n",
    "                if location_info.latitude and location_info.longitude:\n",
    "                    self.stats['locations_with_coordinates'] += 1\n",
    "        \n",
    "        if not entities and not enriched_locations:\n",
    "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
    "            return \"\"\n",
    "        \n",
    "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
    "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
    "        \n",
    "        # Get KG facts for entities (this remains the same)\n",
    "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
    "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
    "        location_context = self.format_location_context(enriched_locations)\n",
    "        \n",
    "        # SIMPLIFIED PROMPT WITHOUT RAG - OPTIMIZED FOR CLAUDE 4\n",
    "        simplified_prompt = f\"\"\"You are extracting historical events from text chunks. Use knowledge graph facts and location coordinates to enhance your extraction.\n",
    "\n",
    "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
    "{chunk}\n",
    "\n",
    "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
    "{kg_context}\n",
    "\n",
    "LOCATION INFORMATION WITH COORDINATES:\n",
    "{location_context}\n",
    "\n",
    "TASK: Extract ONLY the events that are actually mentioned in the current text chunk.\n",
    "\n",
    "Requirements:\n",
    "1. Extract ONLY events mentioned in the CURRENT text chunk\n",
    "2. Use KG facts to enhance entity information\n",
    "3. Use location coordinates to provide precise geographical data\n",
    "4. Include ALL these properties for each event:\n",
    "   - ste:hasType (description of event)\n",
    "   - ste:hasAgent (who caused/led the event)\n",
    "   - ste:hasTime (when it happened)\n",
    "   - ste:hasLocation (location name from text)\n",
    "   - ste:hasLatitude (latitude coordinate if available)\n",
    "   - ste:hasLongitude (longitude coordinate if available)\n",
    "   - ste:hasCountry (country if available)\n",
    "   - ste:hasRegion (region if available)\n",
    "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
    "   - ste:hasResult (outcome/consequence)\n",
    "\n",
    "Output format (do not include prefixes, they will be added later):\n",
    "```turtle\n",
    "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
    "    ste:hasType \"specific description from current chunk\" ;\n",
    "    ste:hasAgent \"specific person from current chunk\" ;\n",
    "    ste:hasTime \"specific date from current chunk\" ;\n",
    "    ste:hasLocation \"specific location from current chunk\" ;\n",
    "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
    "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
    "    ste:hasCountry \"Italy\" ;\n",
    "    ste:hasRegion \"Sicily\" ;\n",
    "    ste:hasLocationSource \"wikidata\" ;\n",
    "    ste:hasResult \"specific outcome from current chunk\" .\n",
    "```\n",
    "\n",
    "IMPORTANT: \n",
    "- Extract events ONLY from the CURRENT text chunk\n",
    "- Use KG facts to enrich entity details\n",
    "- Include precise coordinates from location sources\n",
    "- Only extract events explicitly mentioned in the current chunk\n",
    "- If no clear events are found in current chunk, return empty\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # CHANGED: Claude API call instead of OpenAI\n",
    "            response = claude_client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",  # Latest Claude model\n",
    "                max_tokens=4000,\n",
    "                temperature=0,\n",
    "                messages=[{\"role\": \"user\", \"content\": simplified_prompt}]\n",
    "            )\n",
    "            \n",
    "            turtle_output = self.clean_turtle(response.content[0].text)\n",
    "            self.stats['chunks_processed'] += 1\n",
    "            logger.info(f\"Generated RDF for chunk {chunk_num} (Claude 4, without RAG)\")\n",
    "            return turtle_output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing chunk {chunk_num} with Claude: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
    "        \"\"\"Format location information into context string\"\"\"\n",
    "        if not enriched_locations:\n",
    "            return \"No location coordinates available.\"\n",
    "        \n",
    "        context_parts = [\"\\n=== Location Information ===\"]\n",
    "        \n",
    "        for location_name, location_info in enriched_locations.items():\n",
    "            context_parts.append(f\"\\n{location_name}:\")\n",
    "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
    "            \n",
    "            if location_info.latitude and location_info.longitude:\n",
    "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
    "                if location_info.source == \"corrected\":\n",
    "                    context_parts.append(f\"  - NOTE: Coordinates were corrected for historical accuracy\")\n",
    "            else:\n",
    "                context_parts.append(\"  - Coordinates: Not available\")\n",
    "            \n",
    "            if location_info.country:\n",
    "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
    "            \n",
    "            if location_info.region:\n",
    "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
    "            \n",
    "            if location_info.uri:\n",
    "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_global_location_rdf(self) -> str:\n",
    "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
    "        if not self.global_locations:\n",
    "            return \"\"\n",
    "        \n",
    "        location_rdf_parts = []\n",
    "        \n",
    "        for location_key, location_info in self.global_locations.items():\n",
    "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
    "            location_id = f\"ste:Location_{clean_name}\"\n",
    "            \n",
    "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
    "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
    "            \n",
    "            if location_info.latitude and location_info.longitude:\n",
    "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
    "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
    "            \n",
    "            if location_info.country:\n",
    "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
    "            \n",
    "            if location_info.region:\n",
    "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
    "            \n",
    "            if location_info.source:\n",
    "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
    "            \n",
    "            if location_info.uri:\n",
    "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
    "            \n",
    "            if rdf_lines[-1].endswith(' ;'):\n",
    "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
    "            \n",
    "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
    "        \n",
    "        return '\\n\\n'.join(location_rdf_parts)\n",
    "    \n",
    "    def clean_turtle(self, raw_output: str) -> str:\n",
    "        \"\"\"Clean turtle output\"\"\"\n",
    "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "        \n",
    "        lines = raw_output.strip().split('\\n')\n",
    "        turtle_lines = []\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if (stripped.startswith('@') or stripped.startswith('<') or \n",
    "                stripped.startswith(':') or stripped.startswith('_') or \n",
    "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
    "                turtle_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(turtle_lines)\n",
    "\n",
    "    # RAG METHODS DEACTIVATED\n",
    "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
    "        \"\"\"RAG DEACTIVATED: Vector store preparation disabled\"\"\"\n",
    "        logger.info(\"RAG functionality is DEACTIVATED - vectorstore not created\")\n",
    "        return False\n",
    "    \n",
    "    def rag_query(self, query: str, claude_client, k: int = 20) -> Dict[str, Any]:  # CHANGED: Parameter name\n",
    "        \"\"\"RAG DEACTIVATED: RAG queries disabled\"\"\"\n",
    "        return {\"error\": \"RAG functionality is DEACTIVATED. Set RAG_ENABLED=True to enable RAG features.\"}\n",
    "    \n",
    "    def interactive_rag_session(self, claude_client):  # CHANGED: Parameter name\n",
    "        \"\"\"RAG DEACTIVATED: Interactive RAG session disabled\"\"\"\n",
    "        print(\"\\n‚ùå RAG functionality is DEACTIVATED\")\n",
    "        print(\"To enable RAG, set RAG_ENABLED=True at the top of the script\")\n",
    "\n",
    "# Utility functions\n",
    "def load_api_key():\n",
    "    \"\"\"Load Anthropic API key\"\"\"  # CHANGED: Comment\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")  # CHANGED: Environment variable name\n",
    "    if not api_key:\n",
    "        print(\"Error: ANTHROPIC_API_KEY not found in environment variables\")  # CHANGED: Error message\n",
    "        print(\"Please set your Anthropic API key:\")\n",
    "        print(\"export ANTHROPIC_API_KEY='your-api-key-here'\")\n",
    "        return None\n",
    "    print(\"Anthropic API Key loaded successfully.\")  # CHANGED: Success message\n",
    "    return api_key\n",
    "\n",
    "def load_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Load text from file\"\"\"\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        print(f\"Loaded text from {filepath}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def initialize_claude_client(api_key: str):  # CHANGED: Function name and logic\n",
    "    \"\"\"Initialize Claude client\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    try:\n",
    "        client = anthropic.Anthropic(api_key=api_key)\n",
    "        print(\"Claude 4 client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Claude client: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
    "    \"\"\"RAG DEACTIVATED: Vector store creation disabled\"\"\"\n",
    "    if not RAG_ENABLED:\n",
    "        logger.info(\"RAG functionality is DEACTIVATED - vectorstore not created\")\n",
    "        return None\n",
    "    \n",
    "    # Original code would go here if RAG_ENABLED was True\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with chunking support (RAG DEACTIVATED) - CLAUDE 4 VERSION\"\"\"\n",
    "    print(\"üöÄ Starting Multi-Knowledge Graph System with Chunking (RAG DEACTIVATED) - CLAUDE 4 VERSION\")\n",
    "    \n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        return\n",
    "    \n",
    "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
    "    if not domain_text:\n",
    "        print(\"‚ö†Ô∏è  No input file found, using sample text\")\n",
    "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC. \n",
    "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes. \n",
    "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
    "    else:\n",
    "        print(f\"üìÑ Using YOUR text from {INPUT_TEXT_FILE}\")\n",
    "        print(f\"üìù Text length: {len(domain_text)} characters\")\n",
    "    \n",
    "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
    "    claude_client = initialize_claude_client(api_key)  # CHANGED: Function call\n",
    "    \n",
    "    if not claude_client:  # CHANGED: Variable name\n",
    "        return\n",
    "    \n",
    "    # Vector store preparation SKIPPED (RAG deactivated)\n",
    "    print(\"\\n‚ùå RAG vector store setup SKIPPED (RAG is DEACTIVATED)\")\n",
    "    \n",
    "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
    "    print(f\"üî¢ Total tokens in text: {token_count:,}\")\n",
    "    \n",
    "    if token_count > 25000:  # CHANGED: Increased threshold for Claude\n",
    "        print(\"üìä Text is large, chunking into smaller pieces...\")\n",
    "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=25000)  # CHANGED: Increased\n",
    "        print(f\"üìÑ Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"üìÑ Text is small enough to process as single chunk\")\n",
    "        chunks = [domain_text]\n",
    "    \n",
    "    # Extract events and create RDF (without RAG)\n",
    "    all_turtle_outputs = []\n",
    "    all_entities = set()\n",
    "    \n",
    "    print(\"\\nüîÑ Processing chunks for event extraction (without RAG, using Claude 4)...\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\nüîÑ Processing chunk {i}/{len(chunks)} with Claude 4...\")\n",
    "        \n",
    "        turtle_output = multi_kg_system.process_chunk(chunk, i, claude_client)  # CHANGED: Parameter\n",
    "        if turtle_output:\n",
    "            all_turtle_outputs.append(turtle_output)\n",
    "            \n",
    "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
    "        all_entities.update(chunk_entities)\n",
    "        \n",
    "        if i < len(chunks):\n",
    "            time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Save RDF output\n",
    "    if all_turtle_outputs:\n",
    "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
    "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
    "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        final_output = prefixes + \"# Historical Events with Knowledge Graph Enhanced Location Data (RAG DEACTIVATED, CLAUDE 4)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
    "        \n",
    "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_output)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Saved RDF to {OUTPUT_RAG_TTL}\")\n",
    "        print(f\"üìä Processing Statistics (Claude 4):\")\n",
    "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
    "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
    "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
    "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
    "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
    "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
    "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
    "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
    "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
    "        print(f\"   - LLM used: Claude 4 (Anthropic)\")  # CHANGED: Added LLM info\n",
    "        print(f\"   - RAG status: DEACTIVATED\")\n",
    "        \n",
    "        print(f\"\\nüîó Knowledge Graph Connector Statistics:\")\n",
    "        for name, connector in multi_kg_system.connectors.items():\n",
    "            stats = connector.get_stats()\n",
    "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
    "        \n",
    "        if multi_kg_system.location_extractor.location_cache:\n",
    "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
    "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
    "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample of generated RDF:\")\n",
    "        print(\"=\"*60)\n",
    "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No events were extracted from any chunks\")\n",
    "    \n",
    "    # RAG SESSION DEACTIVATED\n",
    "    print(f\"\\n‚ùå RAG System is DEACTIVATED\")\n",
    "    print(f\"üí° To enable RAG functionality:\")\n",
    "    print(f\"   1. Set RAG_ENABLED = True at the top of the script\")\n",
    "    print(f\"   2. Ensure langchain dependencies are installed\")\n",
    "    print(f\"   3. Re-run the script\")\n",
    "    \n",
    "    print(f\"\\nüéâ Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
    "    print(f\"üìä System ran in NON-RAG mode with Claude 4 - only Knowledge Graph and Location enrichment was used.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
