{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjLPDTQr4VJV",
        "outputId": "313a7a11-eeba-4cee-ad82-726c920e650d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 0.2.0 requires sentence-transformers>=2.6.0, but you have sentence-transformers 2.4.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "✅ Numpy reinstalled. Now RESTART the runtime (Runtime > Restart runtime) before continuing.\n"
          ]
        }
      ],
      "source": [
        "# 🚑 Fix for \"Numpy is not available\" in Colab\n",
        "\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.4\n",
        "# (Optional) Also reinstall other core packages if needed:\n",
        "# !pip install --upgrade --force-reinstall torch transformers pandas\n",
        "\n",
        "print(\"✅ Numpy reinstalled. Now RESTART the runtime (Runtime > Restart runtime) before continuing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Ollama and Start Server\n",
        "\n",
        "# 1. Download and install Ollama\n",
        "# This script automatically downloads the correct version for your Colab environment (Linux)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 2. Start the Ollama server in the background\n",
        "# We use nohup to ensure it runs even if the cell output is cleared.\n",
        "# We also redirect output to a file and run it in the background (&).\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# 3. Give Ollama a moment to start\n",
        "import time\n",
        "print(\"Waiting for Ollama server to start...\")\n",
        "time.sleep(10) # Give it 10 seconds to fully initialize\n",
        "print(\"Ollama server should be running.\")\n",
        "\n",
        "# (Optional) Verify Ollama is running by checking its process\n",
        "!ps aux | grep ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsH_JoaPGbY8",
        "outputId": "9ff18cd9-41a6-42ad-bfa5-2eca87675b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Waiting for Ollama server to start...\n",
            "Ollama server should be running.\n",
            "root        3154  1.8  0.0 6828428 43116 ?       Sl   16:36   0:00 ollama serve\n",
            "root        3208  0.0  0.0   7376  3504 ?        S    16:36   0:00 /bin/bash -c ps aux | grep ollama\n",
            "root        3210  0.0  0.0   6484  2272 ?        S    16:36   0:00 grep ollama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list\n",
        "\n",
        "# If nothing shows up, pull the model\n",
        "!ollama pull llama3.2\n",
        "\n",
        "# Check again\n",
        "!ollama list\n",
        "\n",
        "# If you want to see more details about what's happening\n",
        "!ollama show llama3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ReKk2H45yJM",
        "outputId": "17b3cbfd-86ad-4b93-cb1e-bab90ef417f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME    ID    SIZE    MODIFIED \n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "NAME               ID              SIZE      MODIFIED               \n",
            "llama3.2:latest    a80c4f17acd5    2.0 GB    Less than a second ago    \n",
            "  Model\n",
            "    architecture        llama     \n",
            "    parameters          3.2B      \n",
            "    context length      131072    \n",
            "    embedding length    3072      \n",
            "    quantization        Q4_K_M    \n",
            "\n",
            "  Capabilities\n",
            "    completion    \n",
            "    tools         \n",
            "\n",
            "  Parameters\n",
            "    stop    \"<|start_header_id|>\"    \n",
            "    stop    \"<|end_header_id|>\"      \n",
            "    stop    \"<|eot_id|>\"             \n",
            "\n",
            "  License\n",
            "    LLAMA 3.2 COMMUNITY LICENSE AGREEMENT                 \n",
            "    Llama 3.2 Version Release Date: September 25, 2024    \n",
            "    ...                                                   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Ollama and Start Server\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Download and install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 2. Start the Ollama server in the background\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# 3. Give Ollama a moment to start\n",
        "import time\n",
        "print(\"Waiting for Ollama server to start...\")\n",
        "time.sleep(10)\n",
        "print(\"Ollama server should be running.\")\n",
        "\n",
        "# Verify Ollama is running\n",
        "!ps aux | grep ollama\n",
        "\n",
        "# Cell 2: Pull Llama Model\n",
        "# Pull the Llama model we want to use\n",
        "!ollama pull llama3.2\n",
        "\n",
        "print(\"✅ Llama 3.2 model downloaded successfully!\")\n",
        "\n",
        "\"\"\"\n",
        "TRUE BASE GENERATION SYSTEM - LOCAL OLLAMA VERSION\n",
        "No external knowledge sources - pure LLM generation from text only\n",
        "All external APIs, Knowledge Graphs, and Location enrichment DISABLED\n",
        "Uses LOCAL Ollama instead of HuggingFace authentication\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import logging\n",
        "import requests\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Configuration\n",
        "INPUT_TEXT_FILE = \"/content/drive/MyDrive/part_aa\"\n",
        "OUTPUT_BASE_TTL = '/content/drive/MyDrive/extracted_events_base_generation_ollama.ttl'\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "# TRUE BASE GENERATION FLAGS - All external knowledge DISABLED\n",
        "RAG_ENABLED = False\n",
        "KNOWLEDGE_GRAPHS_ENABLED = False\n",
        "LOCATION_ENRICHMENT_ENABLED = False\n",
        "EXTERNAL_APIS_ENABLED = False\n",
        "\n",
        "# Token limits\n",
        "MAX_TOKENS_PER_REQUEST = 100000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class OllamaLLM:\n",
        "    \"\"\"Local Ollama LLM wrapper\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"llama3.2\", base_url: str = OLLAMA_BASE_URL):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = base_url\n",
        "        self.generate_url = f\"{base_url}/api/generate\"\n",
        "\n",
        "        print(f\"🦙 Initializing Local Ollama with model: {model_name}\")\n",
        "\n",
        "        # Test connection to Ollama\n",
        "        try:\n",
        "            response = requests.get(f\"{base_url}/api/tags\", timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                models = response.json().get('models', [])\n",
        "                model_names = [m['name'] for m in models]\n",
        "                print(f\"✅ Connected to Ollama. Available models: {model_names}\")\n",
        "\n",
        "                if any(model_name in name for name in model_names):\n",
        "                    print(f\"✅ Model {model_name} is available!\")\n",
        "                else:\n",
        "                    print(f\"⚠️ Model {model_name} not found. Attempting to pull...\")\n",
        "                    self.pull_model()\n",
        "            else:\n",
        "                raise Exception(f\"Ollama server returned status {response.status_code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error connecting to Ollama: {e}\")\n",
        "            print(\"💡 Make sure Ollama server is running: ollama serve\")\n",
        "            raise\n",
        "\n",
        "    def pull_model(self):\n",
        "        \"\"\"Pull the model if it's not available\"\"\"\n",
        "        try:\n",
        "            print(f\"🔄 Pulling model {self.model_name}...\")\n",
        "            pull_url = f\"{self.base_url}/api/pull\"\n",
        "            response = requests.post(pull_url, json={\"name\": self.model_name}, timeout=300)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                print(f\"✅ Successfully pulled model {self.model_name}\")\n",
        "            else:\n",
        "                print(f\"❌ Failed to pull model: {response.text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error pulling model: {e}\")\n",
        "\n",
        "    def generate(self, prompt: str, **kwargs) -> str:\n",
        "        \"\"\"Generate response using Ollama\"\"\"\n",
        "        try:\n",
        "            payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"temperature\": kwargs.get(\"temperature\", 0.7),\n",
        "                    \"num_predict\": kwargs.get(\"max_length\", 200),\n",
        "                    \"top_p\": kwargs.get(\"top_p\", 0.9)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                self.generate_url,\n",
        "                json=payload,\n",
        "                timeout=120  # 2 minute timeout\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result.get(\"response\", \"\")\n",
        "            else:\n",
        "                logger.error(f\"Ollama request failed: {response.status_code} - {response.text}\")\n",
        "                return \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating with Ollama: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Handles text chunking to manage token limits\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"llama3.2\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Approximate token count (roughly 4 chars per token)\"\"\"\n",
        "        return len(text) // 4\n",
        "\n",
        "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 1000) -> List[str]:\n",
        "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class TrueBaseGenerationSystem:\n",
        "    \"\"\"TRUE Base Generation System - No external knowledge sources\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.chunker = TextChunker()\n",
        "        self.stats = {\n",
        "            'chunks_processed': 0,\n",
        "            'events_extracted': 0,\n",
        "            'external_api_calls': 0,  # This will stay 0\n",
        "            'knowledge_sources_used': 0,  # This will stay 0\n",
        "        }\n",
        "        logger.info(\"TRUE BASE GENERATION SYSTEM INITIALIZED\")\n",
        "        logger.info(\"❌ Knowledge Graphs: DISABLED\")\n",
        "        logger.info(\"❌ Location Enrichment: DISABLED\")\n",
        "        logger.info(\"❌ External APIs: DISABLED\")\n",
        "        logger.info(\"❌ RAG Text Retrieval: DISABLED\")\n",
        "        logger.info(\"✅ Pure LLM Generation: ENABLED\")\n",
        "\n",
        "    def extract_basic_entities_from_text_only(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract entities using ONLY pattern matching - no external validation\"\"\"\n",
        "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        stop_words = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If',\n",
        "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During'\n",
        "        }\n",
        "\n",
        "        filtered_entities = []\n",
        "        for entity in matches:\n",
        "            entity = entity.strip()\n",
        "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
        "                filtered_entities.append(entity)\n",
        "\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for entity in filtered_entities:\n",
        "            if entity.lower() not in seen:\n",
        "                seen.add(entity.lower())\n",
        "                unique_entities.append(entity)\n",
        "\n",
        "        return unique_entities[:5]\n",
        "\n",
        "    def extract_basic_locations_from_text_only(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract locations using ONLY pattern matching - no coordinate lookup\"\"\"\n",
        "        location_patterns = [\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
        "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
        "        ]\n",
        "\n",
        "        locations = []\n",
        "        for pattern in location_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            locations.extend(matches)\n",
        "\n",
        "        location_stopwords = {\n",
        "            'The', 'This', 'That', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
        "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "            'September', 'October', 'November', 'December'\n",
        "        }\n",
        "\n",
        "        filtered_locations = []\n",
        "        for loc in locations:\n",
        "            loc = loc.strip()\n",
        "            if (loc not in location_stopwords and len(loc) > 2 and not loc.isdigit()):\n",
        "                filtered_locations.append(loc)\n",
        "\n",
        "        return list(set(filtered_locations))[:3]\n",
        "\n",
        "    def process_chunk_true_base(self, chunk: str, chunk_num: int, llm) -> str:\n",
        "        \"\"\"Process chunk with TRUE base generation - no external knowledge\"\"\"\n",
        "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars) - TRUE BASE GENERATION\")\n",
        "\n",
        "        entities = self.extract_basic_entities_from_text_only(chunk)\n",
        "        locations = self.extract_basic_locations_from_text_only(chunk)\n",
        "\n",
        "        logger.info(f\"Found entities (text-only): {entities[:3]}...\")\n",
        "        logger.info(f\"Found locations (text-only): {locations[:3]}...\")\n",
        "\n",
        "        if not entities and not locations:\n",
        "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
        "            return \"\"\n",
        "\n",
        "        base_prompt = f\"\"\"You are extracting historical events from text using ONLY the information provided in the text chunk. Do not use external knowledge sources, but you CAN and SHOULD make reasonable inferences from the text.\n",
        "\n",
        "TEXT CHUNK {chunk_num} TO ANALYZE:\n",
        "{chunk}\n",
        "\n",
        "ENTITIES FOUND IN TEXT: {', '.join(entities) if entities else 'None'}\n",
        "LOCATIONS FOUND IN TEXT: {', '.join(locations) if locations else 'None'}\n",
        "\n",
        "TASK: Extract historical events mentioned in this text chunk using the text information and making REASONABLE INFERENCES.\n",
        "\n",
        "REQUIREMENTS:\n",
        "1. Extract ONLY events explicitly mentioned in the text chunk\n",
        "2. Use information directly stated in the text\n",
        "3. MAKE REASONABLE INFERENCES from context clues in the text\n",
        "4. If you can reasonably infer coordinates, countries, regions from textual context, DO IT\n",
        "5. Include ALL these properties for each event:\n",
        "   - ste:hasType (description of event, enhanced with context)\n",
        "   - ste:hasAgent (who caused/led the event, with inferred roles)\n",
        "   - ste:hasTime (when it happened, with inferred specificity)\n",
        "   - ste:hasLocation (location name from text)\n",
        "   - ste:hasLatitude (infer approximate coordinates if you can from text context)\n",
        "   - ste:hasLongitude (infer approximate coordinates if you can from text context)\n",
        "   - ste:hasCountry (infer country from textual geographic context)\n",
        "   - ste:hasRegion (infer region from textual geographic context)\n",
        "   - ste:hasLocationSource \"inferred\" (if you made geographic inferences)\n",
        "   - ste:hasResult (outcome, enhanced with contextual inference)\n",
        "\n",
        "INFERENCE GUIDELINES:\n",
        "- If text mentions \"Athens\", infer it's in Greece, approximate coordinates\n",
        "- If text mentions \"Sicily\", infer it's in Italy, Mediterranean coordinates\n",
        "- If text mentions \"Sparta/Lacedaemon\", infer Peloponnese, Greece\n",
        "- If you know from context clues what geographic region events occurred in, infer coordinates\n",
        "- If someone is called \"King\", infer royal title\n",
        "- If text implies timeframes, infer more specific dates\n",
        "- If outcomes are implied, infer likely results\n",
        "\n",
        "Output format (do not include prefixes):\n",
        "```turtle\n",
        "ste:Event{chunk_num}_1 a ste:Event ;\n",
        "    ste:hasType \"specific event type inferred from context\" ;\n",
        "    ste:hasAgent \"person/group with inferred roles\" ;\n",
        "    ste:hasTime \"time period with inferred specificity\" ;\n",
        "    ste:hasLocation \"location name from text\" ;\n",
        "    ste:hasLatitude \"37.9838\" ;\n",
        "    ste:hasLongitude \"23.7275\" ;\n",
        "    ste:hasCountry \"Greece\" ;\n",
        "    ste:hasRegion \"Attica\" ;\n",
        "    ste:hasLocationSource \"inferred\" ;\n",
        "    ste:hasResult \"outcome inferred from context\" .\n",
        "```\n",
        "\n",
        "CRITICAL:\n",
        "- Use the SAME output format as enhanced systems for fair comparison\n",
        "- INFER coordinates, countries, regions if you can reasonably deduce them from text\n",
        "- Make the events as detailed and specific as possible through inference\n",
        "- If you truly cannot infer something, then use empty string \"\"\n",
        "- The goal is to extract maximum information through text analysis and inference\n",
        "\n",
        "If no clear historical events are mentioned in the text, return empty.\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Generate response using Ollama\n",
        "            generated_text = llm.generate(\n",
        "                base_prompt,\n",
        "                temperature=0.7,\n",
        "                max_length=400\n",
        "            )\n",
        "\n",
        "            # DEBUG: Print what the model actually generated\n",
        "            print(f\"🔍 Raw model output for chunk {chunk_num}:\")\n",
        "            print(f\"'{generated_text[:200]}...'\")\n",
        "\n",
        "            turtle_output = self.clean_turtle(generated_text)\n",
        "\n",
        "            # DEBUG: Print cleaned output\n",
        "            print(f\"🔍 Cleaned turtle output:\")\n",
        "            print(f\"'{turtle_output[:200]}...'\")\n",
        "\n",
        "            if turtle_output:\n",
        "                self.stats['chunks_processed'] += 1\n",
        "                event_count = turtle_output.count('ste:Event')\n",
        "                self.stats['events_extracted'] += event_count\n",
        "                logger.info(f\"Generated {event_count} events from chunk {chunk_num}\")\n",
        "            else:\n",
        "                print(f\"⚠️ No turtle output after cleaning for chunk {chunk_num}\")\n",
        "\n",
        "            return turtle_output\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing chunk {chunk_num}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_turtle(self, raw_output: str) -> str:\n",
        "        \"\"\"Clean turtle output\"\"\"\n",
        "        # Look for turtle code blocks first\n",
        "        import re\n",
        "        turtle_match = re.search(r'```(?:turtle)?\\s*(.*?)```', raw_output, re.DOTALL | re.IGNORECASE)\n",
        "        if turtle_match:\n",
        "            raw_output = turtle_match.group(1)\n",
        "\n",
        "        lines = raw_output.strip().split('\\n')\n",
        "        turtle_lines = []\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if (stripped.startswith('ste:') or\n",
        "                stripped.startswith('a ') or\n",
        "                ':' in stripped and ('hasType' in stripped or 'hasAgent' in stripped or 'hasTime' in stripped)):\n",
        "                turtle_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(turtle_lines)\n",
        "\n",
        "# Utility functions\n",
        "def load_api_key():\n",
        "    \"\"\"Ollama doesn't need an API key\"\"\"\n",
        "    print(\"✅ Using Local Ollama - no API key needed.\")\n",
        "    return \"local\"\n",
        "\n",
        "def load_text_from_file(filepath: str) -> str:\n",
        "    \"\"\"Load text from file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(f\"❌ File not found: {filepath}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        print(f\"✅ Loaded text from {filepath}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error reading file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def initialize_llm(api_key: str):\n",
        "    \"\"\"Initialize Local Ollama\"\"\"\n",
        "    try:\n",
        "        print(\"🦙 Initializing Local Ollama...\")\n",
        "        llm = OllamaLLM(model_name=\"llama3.2\")\n",
        "        print(\"✅ Local Ollama initialized successfully.\")\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing Ollama: {e}\")\n",
        "        print(\"💡 Make sure Ollama server is running: ollama serve\")\n",
        "        print(\"💡 And the model is pulled: ollama pull llama3.2\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function - TRUE BASE GENERATION WITH LOCAL OLLAMA\"\"\"\n",
        "    print(\"🚀 Starting TRUE BASE GENERATION SYSTEM with Local Ollama\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"🦙 LLM: Local Ollama (No Authentication Required)\")\n",
        "    print(\"❌ Knowledge Graphs: DISABLED\")\n",
        "    print(\"❌ Location Coordinate Lookup: DISABLED\")\n",
        "    print(\"❌ External APIs: DISABLED\")\n",
        "    print(\"❌ RAG Text Retrieval: DISABLED\")\n",
        "    print(\"❌ All External Knowledge Sources: DISABLED\")\n",
        "    print(\"✅ Pure LLM Generation from Text Only: ENABLED\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    api_key = load_api_key()\n",
        "    if not api_key:\n",
        "        return\n",
        "\n",
        "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
        "    if not domain_text:\n",
        "        print(\"⚠️  No input file found, using sample text\")\n",
        "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC.\n",
        "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes.\n",
        "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
        "    else:\n",
        "        print(f\"📄 Using text from {INPUT_TEXT_FILE}\")\n",
        "        print(f\"📝 Text length: {len(domain_text)} characters\")\n",
        "\n",
        "    base_system = TrueBaseGenerationSystem()\n",
        "    llm = initialize_llm(api_key)\n",
        "\n",
        "    if not llm:\n",
        "        return\n",
        "\n",
        "    token_count = base_system.chunker.count_tokens(domain_text)\n",
        "    print(f\"🔢 Total tokens in text: {token_count:,}\")\n",
        "\n",
        "    # Use smaller chunks for Ollama\n",
        "    if token_count > 2000:  # Reduced for Ollama\n",
        "        print(\"📊 Text is large, chunking into smaller pieces...\")\n",
        "        chunks = base_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=10000)\n",
        "        print(f\"📄 Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(\"📄 Text is small enough to process as single chunk\")\n",
        "        chunks = [domain_text]\n",
        "\n",
        "    # Process chunks with TRUE base generation\n",
        "    all_turtle_outputs = []\n",
        "\n",
        "    print(\"\\n🔄 Processing chunks with TRUE BASE GENERATION...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"\\n🔄 Processing chunk {i}/{len(chunks)}...\")\n",
        "\n",
        "        turtle_output = base_system.process_chunk_true_base(chunk, i, llm)\n",
        "        if turtle_output:\n",
        "            all_turtle_outputs.append(turtle_output)\n",
        "\n",
        "        if i < len(chunks):\n",
        "            time.sleep(2)  # Pause between chunks for Ollama\n",
        "\n",
        "    # Save RDF output\n",
        "    if all_turtle_outputs:\n",
        "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
        "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
        "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
        "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        final_output = prefixes + \"# TRUE BASE GENERATION - No External Knowledge Sources (Local Ollama)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
        "\n",
        "        with open(OUTPUT_BASE_TTL, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output)\n",
        "\n",
        "        print(f\"\\n✅ Saved TRUE BASE GENERATION RDF to {OUTPUT_BASE_TTL}\")\n",
        "        print(f\"📊 TRUE BASE GENERATION Statistics:\")\n",
        "        print(f\"   - LLM: Local Ollama (No Authentication)\")\n",
        "        print(f\"   - Generation Mode: PURE BASE (No External Knowledge)\")\n",
        "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
        "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
        "        print(f\"   - Events extracted: {base_system.stats['events_extracted']}\")\n",
        "\n",
        "        print(f\"\\n📝 Sample of TRUE BASE GENERATION RDF:\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_output[:400] + \"...\" if len(final_output) > 400 else final_output)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No events were extracted from any chunks\")\n",
        "\n",
        "    print(f\"\\n🎉 TRUE BASE GENERATION complete!\")\n",
        "    print(f\"📄 Output file: {OUTPUT_BASE_TTL}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKSrAmjxR7St",
        "outputId": "7cc8e9fc-4055-457c-cbfd-d30de4d319c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Waiting for Ollama server to start...\n",
            "Ollama server should be running.\n",
            "root       32227  1.7  0.0 6827888 44068 ?       Sl   09:47   0:00 ollama serve\n",
            "root       32283  0.0  0.0   7376  3516 ?        S    09:47   0:00 /bin/bash -c ps aux | grep ollama\n",
            "root       32285  0.0  0.0   6484  2192 ?        S    09:47   0:00 grep ollama\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "✅ Llama 3.2 model downloaded successfully!\n",
            "🚀 Starting TRUE BASE GENERATION SYSTEM with Local Ollama\n",
            "============================================================\n",
            "🦙 LLM: Local Ollama (No Authentication Required)\n",
            "❌ Knowledge Graphs: DISABLED\n",
            "❌ Location Coordinate Lookup: DISABLED\n",
            "❌ External APIs: DISABLED\n",
            "❌ RAG Text Retrieval: DISABLED\n",
            "❌ All External Knowledge Sources: DISABLED\n",
            "✅ Pure LLM Generation from Text Only: ENABLED\n",
            "============================================================\n",
            "✅ Using Local Ollama - no API key needed.\n",
            "✅ Loaded text from /content/drive/MyDrive/part_aa\n",
            "📄 Using text from /content/drive/MyDrive/part_aa\n",
            "📝 Text length: 398568 characters\n",
            "🦙 Initializing Local Ollama...\n",
            "🦙 Initializing Local Ollama with model: llama3.2\n",
            "✅ Connected to Ollama. Available models: ['llama3.2:latest', 'llama3:latest']\n",
            "✅ Model llama3.2 is available!\n",
            "✅ Local Ollama initialized successfully.\n",
            "🔢 Total tokens in text: 99,642\n",
            "📊 Text is large, chunking into smaller pieces...\n",
            "📄 Created 10 chunks\n",
            "\n",
            "🔄 Processing chunks with TRUE BASE GENERATION...\n",
            "\n",
            "🔄 Processing chunk 1/10...\n",
            "🔍 Raw model output for chunk 1:\n",
            "'```turtle\n",
            "ste:TheCorcyraeansDeclareWarOnEpidamnus_1 a ste:Event ;\n",
            "    ste:hasType \"Declaration of War\" ;\n",
            "    ste:hasAgent \"Corcyraeans\" ;\n",
            "    ste:hasTime \"During the Peloponnesian War, specifically af...'\n",
            "🔍 Cleaned turtle output:\n",
            "'ste:TheCorcyraeansDeclareWarOnEpidamnus_1 a ste:Event ;\n",
            "    ste:hasType \"Declaration of War\" ;\n",
            "    ste:hasAgent \"Corcyraeans\" ;\n",
            "    ste:hasTime \"During the Peloponnesian War, specifically after the Ba...'\n",
            "\n",
            "🔄 Processing chunk 2/10...\n",
            "🔍 Raw model output for chunk 2:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "1. **The revolt of Potidæa**\n",
            "   - ste:hasType \"battle\"\n",
            "   - ste:hasAgent \"Potidæans and Peloponnesians\"\n",
            "   - ste:hasTime \"time period a...'\n",
            "🔍 Cleaned turtle output:\n",
            "'   - ste:hasType \"battle\"\n",
            "   - ste:hasAgent \"Potidæans and Peloponnesians\"\n",
            "   - ste:hasTime \"time period after the Athenian ships arrived in Macedonia\" ;\n",
            "   - ste:hasType \"military campaign\"\n",
            "   - ste:...'\n",
            "\n",
            "🔄 Processing chunk 3/10...\n",
            "🔍 Raw model output for chunk 3:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "**Event 1: Lacedaemonian dismissal of Athenian ambassadors**\n",
            "\n",
            "ste:Event1_1 a ste:Event ;\n",
            "    ste:hasType \"diplomatic event\" ;\n",
            "    ste:h...'\n",
            "🔍 Cleaned turtle output:\n",
            "'ste:Event1_1 a ste:Event ;\n",
            "    ste:hasType \"diplomatic event\" ;\n",
            "    ste:hasAgent \"Lacedaemonian authorities\" ;\n",
            "    ste:hasTime \"around 479 BCE\" ;\n",
            "    ste:hasLocation \"Sparta, Peloponnese, Greece\" ;\n",
            "  ...'\n",
            "\n",
            "🔄 Processing chunk 4/10...\n",
            "🔍 Raw model output for chunk 4:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "```turtle\n",
            "ste:Event1_1 a ste:Event ;\n",
            "    ste:hasType \"Diasia festival at Athens\" ;\n",
            "    ste:hasAgent \"Cylon and his companions\" ;\n",
            "    st...'\n",
            "🔍 Cleaned turtle output:\n",
            "'ste:Event1_1 a ste:Event ;\n",
            "    ste:hasType \"Diasia festival at Athens\" ;\n",
            "    ste:hasAgent \"Cylon and his companions\" ;\n",
            "    ste:hasTime \"former generations, specific date not mentioned\" ;\n",
            "    ste:hasLo...'\n",
            "\n",
            "🔄 Processing chunk 5/10...\n",
            "🔍 Raw model output for chunk 5:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "```turtle\n",
            "ste:Event5_1 a ste:Event ;\n",
            "    ste:hasType \"Invasion of Attica by Thebans\" ;\n",
            "    ste:hasAgent \"Theban forces, led by Eurymach...'\n",
            "🔍 Cleaned turtle output:\n",
            "'ste:Event5_1 a ste:Event ;\n",
            "    ste:hasType \"Invasion of Attica by Thebans\" ;\n",
            "    ste:hasAgent \"Theban forces, led by Eurymachus\" ;\n",
            "    ste:hasTime \"After the affair at Plataea\" ;\n",
            "    ste:hasLocation \"...'\n",
            "\n",
            "🔄 Processing chunk 6/10...\n",
            "🔍 Raw model output for chunk 6:\n",
            "'Unfortunately, there are no explicit historical events mentioned in this text chunk. However, I can provide some inferred information based on the context.\n",
            "\n",
            "Since the text mentions \"Athens\" and \"Pelop...'\n",
            "🔍 Cleaned turtle output:\n",
            "'ste:Event  ....'\n",
            "\n",
            "🔄 Processing chunk 7/10...\n",
            "🔍 Raw model output for chunk 7:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "1. **Slaying of Athenian ambassadors**\n",
            "    - ste:hasType \"Execution\"\n",
            "    - ste:hasAgent \"Sitalces\" ;\n",
            "        - ste:hasRole \"King/Leader...'\n",
            "🔍 Cleaned turtle output:\n",
            "'    - ste:hasType \"Execution\"\n",
            "    - ste:hasAgent \"Sitalces\" ;\n",
            "    - ste:hasTime \"end of summer, same year as Pericles' death\"\n",
            "    - ste:hasType \"Battle\"\n",
            "    - ste:hasAgent \"Corinthian forces\" ;\n",
            "    - ...'\n",
            "\n",
            "🔄 Processing chunk 8/10...\n",
            "🔍 Raw model output for chunk 8:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "1. **Battle of Achaean Rhium**\n",
            "\t* ste:hasType \" naval battle\"\n",
            "\t* ste:hasAgent \"Peloponnesian fleet led by Cnemus and Brasidas\"\n",
            "\t* ste:h...'\n",
            "🔍 Cleaned turtle output:\n",
            "'\t* ste:hasType \" naval battle\"\n",
            "\t* ste:hasAgent \"Peloponnesian fleet led by Cnemus and Brasidas\"\n",
            "\t* ste:hasTime \"summer, before winter\"\n",
            "\t* ste:hasType \" naval invasion\"\n",
            "\t* ste:hasAgent \"Peloponnesian f...'\n",
            "\n",
            "🔄 Processing chunk 9/10...\n",
            "🔍 Raw model output for chunk 9:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "1. **Plataean Escape**\n",
            "   - ste:hasType \"Mass escape from Peloponnesian walls\"\n",
            "   - ste:hasAgent \"Plataeans (300 men)\"\n",
            "   - ste:hasTime...'\n",
            "🔍 Cleaned turtle output:\n",
            "'   - ste:hasType \"Mass escape from Peloponnesian walls\"\n",
            "   - ste:hasAgent \"Plataeans (300 men)\"\n",
            "   - ste:hasTime \"During stormy night, when moon was not visible\"\n",
            "   - ste:hasType \"Announcement of impe...'\n",
            "\n",
            "🔄 Processing chunk 10/10...\n",
            "🔍 Raw model output for chunk 10:\n",
            "'Here are the extracted historical events with inferred details:\n",
            "\n",
            "```turtle\n",
            "ste:Event10_1 a ste:Event ;\n",
            "    ste:hasType \"Trial of Plataeans\" ;\n",
            "    ste:hasAgent \"Peloponnesian commander\" ;\n",
            "    ste:hasTi...'\n",
            "🔍 Cleaned turtle output:\n",
            "'ste:Event10_1 a ste:Event ;\n",
            "    ste:hasType \"Trial of Plataeans\" ;\n",
            "    ste:hasAgent \"Peloponnesian commander\" ;\n",
            "    ste:hasTime \"Summer, 5th year of war\" ;\n",
            "    ste:hasLocation \"Plataea\" ;\n",
            "    ste:hasL...'\n",
            "\n",
            "✅ Saved TRUE BASE GENERATION RDF to /content/drive/MyDrive/extracted_events_base_generation_ollama.ttl\n",
            "📊 TRUE BASE GENERATION Statistics:\n",
            "   - LLM: Local Ollama (No Authentication)\n",
            "   - Generation Mode: PURE BASE (No External Knowledge)\n",
            "   - Total chunks processed: 10\n",
            "   - Successful chunks: 10\n",
            "   - Events extracted: 24\n",
            "\n",
            "📝 Sample of TRUE BASE GENERATION RDF:\n",
            "============================================================\n",
            "@prefix ste: <http://www.example.org/ste#> .\n",
            "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
            "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
            "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
            "\n",
            "# TRUE BASE GENERATION - No External Knowledge Sources (Local Ollama)\n",
            "ste:TheCorcyraeansDeclareWarOnEpidamnus_1 a ste:Event ;\n",
            "    ste:hasType \"Declaration of War\" ;\n",
            "    ste:hasAgent \"Cor...\n",
            "============================================================\n",
            "\n",
            "🎉 TRUE BASE GENERATION complete!\n",
            "📄 Output file: /content/drive/MyDrive/extracted_events_base_generation_ollama.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Pull the llama3.2 model\n",
        "\n",
        "print(\"Pulling llama3.2 model (this may take a while)...\")\n",
        "# You can check the progress in the output of this cell\n",
        "!ollama pull llama3.2\n",
        "\n",
        "print(\"\\nModel 'llama3.2' should now be available.\")\n",
        "# (Optional) List installed models to confirm\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3jdcLgCGfA6",
        "outputId": "1abe4d38-43fe-49b8-ae7f-02617ab7994f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulling llama3.2 model (this may take a while)...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\n",
            "Model 'llama3.2' should now be available.\n",
            "NAME               ID              SIZE      MODIFIED               \n",
            "llama3.2:latest    a80c4f17acd5    2.0 GB    Less than a second ago    \n",
            "llama3:latest      365c0bd3c000    4.7 GB    25 minutes ago            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"✅ Numpy is available:\", np.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0dSUVa4Efo7",
        "outputId": "078b0e53-d24e-4040-b129-f92b7ba4609c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Numpy is available: 1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 1: Mount Drive and Install Dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Dependency Compatibility Guard ---\n",
        "import os\n",
        "\n",
        "# Force compatible versions for transformers and sentence-transformers\n",
        "os.system(\"pip install transformers==4.40.2 sentence-transformers==2.4.0 --quiet\")\n",
        "\n",
        "# (Optional) If you use torch, ensure it's compatible too:\n",
        "# os.system(\"pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu --quiet\")\n",
        "\n",
        "# Now safe to import\n",
        "\n",
        "!pip install  torch accelerate rdflib\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Enhanced Multi-Knowledge Graph RAG System with Text Chunking - LLAMA VERSION\n",
        "Handles large texts by processing them in chunks to avoid token limits\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import requests\n",
        "\n",
        "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal # <--- This line is crucial\n",
        "from rdflib.namespace import XSD, SKOS\n",
        "# Add a direct import test here to diagnose immediately\n",
        "print(\"\\nVerifying Langchain Ollama import...\")\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "    print(\"✅ ChatOllama successfully imported after reinstallation.\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ CRITICAL ERROR: ChatOllama still cannot be imported after reinstallation: {e}\")\n",
        "    print(\"Please check your pip installation output for errors and ensure your Colab runtime is healthy.\")\n",
        "    # You might want to exit here if this is a persistent issue\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"❌ An unexpected error occurred during ChatOllama import verification: {e}\")\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "\n",
        "\n",
        "# Configuration\n",
        "INPUT_TEXT_FILE = \"/content/drive/MyDrive/part_aa\"\n",
        "ONTOLOGY_PATH = \"/content/drive/MyDrive/wiki.owl\"\n",
        "LOCATION_ONTOLOGY_PATH = \"/content/drive/MyDrive/locations.owl\"\n",
        "OUTPUT_RAG_TTL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl'\n",
        "OUTPUT_RAG_OWL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.owl'\n",
        "KG_CACHE_FILE = '/content/drive/MyDrive/kg_cache.json'\n",
        "LOCATION_CACHE_FILE = '/content/drive/MyDrive/location_cache.json'\n",
        "KG_ANALYSIS_REPORT = '/content/drive/MyDrive/multi_kg_analysis_report.txt'\n",
        "\n",
        "# Token limits - adjusted for Llama\n",
        "MAX_TOKENS_PER_REQUEST = 50000  # Conservative limit for Llama\n",
        "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
        "\n",
        "# Namespaces\n",
        "EX = Namespace(\"http://example.org/\")\n",
        "STE = Namespace(\"http://www.example.org/ste#\")\n",
        "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
        "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
        "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
        "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
        "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
        "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Imports\n",
        "try:\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_ollama import ChatOllama  # Changed from OpenAI to Ollama\n",
        "    from langchain.schema import HumanMessage\n",
        "except ImportError as e:\n",
        "    print(f\"ImportError: {e}\")\n",
        "    print(\"pip install rdflib python-dotenv langchain langchain-ollama langchain-community faiss-cpu sentence-transformers requests\")\n",
        "    exit(1)\n",
        "\n",
        "@dataclass\n",
        "class LocationInfo:\n",
        "    \"\"\"Location information with coordinates\"\"\"\n",
        "    name: str\n",
        "    latitude: Optional[float] = None\n",
        "    longitude: Optional[float] = None\n",
        "    country: Optional[str] = None\n",
        "    region: Optional[str] = None\n",
        "    source: str = \"extracted\"\n",
        "    confidence: float = 1.0\n",
        "    uri: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class EnhancedKnowledgeFact:\n",
        "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
        "    subject: str\n",
        "    predicate: str\n",
        "    object: str\n",
        "    source: str\n",
        "    confidence: float = 1.0\n",
        "    context: Optional[str] = None\n",
        "    temporal: Optional[str] = None\n",
        "    spatial: Optional[str] = None\n",
        "    evidence_score: float = 1.0\n",
        "    source_uri: Optional[str] = None\n",
        "\n",
        "class LocationExtractor:\n",
        "    \"\"\"Extracts and enriches location information\"\"\"\n",
        "\n",
        "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
        "        self.ontology_path = ontology_path\n",
        "        self.location_graph = None\n",
        "        self.location_cache = self._load_location_cache()\n",
        "        self.load_location_ontology()\n",
        "\n",
        "    def _load_location_cache(self) -> Dict:\n",
        "        \"\"\"Load location cache\"\"\"\n",
        "        if os.path.exists(LOCATION_CACHE_FILE):\n",
        "            try:\n",
        "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load location cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_location_cache(self):\n",
        "        \"\"\"Save location cache\"\"\"\n",
        "        try:\n",
        "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save location cache: {e}\")\n",
        "\n",
        "    def load_location_ontology(self):\n",
        "        \"\"\"Load locations.owl ontology\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.ontology_path):\n",
        "                self.location_graph = Graph()\n",
        "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
        "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
        "                self.location_graph = None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading location ontology: {e}\")\n",
        "            self.location_graph = None\n",
        "\n",
        "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract potential location names from text\"\"\"\n",
        "        location_patterns = [\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
        "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
        "        ]\n",
        "\n",
        "        locations = []\n",
        "        for pattern in location_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            locations.extend(matches)\n",
        "\n",
        "        location_stopwords = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If',\n",
        "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
        "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday',\n",
        "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
        "        }\n",
        "\n",
        "        filtered_locations = []\n",
        "        for loc in locations:\n",
        "            loc = loc.strip()\n",
        "            if (loc not in location_stopwords and len(loc) > 2 and\n",
        "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
        "                filtered_locations.append(loc)\n",
        "\n",
        "        return list(set(filtered_locations))\n",
        "\n",
        "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location info from local ontology\"\"\"\n",
        "        if not self.location_graph:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            query = f\"\"\"\n",
        "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
        "                ?location rdfs:label ?label .\n",
        "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
        "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
        "                OPTIONAL {{ ?location geo:long ?long }}\n",
        "                OPTIONAL {{ ?location dbp:country ?country }}\n",
        "                OPTIONAL {{ ?location dbp:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            results = self.location_graph.query(query)\n",
        "            for row in results:\n",
        "                return LocationInfo(\n",
        "                    name=location_name,\n",
        "                    latitude=float(row.lat) if row.lat else None,\n",
        "                    longitude=float(row.long) if row.long else None,\n",
        "                    country=str(row.country) if row.country else None,\n",
        "                    region=str(row.region) if row.region else None,\n",
        "                    source=\"local_ontology\",\n",
        "                    uri=str(row.location) if row.location else None\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
        "                <{entity_uri}> geo:lat ?lat ;\n",
        "                               geo:long ?long .\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                if bindings:\n",
        "                    binding = bindings[0]\n",
        "                    return LocationInfo(\n",
        "                        name=location_name,\n",
        "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
        "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
        "                        country=binding.get('country', {}).get('value', ''),\n",
        "                        region=binding.get('region', {}).get('value', ''),\n",
        "                        source=\"dbpedia\",\n",
        "                        uri=entity_uri\n",
        "                    )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            # Try multiple query strategies to get the right location\n",
        "            queries = [\n",
        "                # Try exact label match first\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\",\n",
        "                # Try with additional filters for places/locations\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\"\n",
        "            ]\n",
        "\n",
        "            for query in queries:\n",
        "                params = {'query': query, 'format': 'json'}\n",
        "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                    if bindings:\n",
        "                        # Prefer results with country information\n",
        "                        best_binding = None\n",
        "                        for binding in bindings:\n",
        "                            if binding.get('country'):\n",
        "                                best_binding = binding\n",
        "                                break\n",
        "\n",
        "                        if not best_binding:\n",
        "                            best_binding = bindings[0]\n",
        "\n",
        "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
        "\n",
        "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
        "                        if coord_match:\n",
        "                            longitude = float(coord_match.group(1))\n",
        "                            latitude = float(coord_match.group(2))\n",
        "\n",
        "                            return LocationInfo(\n",
        "                                name=location_name,\n",
        "                                latitude=latitude,\n",
        "                                longitude=longitude,\n",
        "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
        "                                source=\"wikidata\",\n",
        "                                uri=best_binding.get('item', {}).get('value', '')\n",
        "                            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
        "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
        "        if not location_info.latitude or not location_info.longitude:\n",
        "            return True\n",
        "\n",
        "        lat, lon = location_info.latitude, location_info.longitude\n",
        "\n",
        "        # Basic coordinate range validation\n",
        "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
        "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
        "        if location_name in self.location_cache:\n",
        "            cached = self.location_cache[location_name]\n",
        "            return LocationInfo(**cached) if cached else None\n",
        "\n",
        "        location_info = None\n",
        "\n",
        "        location_info = self.get_location_from_ontology(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_wikidata(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_dbpedia(location_name)\n",
        "\n",
        "        if location_info:\n",
        "            self.location_cache[location_name] = {\n",
        "                'name': location_info.name,\n",
        "                'latitude': location_info.latitude,\n",
        "                'longitude': location_info.longitude,\n",
        "                'country': location_info.country,\n",
        "                'region': location_info.region,\n",
        "                'source': location_info.source,\n",
        "                'confidence': location_info.confidence,\n",
        "                'uri': location_info.uri\n",
        "            }\n",
        "        else:\n",
        "            self.location_cache[location_name] = None\n",
        "\n",
        "        self._save_location_cache()\n",
        "\n",
        "        if location_info:\n",
        "            self.validate_coordinates(location_info)\n",
        "\n",
        "        return location_info\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Handles text chunking to manage token limits for Llama\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"llama3.2\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Approximate token count for Llama (roughly 4 chars per token)\"\"\"\n",
        "        return len(text) // 4\n",
        "\n",
        "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 10000) -> List[str]:\n",
        "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class BaseKGConnector:\n",
        "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
        "        self.name = name\n",
        "        self.base_url = base_url\n",
        "        self.rate_limit = rate_limit\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "        self.success_count = 0\n",
        "\n",
        "    def _rate_limit_wait(self):\n",
        "        \"\"\"Enforce rate limiting\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "        if time_since_last < self.rate_limit:\n",
        "            time.sleep(self.rate_limit - time_since_last)\n",
        "        self.last_request_time = time.time()\n",
        "        self.request_count += 1\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get connector statistics\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'requests': self.request_count,\n",
        "            'successes': self.success_count,\n",
        "            'success_rate': self.success_count / max(1, self.request_count)\n",
        "        }\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EnhancedWikidataConnector(BaseKGConnector):\n",
        "    \"\"\"Wikidata connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
        "              {{\n",
        "                ?subject ?label \"{entity}\"@en .\n",
        "              }} UNION {{\n",
        "                ?subject rdfs:label \"{entity}\"@en .\n",
        "              }}\n",
        "\n",
        "              ?subject ?predicate ?object .\n",
        "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
        "\n",
        "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
        "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
        "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
        "                        source=self.name,\n",
        "                        confidence=0.9,\n",
        "                        source_uri=binding.get('subject', {}).get('value')\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedDBpediaConnector(BaseKGConnector):\n",
        "    \"\"\"DBpedia connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
        "              <{entity_uri}> ?predicate ?object .\n",
        "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
        "              FILTER(!isBlank(?object))\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    predicate = binding.get('predicate', {}).get('value', '')\n",
        "                    obj = binding.get('object', {}).get('value', '')\n",
        "\n",
        "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
        "\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=entity,\n",
        "                        predicate=predicate_name,\n",
        "                        object=obj,\n",
        "                        source=self.name,\n",
        "                        confidence=0.85,\n",
        "                        source_uri=entity_uri\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedConceptNetConnector(BaseKGConnector):\n",
        "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
        "\n",
        "    def search_related_concepts(self, entity: str) -> List[str]:\n",
        "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
        "        try:\n",
        "            # Try search API first\n",
        "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
        "            response = requests.get(search_url, timeout=10)\n",
        "\n",
        "            related_concepts = []\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for edge in data.get('edges', []):\n",
        "                    start = edge.get('start', {}).get('label', '')\n",
        "                    end = edge.get('end', {}).get('label', '')\n",
        "\n",
        "                    # Extract concept paths and clean them\n",
        "                    for concept_path in [start, end]:\n",
        "                        if concept_path and '/c/en/' in concept_path:\n",
        "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
        "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
        "                                related_concepts.append(concept)\n",
        "\n",
        "            return list(set(related_concepts))[:5]  # Return top 5 unique concepts\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
        "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
        "        try:\n",
        "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
        "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return data.get('edges', [])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "            all_facts = []\n",
        "\n",
        "            # Strategy 1: Try direct query first\n",
        "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
        "\n",
        "            # Strategy 2: Search for related concepts and query them\n",
        "            related_concepts = self.search_related_concepts(entity)\n",
        "\n",
        "            # Process direct edges\n",
        "            for edge in direct_edges:\n",
        "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
        "                if fact:\n",
        "                    all_facts.append(fact)\n",
        "\n",
        "            # Process related concept edges\n",
        "            for concept in related_concepts:\n",
        "                concept_edges = self.query_concept_directly(concept, 5)\n",
        "                for edge in concept_edges:\n",
        "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
        "                    if fact:\n",
        "                        all_facts.append(fact)\n",
        "\n",
        "            if all_facts:\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
        "                if related_concepts:\n",
        "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
        "\n",
        "            return all_facts[:limit]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
        "        try:\n",
        "            start = edge.get('start', {})\n",
        "            end = edge.get('end', {})\n",
        "            relation = edge.get('rel', {})\n",
        "            weight = edge.get('weight', 1.0)\n",
        "\n",
        "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            rel_label = relation.get('label', 'related_to')\n",
        "\n",
        "            # Skip if labels are empty or too short\n",
        "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
        "                return None\n",
        "\n",
        "            # Determine confidence based on discovery method\n",
        "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
        "\n",
        "            return EnhancedKnowledgeFact(\n",
        "                subject=original_entity,\n",
        "                predicate=rel_label,\n",
        "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
        "                source=self.name,\n",
        "                confidence=min(weight * confidence_multiplier, 1.0),\n",
        "                context=f\"Discovered {discovery_method}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
        "            return None\n",
        "\n",
        "class MultiKGCache:\n",
        "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
        "\n",
        "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
        "        self.cache_file = cache_file\n",
        "        self.cache = self._load_cache()\n",
        "\n",
        "    def _load_cache(self) -> Dict:\n",
        "        \"\"\"Load cache from file\"\"\"\n",
        "        if os.path.exists(self.cache_file):\n",
        "            try:\n",
        "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_cache(self):\n",
        "        \"\"\"Save cache to file\"\"\"\n",
        "        try:\n",
        "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save cache: {e}\")\n",
        "\n",
        "    def get_cache_key(self, source: str, entity: str) -> str:\n",
        "        \"\"\"Generate cache key\"\"\"\n",
        "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
        "\n",
        "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Get cached facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        return self.cache.get(key)\n",
        "\n",
        "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
        "        \"\"\"Cache facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        serializable_facts = []\n",
        "        for fact in facts:\n",
        "            serializable_facts.append({\n",
        "                'subject': fact.subject,\n",
        "                'predicate': fact.predicate,\n",
        "                'object': fact.object,\n",
        "                'source': fact.source,\n",
        "                'confidence': fact.confidence,\n",
        "                'context': fact.context,\n",
        "                'temporal': fact.temporal,\n",
        "                'spatial': fact.spatial,\n",
        "                'evidence_score': fact.evidence_score,\n",
        "                'source_uri': fact.source_uri\n",
        "            })\n",
        "        self.cache[key] = serializable_facts\n",
        "        self._save_cache()\n",
        "\n",
        "class EnhancedMultiKGRAGSystem:\n",
        "    \"\"\"Multi-Knowledge Graph RAG system with chunking and location extraction - LLAMA VERSION\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.connectors = {\n",
        "            'wikidata': EnhancedWikidataConnector(),\n",
        "            'dbpedia': EnhancedDBpediaConnector(),\n",
        "            'conceptnet': EnhancedConceptNetConnector()\n",
        "        }\n",
        "        self.cache = MultiKGCache()\n",
        "        self.chunker = TextChunker()\n",
        "        self.location_extractor = LocationExtractor()\n",
        "        self.global_locations = {}\n",
        "        self.vectorstore = None  # For RAG retrieval\n",
        "        self.document_chunks = []  # Store processed chunks for RAG\n",
        "        self.stats = {\n",
        "            'queries_processed': 0,\n",
        "            'entities_extracted': 0,\n",
        "            'facts_retrieved': 0,\n",
        "            'cache_hits': 0,\n",
        "            'chunks_processed': 0,\n",
        "            'locations_found': 0,\n",
        "            'locations_with_coordinates': 0,\n",
        "            'location_duplicates_avoided': 0,\n",
        "            'rag_queries': 0\n",
        "        }\n",
        "\n",
        "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract entities from text\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
        "        matches = re.findall(pattern, text)\n",
        "        entities.extend(matches)\n",
        "\n",
        "        stop_words = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
        "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
        "        }\n",
        "\n",
        "        filtered_entities = []\n",
        "        for entity in entities:\n",
        "            entity = entity.strip()\n",
        "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
        "                filtered_entities.append(entity)\n",
        "\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for entity in filtered_entities:\n",
        "            if entity.lower() not in seen:\n",
        "                seen.add(entity.lower())\n",
        "                unique_entities.append(entity)\n",
        "\n",
        "        return unique_entities[:12]\n",
        "\n",
        "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
        "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
        "        all_facts = {}\n",
        "        cache_hits = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            for entity in entities:\n",
        "                for source_name, connector in self.connectors.items():\n",
        "                    # Check cache first\n",
        "                    cached_facts = self.cache.get(source_name, entity)\n",
        "                    if cached_facts:\n",
        "                        cache_hits += 1\n",
        "                        if entity not in all_facts:\n",
        "                            all_facts[entity] = []\n",
        "                        for fact_data in cached_facts:\n",
        "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
        "                            all_facts[entity].append(fact)\n",
        "                    else:\n",
        "                        future = executor.submit(connector.retrieve_facts, entity, 5)\n",
        "                        futures[future] = (entity, source_name)\n",
        "\n",
        "            # Collect results with better timeout handling\n",
        "            completed = 0\n",
        "            total_futures = len(futures)\n",
        "\n",
        "            try:\n",
        "                for future in as_completed(futures, timeout=45):  # Increased timeout\n",
        "                    entity, source_name = futures[future]\n",
        "                    completed += 1\n",
        "\n",
        "                    try:\n",
        "                        facts = future.result(timeout=5)  # Individual future timeout\n",
        "                        if facts:\n",
        "                            self.cache.set(source_name, entity, facts)\n",
        "\n",
        "                            if entity not in all_facts:\n",
        "                                all_facts[entity] = []\n",
        "                            all_facts[entity].extend(facts)\n",
        "\n",
        "                            self.stats['facts_retrieved'] += len(facts)\n",
        "\n",
        "                        logger.debug(f\"✅ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"❌ {source_name} failed for {entity}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except TimeoutError:\n",
        "                pending_count = total_futures - completed\n",
        "                logger.warning(f\"⏰ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
        "\n",
        "                # Cancel remaining futures\n",
        "                for future in futures:\n",
        "                    if not future.done():\n",
        "                        future.cancel()\n",
        "\n",
        "        self.stats['cache_hits'] += cache_hits\n",
        "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
        "        return all_facts\n",
        "\n",
        "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
        "        \"\"\"Format KG facts into context string\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        for entity, facts in kg_facts.items():\n",
        "            if facts:\n",
        "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
        "\n",
        "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
        "\n",
        "                by_source = {}\n",
        "                for fact in sorted_facts[:4]:\n",
        "                    if fact.source not in by_source:\n",
        "                        by_source[fact.source] = []\n",
        "                    by_source[fact.source].append(fact)\n",
        "\n",
        "                for source, source_facts in by_source.items():\n",
        "                    context_parts.append(f\"\\nFrom {source}:\")\n",
        "                    for fact in source_facts[:2]:\n",
        "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
        "                        if fact.confidence < 0.8:\n",
        "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
        "                        context_parts.append(fact_str)\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
        "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
        "        location_key = location_info.name.lower().strip()\n",
        "\n",
        "        if location_key in self.global_locations:\n",
        "            existing = self.global_locations[location_key]\n",
        "            if (location_info.latitude and location_info.longitude and\n",
        "                (not existing.latitude or not existing.longitude)):\n",
        "                self.global_locations[location_key] = location_info\n",
        "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
        "            else:\n",
        "                self.stats['location_duplicates_avoided'] += 1\n",
        "                logger.debug(f\"Location {location_info.name} already registered\")\n",
        "        else:\n",
        "            self.global_locations[location_key] = location_info\n",
        "            logger.info(f\"Registered new location: {location_info.name}\")\n",
        "\n",
        "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "        return f\"ste:Location_{clean_name}\"\n",
        "\n",
        "    def process_chunk(self, chunk: str, chunk_num: int, llm) -> str:\n",
        "        \"\"\"Process a single chunk of text with RAG-enhanced location extraction\"\"\"\n",
        "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars)\")\n",
        "\n",
        "        # 1. RAG RETRIEVAL: Get relevant context from vector store\n",
        "        relevant_context = \"\"\n",
        "        if self.vectorstore:\n",
        "            try:\n",
        "                # Use the chunk as a query to retrieve similar/relevant text\n",
        "                relevant_docs = self.vectorstore.similarity_search(chunk, k=7)\n",
        "                retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "                relevant_context = \"\\n---\\n\".join(retrieved_chunks)\n",
        "                logger.info(f\"Retrieved {len(retrieved_chunks)} relevant chunks via RAG for chunk {chunk_num}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"RAG retrieval failed for chunk {chunk_num}: {e}\")\n",
        "                relevant_context = \"\"\n",
        "\n",
        "        # 2. Extract entities and locations\n",
        "        entities = self.extract_entities_advanced(chunk)\n",
        "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
        "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
        "\n",
        "        # 3. Enrich locations with coordinates\n",
        "        enriched_locations = {}\n",
        "        for location_name in locations[:5]:  # Limit to 5 to avoid overwhelming\n",
        "            location_info = self.location_extractor.enrich_location(location_name)\n",
        "            if location_info:\n",
        "                self.register_global_location(location_info)\n",
        "                enriched_locations[location_name] = location_info\n",
        "                self.stats['locations_found'] += 1\n",
        "                if location_info.latitude and location_info.longitude:\n",
        "                    self.stats['locations_with_coordinates'] += 1\n",
        "\n",
        "        if not entities and not enriched_locations:\n",
        "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
        "            return \"\"\n",
        "\n",
        "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
        "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
        "\n",
        "        # 4. Get KG facts for entities\n",
        "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
        "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "        location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "        # 5. RAG-ENHANCED PROMPT: Use retrieved context + KG facts + locations\n",
        "        enhanced_prompt = f\"\"\"You are extracting historical events using RAG (Retrieval-Augmented Generation). Use ALL available context sources to enhance your extraction.\n",
        "\n",
        "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
        "{chunk}\n",
        "\n",
        "RAG RETRIEVED RELEVANT CONTEXT:\n",
        "{relevant_context if relevant_context else \"No relevant context retrieved.\"}\n",
        "\n",
        "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
        "{kg_context}\n",
        "\n",
        "LOCATION INFORMATION WITH COORDINATES:\n",
        "{location_context}\n",
        "\n",
        "TASK: Extract ONLY the events that are actually mentioned in the current text chunk. Use the RAG retrieved context, KG facts, and location coordinates to enhance details but stay faithful to what's actually in the current chunk.\n",
        "\n",
        "Requirements:\n",
        "1. Extract ONLY events mentioned in the CURRENT text chunk (not from retrieved context)\n",
        "2. Use RAG retrieved context to provide additional historical context and validation\n",
        "3. Use KG facts to enhance entity information\n",
        "4. Use location coordinates to provide precise geographical data\n",
        "5. Include ALL these properties for each event:\n",
        "   - ste:hasType (description of event)\n",
        "   - ste:hasAgent (who caused/led the event)\n",
        "   - ste:hasTime (when it happened)\n",
        "   - ste:hasLocation (location name from text)\n",
        "   - ste:hasLatitude (latitude coordinate if available)\n",
        "   - ste:hasLongitude (longitude coordinate if available)\n",
        "   - ste:hasCountry (country if available)\n",
        "   - ste:hasRegion (region if available)\n",
        "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
        "   - ste:hasResult (outcome/consequence)\n",
        "   - ste:hasRAGContext \"yes\" (to indicate this was RAG-enhanced)\n",
        "\n",
        "Output format (do not include prefixes, they will be added later):\n",
        "```turtle\n",
        "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
        "    ste:hasType \"specific description from current chunk\" ;\n",
        "    ste:hasAgent \"specific person from current chunk\" ;\n",
        "    ste:hasTime \"specific date from current chunk\" ;\n",
        "    ste:hasLocation \"specific location from current chunk\" ;\n",
        "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
        "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
        "    ste:hasCountry \"Italy\" ;\n",
        "    ste:hasRegion \"Sicily\" ;\n",
        "    ste:hasLocationSource \"wikidata\" ;\n",
        "    ste:hasResult \"specific outcome from current chunk\" ;\n",
        "    ste:hasRAGContext \"yes\" .\n",
        "```\n",
        "\n",
        "IMPORTANT:\n",
        "- The PRIMARY source is the CURRENT text chunk - extract events from IT\n",
        "- Use RAG retrieved context to validate and enhance your understanding\n",
        "- Use KG facts to enrich entity details\n",
        "- Include precise coordinates from location sources\n",
        "- Mark all events with ste:hasRAGContext \"yes\" to show RAG was used\n",
        "- Only extract events explicitly mentioned in the current chunk\n",
        "- If no clear events are found in current chunk, return empty\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = llm.invoke([HumanMessage(content=enhanced_prompt)])\n",
        "            turtle_output = self.clean_turtle(response.content)\n",
        "            self.stats['chunks_processed'] += 1\n",
        "            self.stats['rag_queries'] += 1  # Count as RAG usage\n",
        "            logger.info(f\"Generated RAG-enhanced RDF for chunk {chunk_num}\")\n",
        "            return turtle_output\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing chunk {chunk_num} with RAG: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
        "        \"\"\"Format location information into context string\"\"\"\n",
        "        if not enriched_locations:\n",
        "            return \"No location coordinates available.\"\n",
        "\n",
        "        context_parts = [\"\\n=== Location Information ===\"]\n",
        "\n",
        "        for location_name, location_info in enriched_locations.items():\n",
        "            context_parts.append(f\"\\n{location_name}:\")\n",
        "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
        "            else:\n",
        "                context_parts.append(\"  - Coordinates: Not available\")\n",
        "\n",
        "            if location_info.country:\n",
        "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
        "\n",
        "            if location_info.region:\n",
        "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
        "\n",
        "            if location_info.uri:\n",
        "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def generate_global_location_rdf(self) -> str:\n",
        "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
        "        if not self.global_locations:\n",
        "            return \"\"\n",
        "\n",
        "        location_rdf_parts = []\n",
        "\n",
        "        for location_key, location_info in self.global_locations.items():\n",
        "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "            location_id = f\"ste:Location_{clean_name}\"\n",
        "\n",
        "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
        "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
        "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
        "\n",
        "            if location_info.country:\n",
        "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
        "\n",
        "            if location_info.region:\n",
        "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
        "\n",
        "            if location_info.source:\n",
        "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
        "\n",
        "            if location_info.uri:\n",
        "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
        "\n",
        "            if rdf_lines[-1].endswith(' ;'):\n",
        "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
        "\n",
        "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
        "\n",
        "        return '\\n\\n'.join(location_rdf_parts)\n",
        "\n",
        "    def clean_turtle(self, raw_output: str) -> str:\n",
        "        \"\"\"Clean turtle output\"\"\"\n",
        "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "\n",
        "        lines = raw_output.strip().split('\\n')\n",
        "        turtle_lines = []\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if (stripped.startswith('@') or stripped.startswith('<') or\n",
        "                stripped.startswith(':') or stripped.startswith('_') or\n",
        "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
        "                turtle_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(turtle_lines)\n",
        "\n",
        "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
        "        \"\"\"Create vector store from text chunks for RAG retrieval\"\"\"\n",
        "        try:\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            from langchain_community.vectorstores import FAISS\n",
        "\n",
        "            embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "            )\n",
        "\n",
        "            # Prepare chunks with metadata\n",
        "            documents = []\n",
        "            metadatas = []\n",
        "\n",
        "            for i, chunk in enumerate(text_chunks):\n",
        "                if len(chunk.strip()) > 50:  # Only include substantial chunks\n",
        "                    documents.append(chunk)\n",
        "                    metadatas.append({\n",
        "                        'chunk_id': i,\n",
        "                        'length': len(chunk),\n",
        "                        'type': 'text_chunk'\n",
        "                    })\n",
        "\n",
        "            if documents:\n",
        "                self.vectorstore = FAISS.from_texts(documents, embeddings, metadatas=metadatas)\n",
        "                self.document_chunks = documents\n",
        "                logger.info(f\"Created vector store with {len(documents)} chunks\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.warning(\"No suitable chunks found for vector store\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating vector store: {e}\")\n",
        "            return False\n",
        "\n",
        "    def rag_query(self, query: str, llm, k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Perform RAG query: retrieve relevant chunks + KG facts, then generate response\"\"\"\n",
        "        self.stats['rag_queries'] += 1\n",
        "        logger.info(f\"Processing RAG query: '{query[:50]}...'\")\n",
        "\n",
        "        if not self.vectorstore:\n",
        "            return {\"error\": \"Vector store not initialized. Call prepare_vectorstore() first.\"}\n",
        "\n",
        "        try:\n",
        "            # 1. RETRIEVE: Get relevant text chunks\n",
        "            relevant_docs = self.vectorstore.similarity_search(query, k=k)\n",
        "            retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "            # 2. EXTRACT: Get entities from query\n",
        "            query_entities = self.extract_entities_advanced(query)\n",
        "\n",
        "            # 3. RETRIEVE: Get KG facts for entities\n",
        "            kg_facts = self.retrieve_kg_facts_enhanced(query_entities)\n",
        "            kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "\n",
        "            # 4. EXTRACT & ENRICH: Get locations from query\n",
        "            query_locations = self.location_extractor.extract_locations_from_text(query)\n",
        "            enriched_locations = {}\n",
        "            for location_name in query_locations[:5]:\n",
        "                location_info = self.location_extractor.enrich_location(location_name)\n",
        "                if location_info:\n",
        "                    enriched_locations[location_name] = location_info\n",
        "\n",
        "            location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "            # 5. AUGMENT: Create enhanced context for generation\n",
        "            context_parts = [\n",
        "                f\"QUERY: {query}\",\n",
        "                f\"\\nRETRIEVED RELEVANT TEXT CHUNKS:\",\n",
        "                \"\\n\" + \"\\n---\\n\".join(retrieved_chunks),\n",
        "                f\"\\nKNOWLEDGE GRAPH CONTEXT:\",\n",
        "                kg_context,\n",
        "                f\"\\nLOCATION CONTEXT:\",\n",
        "                location_context\n",
        "            ]\n",
        "\n",
        "            enhanced_context = \"\\n\".join(context_parts)\n",
        "\n",
        "            # 6. GENERATE: Create comprehensive response\n",
        "            rag_prompt = f\"\"\"You are an expert historian with access to multiple knowledge sources. Answer the question using the provided context.\n",
        "\n",
        "{enhanced_context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Answer the question comprehensively using ALL available context\n",
        "2. Cite specific information from the retrieved text chunks\n",
        "3. Incorporate relevant knowledge graph facts to enhance your answer\n",
        "4. Include precise location information and coordinates when relevant\n",
        "5. If information conflicts between sources, mention this\n",
        "6. Be specific about dates, places, people, and events\n",
        "7. If you cannot answer completely, explain what information is missing\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "            # 7. GENERATE: Get LLM response\n",
        "            response = llm.invoke([HumanMessage(content=rag_prompt)])\n",
        "\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"answer\": response.content,\n",
        "                \"retrieved_chunks\": retrieved_chunks,\n",
        "                \"entities_found\": query_entities,\n",
        "                \"kg_facts_count\": sum(len(facts) for facts in kg_facts.values()),\n",
        "                \"locations_found\": list(enriched_locations.keys()),\n",
        "                \"sources\": {\n",
        "                    \"text_chunks\": len(retrieved_chunks),\n",
        "                    \"kg_sources\": list(set(fact.source for facts in kg_facts.values() for fact in facts)),\n",
        "                    \"location_sources\": list(set(loc.source for loc in enriched_locations.values()))\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"RAG query failed: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def interactive_rag_session(self, llm):\n",
        "        \"\"\"Start an interactive RAG session\"\"\"\n",
        "        print(\"\\n🤖 Starting Interactive RAG Session\")\n",
        "        print(\"Ask questions about your text. Type 'quit' to exit.\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                query = input(\"\\n❓ Your question: \").strip()\n",
        "\n",
        "                if query.lower() in ['quit', 'exit', 'q']:\n",
        "                    print(\"👋 Goodbye!\")\n",
        "                    break\n",
        "\n",
        "                if not query:\n",
        "                    continue\n",
        "\n",
        "                print(\"\\n🔍 Processing your question...\")\n",
        "                result = self.rag_query(query, llm)\n",
        "\n",
        "                if \"error\" in result:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n👋 Goodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "                continue\n",
        "\n",
        "# Utility functions\n",
        "def load_api_key():\n",
        "    \"\"\"Llama via Ollama doesn't need an API key\"\"\"\n",
        "    print(\"✅ Using local Ollama - no API key needed.\")\n",
        "    return \"local\"\n",
        "\n",
        "def load_text_from_file(filepath: str) -> str:\n",
        "    \"\"\"Load text from file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        print(f\"Loaded text from {filepath}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def initialize_llm(api_key: str):\n",
        "    \"\"\"Initialize Llama LLM\"\"\"\n",
        "    try:\n",
        "        llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
        "        print(\"✅ Llama LLM initialized successfully.\")\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing LLM: {e}\")\n",
        "        print(\"💡 Make sure Ollama is running: ollama serve\")\n",
        "        print(\"💡 And pull the model: ollama pull llama3.2\")\n",
        "        return None\n",
        "\n",
        "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
        "    \"\"\"Create vector store from text\"\"\"\n",
        "    try:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "        )\n",
        "\n",
        "        # Split text into sentences for better retrieval\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        texts = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 20]\n",
        "\n",
        "        if not texts:\n",
        "            return None\n",
        "\n",
        "        vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "        multi_kg_system.vectorstore = vectorstore\n",
        "        multi_kg_system.document_chunks = texts\n",
        "        print(f\"📚 Vector store created with {len(texts)} text segments\")\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with chunking support and RAG capabilities - LLAMA VERSION\"\"\"\n",
        "    print(\"🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\")\n",
        "\n",
        "    api_key = load_api_key()\n",
        "    if not api_key:\n",
        "        return\n",
        "\n",
        "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
        "    if not domain_text:\n",
        "        print(\"⚠️  No input file found, using sample text\")\n",
        "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC.\n",
        "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes.\n",
        "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
        "    else:\n",
        "        print(f\"📄 Using YOUR text from {INPUT_TEXT_FILE}\")\n",
        "        print(f\"📝 Text length: {len(domain_text)} characters\")\n",
        "\n",
        "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
        "    llm = initialize_llm(api_key)\n",
        "\n",
        "    if not llm:\n",
        "        return\n",
        "\n",
        "    # Prepare vector store for RAG FIRST\n",
        "    print(\"\\n📚 Setting up RAG vector store...\")\n",
        "    vectorstore = prepare_vectorstore_from_text(domain_text, multi_kg_system)\n",
        "\n",
        "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
        "    print(f\"🔢 Total tokens in text: {token_count:,}\")\n",
        "\n",
        "    if token_count > 10000:  # Reduced for Llama\n",
        "        print(\"📊 Text is large, chunking into smaller pieces...\")\n",
        "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=10000)\n",
        "        print(f\"📄 Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(\"📄 Text is small enough to process as single chunk\")\n",
        "        chunks = [domain_text]\n",
        "\n",
        "    # Extract events and create RDF\n",
        "    all_turtle_outputs = []\n",
        "    all_entities = set()\n",
        "\n",
        "    print(\"\\n🔄 Processing chunks for event extraction...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"\\n🔄 Processing chunk {i}/{len(chunks)}...\")\n",
        "\n",
        "        turtle_output = multi_kg_system.process_chunk(chunk, i, llm)\n",
        "        if turtle_output:\n",
        "            all_turtle_outputs.append(turtle_output)\n",
        "\n",
        "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
        "        all_entities.update(chunk_entities)\n",
        "\n",
        "        if i < len(chunks):\n",
        "            time.sleep(1)\n",
        "\n",
        "    # Save RDF output\n",
        "    if all_turtle_outputs:\n",
        "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
        "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
        "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
        "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
        "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
        "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
        "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        final_output = prefixes + \"# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
        "\n",
        "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output)\n",
        "\n",
        "        print(f\"\\n✅ Saved enhanced RDF to {OUTPUT_RAG_TTL}\")\n",
        "        print(f\"📊 Processing Statistics:\")\n",
        "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
        "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
        "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
        "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
        "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
        "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
        "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
        "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
        "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
        "        print(f\"   - RAG queries for RDF generation: {multi_kg_system.stats['rag_queries']}\")\n",
        "\n",
        "        print(f\"\\n🔗 Knowledge Graph Connector Statistics:\")\n",
        "        for name, connector in multi_kg_system.connectors.items():\n",
        "            stats = connector.get_stats()\n",
        "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
        "\n",
        "        if multi_kg_system.location_extractor.location_cache:\n",
        "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
        "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
        "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
        "\n",
        "        print(f\"\\n📝 Sample of generated RDF:\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No events were extracted from any chunks\")\n",
        "\n",
        "    # START RAG SESSION HERE!\n",
        "    if vectorstore:\n",
        "        print(f\"\\n🤖 RAG System Ready!\")\n",
        "\n",
        "        # Show example queries\n",
        "        print(f\"\\n💡 Try asking questions like:\")\n",
        "        print(f\"   - 'What battles happened in Sicily?'\")\n",
        "        print(f\"   - 'Who were the main leaders mentioned?'\")\n",
        "        print(f\"   - 'What events occurred in 415 BC?'\")\n",
        "        print(f\"   - 'Describe the naval engagements'\")\n",
        "        print(f\"   - 'What was the outcome of the siege?'\")\n",
        "\n",
        "        # Ask if user wants to start interactive session\n",
        "        response = input(f\"\\n❓ Start interactive RAG session? (y/n): \").strip()\n",
        "\n",
        "        # Check if user typed a question instead of y/n\n",
        "        if response.lower() not in ['y', 'yes', 'n', 'no', '']:\n",
        "            # User typed a question directly!\n",
        "            print(f\"\\n🔍 Processing your question: '{response}'\")\n",
        "            result = multi_kg_system.rag_query(response, llm)\n",
        "\n",
        "            if \"error\" not in result:\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "            else:\n",
        "                print(f\"❌ Error: {result['error']}\")\n",
        "\n",
        "            # Ask if they want to continue with interactive session\n",
        "            continue_response = input(f\"\\n❓ Continue with interactive RAG session? (y/n): \").strip().lower()\n",
        "            if continue_response in ['y', 'yes', '']:\n",
        "                multi_kg_system.interactive_rag_session(llm)\n",
        "\n",
        "        elif response.lower() in ['y', 'yes', '']:\n",
        "            multi_kg_system.interactive_rag_session(llm)\n",
        "        else:\n",
        "            print(f\"\\n💡 You can also query programmatically:\")\n",
        "            print(f\"   result = multi_kg_system.rag_query('your question', llm)\")\n",
        "\n",
        "            # Offer a few sample queries\n",
        "            sample_queries = [\n",
        "                \"What are the main events mentioned in the text?\",\n",
        "                \"Which locations are mentioned?\",\n",
        "                \"Who are the key people involved?\"\n",
        "            ]\n",
        "\n",
        "            print(f\"\\n🔍 Running sample queries:\")\n",
        "            for query in sample_queries:\n",
        "                print(f\"\\n❓ Sample query: '{query}'\")\n",
        "                result = multi_kg_system.rag_query(query, llm)\n",
        "                if \"error\" not in result:\n",
        "                    print(f\"📝 Answer: {result['answer'][:200]}...\")\n",
        "                    print(f\"📊 Sources: {len(result['retrieved_chunks'])} chunks, {result['kg_facts_count']} KG facts\")\n",
        "                else:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️  Could not create vector store for RAG functionality\")\n",
        "\n",
        "    print(f\"\\n🎉 Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAzkVw4cETub",
        "outputId": "0ea48574-ccfb-438d-9313-b0a8d1bdf4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2+cpu)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.11/dist-packages (7.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "\n",
            "Verifying Langchain Ollama import...\n",
            "✅ ChatOllama successfully imported after reinstallation.\n",
            "🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\n",
            "✅ Using local Ollama - no API key needed.\n",
            "Loaded text from /content/drive/MyDrive/part_aa\n",
            "📄 Using YOUR text from /content/drive/MyDrive/part_aa\n",
            "📝 Text length: 398568 characters\n",
            "✅ Llama LLM initialized successfully.\n",
            "\n",
            "📚 Setting up RAG vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Vector store created with 1980 text segments\n",
            "🔢 Total tokens in text: 99,642\n",
            "📊 Text is large, chunking into smaller pieces...\n",
            "📄 Created 10 chunks\n",
            "\n",
            "🔄 Processing chunks for event extraction...\n",
            "\n",
            "🔄 Processing chunk 1/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:Wikidata returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:DBpedia returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:Wikidata returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:DBpedia returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:⏰ Timeout: 1/18 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 2/10...\n",
            "\n",
            "🔄 Processing chunk 3/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:Wikidata returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:DBpedia returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:⏰ Timeout: 1/19 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 4/10...\n",
            "\n",
            "🔄 Processing chunk 5/10...\n",
            "\n",
            "🔄 Processing chunk 6/10...\n",
            "\n",
            "🔄 Processing chunk 7/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VII\n",
            "Second Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VII\n",
            "Second Year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 8/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VIII\n",
            "Third Year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 9/10...\n",
            "\n",
            "🔄 Processing chunk 10/10...\n",
            "\n",
            "✅ Saved enhanced RDF to /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl\n",
            "📊 Processing Statistics:\n",
            "   - Total chunks processed: 10\n",
            "   - Successful chunks: 10\n",
            "   - Unique entities found: 97\n",
            "   - Total KG facts retrieved: 2\n",
            "   - Cache hits: 254\n",
            "   - Locations found: 21\n",
            "   - Locations with coordinates: 21\n",
            "   - Location duplicates avoided: 12\n",
            "   - Unique global locations: 9\n",
            "   - RAG queries for RDF generation: 10\n",
            "\n",
            "🔗 Knowledge Graph Connector Statistics:\n",
            "   - Wikidata: 26/34 requests (76.5% success)\n",
            "   - DBpedia: 21/29 requests (72.4% success)\n",
            "   - ConceptNet: 1/43 requests (2.3% success)\n",
            "   - Location enrichment: 55/178 locations enriched (30.9% success)\n",
            "\n",
            "📝 Sample of generated RDF:\n",
            "============================================================\n",
            "@prefix ste: <http://www.example.org/ste#> .\n",
            "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
            "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
            "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
            "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
            "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
            "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
            "\n",
            "# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\n",
            "ste:Event1_1 a ste:Event, dbp:SpecificEventType ;\n",
            "    ste:hasType \"The Lacedaemonian king, Archidamus, called together the generals of all the states and the principal persons and officers\" ;\n",
            "    ste:hasAgent \"Archidamus\" ;\n",
            "    ste:hasTime \"\" ;\n",
            "    ste:hasLocation \"Lacedaemon\" ;\n",
            "    ste:hasLatitude \"\" ;\n",
            "    ste:hasLongitude \"\" ;\n",
            "    ste:hasCountry \"\" ;\n",
            "    ste:hasRegion \"\" ;\n",
            "    ste:hasLocationSource \"wikidata\" ;\n",
            "    ste:hasResult \"\" ;\n",
            "    ste:hasRAGContext \"yes\" .\n",
            "\n",
            "ste:Event1_2 a ste:Event, dbp:SpecificEventType ;\n",
            "    ste:hasType \"The Peloponnesians and allie...\n",
            "============================================================\n",
            "\n",
            "🤖 RAG System Ready!\n",
            "\n",
            "💡 Try asking questions like:\n",
            "   - 'What battles happened in Sicily?'\n",
            "   - 'Who were the main leaders mentioned?'\n",
            "   - 'What events occurred in 415 BC?'\n",
            "   - 'Describe the naval engagements'\n",
            "   - 'What was the outcome of the siege?'\n",
            "\n",
            "❓ Start interactive RAG session? (y/n): n\n",
            "\n",
            "💡 You can also query programmatically:\n",
            "   result = multi_kg_system.rag_query('your question', llm)\n",
            "\n",
            "🔍 Running sample queries:\n",
            "\n",
            "❓ Sample query: 'What are the main events mentioned in the text?'\n",
            "📝 Answer: Based on the retrieved text chunks and knowledge graph context, I can identify several main events mentioned in the text:\n",
            "\n",
            "1. **The Second Congress at Lacedaemon**: This event is mentioned as a precur...\n",
            "📊 Sources: 3 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Which locations are mentioned?'\n",
            "📝 Answer: Based on the provided context, I can identify several locations mentioned in the retrieved text chunks:\n",
            "\n",
            "1. The Thracian towns: Although not specific locations, it's implied that these towns are locat...\n",
            "📊 Sources: 3 chunks, 12 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Who are the key people involved?'\n",
            "📝 Answer: Based on the provided context, it appears that the question \"Who are the key people involved?\" is related to a speech or oration being delivered, possibly in an ancient Greek setting. The text chunks ...\n",
            "📊 Sources: 3 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "🎉 Process complete! Check /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl for RDF results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnagI49PUKBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚑 Fix sentence-transformers/transformers incompatibility\n",
        "\n",
        "# 1. Uninstall all possibly conflicting versions\n",
        "!pip uninstall -y sentence-transformers transformers\n",
        "\n",
        "# 2. Install compatible versions\n",
        "!pip install transformers==4.40.2\n",
        "!pip install sentence-transformers==2.4.0\n",
        "\n",
        "# 3. Test import\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    print(\"✅ sentence_transformers import now works!\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ sentence_transformers import failed: {e}\")\n",
        "\n",
        "import transformers\n",
        "print(f\"transformers version: {transformers.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_1J2i2pCkUv",
        "outputId": "c3b3c4ab-25e9-4c60-ce37-75f90844dcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: sentence-transformers 4.1.0\n",
            "Uninstalling sentence-transformers-4.1.0:\n",
            "  Successfully uninstalled sentence-transformers-4.1.0\n",
            "Found existing installation: transformers 4.52.3\n",
            "Uninstalling transformers-4.52.3:\n",
            "  Successfully uninstalled transformers-4.52.3\n",
            "Collecting transformers==4.40.2\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2025.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2025.4.26)\n",
            "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 0.2.0 requires sentence-transformers>=2.6.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.2\n",
            "Collecting sentence-transformers==2.4.0\n",
            "  Using cached sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (4.40.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (2.2.2+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (2.2.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (0.32.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.4.0) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (2025.5.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.4.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.4.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.4.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.4.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.4.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.4.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.11.0->sentence-transformers==2.4.0) (1.3.0)\n",
            "Using cached sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
            "Installing collected packages: sentence-transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-huggingface 0.2.0 requires sentence-transformers>=2.6.0, but you have sentence-transformers 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sentence-transformers-2.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-dc5254fd5d22>\", line 12, in <cell line: 0>\n",
            "    from sentence_transformers import SentenceTransformer\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sentence_transformers/__init__.py\", line 3, in <module>\n",
            "    from .datasets import SentencesDataset, ParallelSentencesDataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sentence_transformers/datasets/__init__.py\", line 1, in <module>\n",
            "    from .DenoisingAutoEncoderDataset import DenoisingAutoEncoderDataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py\", line 1, in <module>\n",
            "    from torch.utils.data import Dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ sentence_transformers import now works!\n",
            "transformers version: 4.40.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 1: Mount Drive and Install Dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Dependency Compatibility Guard ---\n",
        "import os\n",
        "\n",
        "# Force compatible versions for transformers and sentence-transformers\n",
        "os.system(\"pip install transformers==4.40.2 sentence-transformers==2.4.0 --quiet\")\n",
        "\n",
        "# (Optional) If you use torch, ensure it's compatible too:\n",
        "# os.system(\"pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu --quiet\")\n",
        "\n",
        "# Now safe to import\n",
        "\n",
        "!pip install  torch accelerate rdflib\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Enhanced Multi-Knowledge Graph RAG System with Text Chunking - LLAMA VERSION\n",
        "Handles large texts by processing them in chunks to avoid token limits\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import requests\n",
        "\n",
        "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal # <--- This line is crucial\n",
        "from rdflib.namespace import XSD, SKOS\n",
        "# Add a direct import test here to diagnose immediately\n",
        "print(\"\\nVerifying Langchain Ollama import...\")\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "    print(\"✅ ChatOllama successfully imported after reinstallation.\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ CRITICAL ERROR: ChatOllama still cannot be imported after reinstallation: {e}\")\n",
        "    print(\"Please check your pip installation output for errors and ensure your Colab runtime is healthy.\")\n",
        "    # You might want to exit here if this is a persistent issue\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"❌ An unexpected error occurred during ChatOllama import verification: {e}\")\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "\n",
        "\n",
        "# Configuration\n",
        "INPUT_TEXT_FILE = \"/content/drive/MyDrive/part_aa\"\n",
        "ONTOLOGY_PATH = \"/content/drive/MyDrive/wiki.owl\"\n",
        "LOCATION_ONTOLOGY_PATH = \"/content/drive/MyDrive/locations.owl\"\n",
        "OUTPUT_RAG_TTL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama_1.ttl'\n",
        "OUTPUT_RAG_OWL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama_1.owl'\n",
        "KG_CACHE_FILE = '/content/drive/MyDrive/kg_cache.json'\n",
        "LOCATION_CACHE_FILE = '/content/drive/MyDrive/location_cache.json'\n",
        "KG_ANALYSIS_REPORT = '/content/drive/MyDrive/multi_kg_analysis_report.txt'\n",
        "\n",
        "# Token limits - adjusted for Llama\n",
        "MAX_TOKENS_PER_REQUEST = 50000  # Conservative limit for Llama\n",
        "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
        "\n",
        "# Namespaces\n",
        "EX = Namespace(\"http://example.org/\")\n",
        "STE = Namespace(\"http://www.example.org/ste#\")\n",
        "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
        "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
        "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
        "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
        "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
        "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Imports\n",
        "try:\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_ollama import ChatOllama  # Changed from OpenAI to Ollama\n",
        "    from langchain.schema import HumanMessage\n",
        "except ImportError as e:\n",
        "    print(f\"ImportError: {e}\")\n",
        "    print(\"pip install rdflib python-dotenv langchain langchain-ollama langchain-community faiss-cpu sentence-transformers requests\")\n",
        "    exit(1)\n",
        "\n",
        "@dataclass\n",
        "class LocationInfo:\n",
        "    \"\"\"Location information with coordinates\"\"\"\n",
        "    name: str\n",
        "    latitude: Optional[float] = None\n",
        "    longitude: Optional[float] = None\n",
        "    country: Optional[str] = None\n",
        "    region: Optional[str] = None\n",
        "    source: str = \"extracted\"\n",
        "    confidence: float = 1.0\n",
        "    uri: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class EnhancedKnowledgeFact:\n",
        "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
        "    subject: str\n",
        "    predicate: str\n",
        "    object: str\n",
        "    source: str\n",
        "    confidence: float = 1.0\n",
        "    context: Optional[str] = None\n",
        "    temporal: Optional[str] = None\n",
        "    spatial: Optional[str] = None\n",
        "    evidence_score: float = 1.0\n",
        "    source_uri: Optional[str] = None\n",
        "\n",
        "class LocationExtractor:\n",
        "    \"\"\"Extracts and enriches location information\"\"\"\n",
        "\n",
        "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
        "        self.ontology_path = ontology_path\n",
        "        self.location_graph = None\n",
        "        self.location_cache = self._load_location_cache()\n",
        "        self.load_location_ontology()\n",
        "\n",
        "    def _load_location_cache(self) -> Dict:\n",
        "        \"\"\"Load location cache\"\"\"\n",
        "        if os.path.exists(LOCATION_CACHE_FILE):\n",
        "            try:\n",
        "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load location cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_location_cache(self):\n",
        "        \"\"\"Save location cache\"\"\"\n",
        "        try:\n",
        "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save location cache: {e}\")\n",
        "\n",
        "    def load_location_ontology(self):\n",
        "        \"\"\"Load locations.owl ontology\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.ontology_path):\n",
        "                self.location_graph = Graph()\n",
        "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
        "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
        "                self.location_graph = None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading location ontology: {e}\")\n",
        "            self.location_graph = None\n",
        "\n",
        "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract potential location names from text\"\"\"\n",
        "        location_patterns = [\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
        "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
        "        ]\n",
        "\n",
        "        locations = []\n",
        "        for pattern in location_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            locations.extend(matches)\n",
        "\n",
        "        location_stopwords = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If',\n",
        "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
        "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday',\n",
        "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
        "        }\n",
        "\n",
        "        filtered_locations = []\n",
        "        for loc in locations:\n",
        "            loc = loc.strip()\n",
        "            if (loc not in location_stopwords and len(loc) > 2 and\n",
        "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
        "                filtered_locations.append(loc)\n",
        "\n",
        "        return list(set(filtered_locations))\n",
        "\n",
        "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location info from local ontology\"\"\"\n",
        "        if not self.location_graph:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            query = f\"\"\"\n",
        "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
        "                ?location rdfs:label ?label .\n",
        "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
        "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
        "                OPTIONAL {{ ?location geo:long ?long }}\n",
        "                OPTIONAL {{ ?location dbp:country ?country }}\n",
        "                OPTIONAL {{ ?location dbp:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            results = self.location_graph.query(query)\n",
        "            for row in results:\n",
        "                return LocationInfo(\n",
        "                    name=location_name,\n",
        "                    latitude=float(row.lat) if row.lat else None,\n",
        "                    longitude=float(row.long) if row.long else None,\n",
        "                    country=str(row.country) if row.country else None,\n",
        "                    region=str(row.region) if row.region else None,\n",
        "                    source=\"local_ontology\",\n",
        "                    uri=str(row.location) if row.location else None\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
        "                <{entity_uri}> geo:lat ?lat ;\n",
        "                               geo:long ?long .\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                if bindings:\n",
        "                    binding = bindings[0]\n",
        "                    return LocationInfo(\n",
        "                        name=location_name,\n",
        "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
        "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
        "                        country=binding.get('country', {}).get('value', ''),\n",
        "                        region=binding.get('region', {}).get('value', ''),\n",
        "                        source=\"dbpedia\",\n",
        "                        uri=entity_uri\n",
        "                    )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            # Try multiple query strategies to get the right location\n",
        "            queries = [\n",
        "                # Try exact label match first\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\",\n",
        "                # Try with additional filters for places/locations\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\"\n",
        "            ]\n",
        "\n",
        "            for query in queries:\n",
        "                params = {'query': query, 'format': 'json'}\n",
        "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                    if bindings:\n",
        "                        # Prefer results with country information\n",
        "                        best_binding = None\n",
        "                        for binding in bindings:\n",
        "                            if binding.get('country'):\n",
        "                                best_binding = binding\n",
        "                                break\n",
        "\n",
        "                        if not best_binding:\n",
        "                            best_binding = bindings[0]\n",
        "\n",
        "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
        "\n",
        "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
        "                        if coord_match:\n",
        "                            longitude = float(coord_match.group(1))\n",
        "                            latitude = float(coord_match.group(2))\n",
        "\n",
        "                            return LocationInfo(\n",
        "                                name=location_name,\n",
        "                                latitude=latitude,\n",
        "                                longitude=longitude,\n",
        "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
        "                                source=\"wikidata\",\n",
        "                                uri=best_binding.get('item', {}).get('value', '')\n",
        "                            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
        "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
        "        if not location_info.latitude or not location_info.longitude:\n",
        "            return True\n",
        "\n",
        "        lat, lon = location_info.latitude, location_info.longitude\n",
        "\n",
        "        # Basic coordinate range validation\n",
        "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
        "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
        "        if location_name in self.location_cache:\n",
        "            cached = self.location_cache[location_name]\n",
        "            return LocationInfo(**cached) if cached else None\n",
        "\n",
        "        location_info = None\n",
        "\n",
        "        location_info = self.get_location_from_ontology(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_wikidata(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_dbpedia(location_name)\n",
        "\n",
        "        if location_info:\n",
        "            self.location_cache[location_name] = {\n",
        "                'name': location_info.name,\n",
        "                'latitude': location_info.latitude,\n",
        "                'longitude': location_info.longitude,\n",
        "                'country': location_info.country,\n",
        "                'region': location_info.region,\n",
        "                'source': location_info.source,\n",
        "                'confidence': location_info.confidence,\n",
        "                'uri': location_info.uri\n",
        "            }\n",
        "        else:\n",
        "            self.location_cache[location_name] = None\n",
        "\n",
        "        self._save_location_cache()\n",
        "\n",
        "        if location_info:\n",
        "            self.validate_coordinates(location_info)\n",
        "\n",
        "        return location_info\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Handles text chunking to manage token limits for Llama\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"llama3.2\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Approximate token count for Llama (roughly 4 chars per token)\"\"\"\n",
        "        return len(text) // 4\n",
        "\n",
        "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 10000) -> List[str]:\n",
        "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class BaseKGConnector:\n",
        "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
        "        self.name = name\n",
        "        self.base_url = base_url\n",
        "        self.rate_limit = rate_limit\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "        self.success_count = 0\n",
        "\n",
        "    def _rate_limit_wait(self):\n",
        "        \"\"\"Enforce rate limiting\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "        if time_since_last < self.rate_limit:\n",
        "            time.sleep(self.rate_limit - time_since_last)\n",
        "        self.last_request_time = time.time()\n",
        "        self.request_count += 1\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get connector statistics\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'requests': self.request_count,\n",
        "            'successes': self.success_count,\n",
        "            'success_rate': self.success_count / max(1, self.request_count)\n",
        "        }\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 2) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EnhancedWikidataConnector(BaseKGConnector):\n",
        "    \"\"\"Wikidata connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 2) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
        "              {{\n",
        "                ?subject ?label \"{entity}\"@en .\n",
        "              }} UNION {{\n",
        "                ?subject rdfs:label \"{entity}\"@en .\n",
        "              }}\n",
        "\n",
        "              ?subject ?predicate ?object .\n",
        "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
        "\n",
        "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
        "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
        "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
        "                        source=self.name,\n",
        "                        confidence=0.9,\n",
        "                        source_uri=binding.get('subject', {}).get('value')\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedDBpediaConnector(BaseKGConnector):\n",
        "    \"\"\"DBpedia connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
        "              <{entity_uri}> ?predicate ?object .\n",
        "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
        "              FILTER(!isBlank(?object))\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    predicate = binding.get('predicate', {}).get('value', '')\n",
        "                    obj = binding.get('object', {}).get('value', '')\n",
        "\n",
        "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
        "\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=entity,\n",
        "                        predicate=predicate_name,\n",
        "                        object=obj,\n",
        "                        source=self.name,\n",
        "                        confidence=0.85,\n",
        "                        source_uri=entity_uri\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedConceptNetConnector(BaseKGConnector):\n",
        "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
        "\n",
        "    def search_related_concepts(self, entity: str) -> List[str]:\n",
        "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
        "        try:\n",
        "            # Try search API first\n",
        "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
        "            response = requests.get(search_url, timeout=10)\n",
        "\n",
        "            related_concepts = []\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for edge in data.get('edges', []):\n",
        "                    start = edge.get('start', {}).get('label', '')\n",
        "                    end = edge.get('end', {}).get('label', '')\n",
        "\n",
        "                    # Extract concept paths and clean them\n",
        "                    for concept_path in [start, end]:\n",
        "                        if concept_path and '/c/en/' in concept_path:\n",
        "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
        "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
        "                                related_concepts.append(concept)\n",
        "\n",
        "            return list(set(related_concepts))[:5]  # Return top 5 unique concepts\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
        "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
        "        try:\n",
        "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
        "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return data.get('edges', [])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 40) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "            all_facts = []\n",
        "\n",
        "            # Strategy 1: Try direct query first\n",
        "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
        "\n",
        "            # Strategy 2: Search for related concepts and query them\n",
        "            related_concepts = self.search_related_concepts(entity)\n",
        "\n",
        "            # Process direct edges\n",
        "            for edge in direct_edges:\n",
        "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
        "                if fact:\n",
        "                    all_facts.append(fact)\n",
        "\n",
        "            # Process related concept edges\n",
        "            for concept in related_concepts:\n",
        "                concept_edges = self.query_concept_directly(concept, 5)\n",
        "                for edge in concept_edges:\n",
        "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
        "                    if fact:\n",
        "                        all_facts.append(fact)\n",
        "\n",
        "            if all_facts:\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
        "                if related_concepts:\n",
        "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
        "\n",
        "            return all_facts[:limit]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
        "        try:\n",
        "            start = edge.get('start', {})\n",
        "            end = edge.get('end', {})\n",
        "            relation = edge.get('rel', {})\n",
        "            weight = edge.get('weight', 1.0)\n",
        "\n",
        "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            rel_label = relation.get('label', 'related_to')\n",
        "\n",
        "            # Skip if labels are empty or too short\n",
        "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
        "                return None\n",
        "\n",
        "            # Determine confidence based on discovery method\n",
        "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
        "\n",
        "            return EnhancedKnowledgeFact(\n",
        "                subject=original_entity,\n",
        "                predicate=rel_label,\n",
        "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
        "                source=self.name,\n",
        "                confidence=min(weight * confidence_multiplier, 1.0),\n",
        "                context=f\"Discovered {discovery_method}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
        "            return None\n",
        "\n",
        "class MultiKGCache:\n",
        "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
        "\n",
        "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
        "        self.cache_file = cache_file\n",
        "        self.cache = self._load_cache()\n",
        "\n",
        "    def _load_cache(self) -> Dict:\n",
        "        \"\"\"Load cache from file\"\"\"\n",
        "        if os.path.exists(self.cache_file):\n",
        "            try:\n",
        "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_cache(self):\n",
        "        \"\"\"Save cache to file\"\"\"\n",
        "        try:\n",
        "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save cache: {e}\")\n",
        "\n",
        "    def get_cache_key(self, source: str, entity: str) -> str:\n",
        "        \"\"\"Generate cache key\"\"\"\n",
        "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
        "\n",
        "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Get cached facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        return self.cache.get(key)\n",
        "\n",
        "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
        "        \"\"\"Cache facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        serializable_facts = []\n",
        "        for fact in facts:\n",
        "            serializable_facts.append({\n",
        "                'subject': fact.subject,\n",
        "                'predicate': fact.predicate,\n",
        "                'object': fact.object,\n",
        "                'source': fact.source,\n",
        "                'confidence': fact.confidence,\n",
        "                'context': fact.context,\n",
        "                'temporal': fact.temporal,\n",
        "                'spatial': fact.spatial,\n",
        "                'evidence_score': fact.evidence_score,\n",
        "                'source_uri': fact.source_uri\n",
        "            })\n",
        "        self.cache[key] = serializable_facts\n",
        "        self._save_cache()\n",
        "\n",
        "class EnhancedMultiKGRAGSystem:\n",
        "    \"\"\"Multi-Knowledge Graph RAG system with chunking and location extraction - LLAMA VERSION\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.connectors = {\n",
        "            'wikidata': EnhancedWikidataConnector(),\n",
        "            'dbpedia': EnhancedDBpediaConnector(),\n",
        "            'conceptnet': EnhancedConceptNetConnector()\n",
        "        }\n",
        "        self.cache = MultiKGCache()\n",
        "        self.chunker = TextChunker()\n",
        "        self.location_extractor = LocationExtractor()\n",
        "        self.global_locations = {}\n",
        "        self.vectorstore = None  # For RAG retrieval\n",
        "        self.document_chunks = []  # Store processed chunks for RAG\n",
        "        self.stats = {\n",
        "            'queries_processed': 0,\n",
        "            'entities_extracted': 0,\n",
        "            'facts_retrieved': 0,\n",
        "            'cache_hits': 0,\n",
        "            'chunks_processed': 0,\n",
        "            'locations_found': 0,\n",
        "            'locations_with_coordinates': 0,\n",
        "            'location_duplicates_avoided': 0,\n",
        "            'rag_queries': 0\n",
        "        }\n",
        "\n",
        "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract entities from text\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
        "        matches = re.findall(pattern, text)\n",
        "        entities.extend(matches)\n",
        "\n",
        "        stop_words = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
        "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
        "        }\n",
        "\n",
        "        filtered_entities = []\n",
        "        for entity in entities:\n",
        "            entity = entity.strip()\n",
        "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
        "                filtered_entities.append(entity)\n",
        "\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for entity in filtered_entities:\n",
        "            if entity.lower() not in seen:\n",
        "                seen.add(entity.lower())\n",
        "                unique_entities.append(entity)\n",
        "\n",
        "        return unique_entities[:12]\n",
        "\n",
        "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
        "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
        "        all_facts = {}\n",
        "        cache_hits = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            for entity in entities:\n",
        "                for source_name, connector in self.connectors.items():\n",
        "                    # Check cache first\n",
        "                    cached_facts = self.cache.get(source_name, entity)\n",
        "                    if cached_facts:\n",
        "                        cache_hits += 1\n",
        "                        if entity not in all_facts:\n",
        "                            all_facts[entity] = []\n",
        "                        for fact_data in cached_facts:\n",
        "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
        "                            all_facts[entity].append(fact)\n",
        "                    else:\n",
        "                        future = executor.submit(connector.retrieve_facts, entity, 5)\n",
        "                        futures[future] = (entity, source_name)\n",
        "\n",
        "            # Collect results with better timeout handling\n",
        "            completed = 0\n",
        "            total_futures = len(futures)\n",
        "\n",
        "            try:\n",
        "                for future in as_completed(futures, timeout=45):  # Increased timeout\n",
        "                    entity, source_name = futures[future]\n",
        "                    completed += 1\n",
        "\n",
        "                    try:\n",
        "                        facts = future.result(timeout=5)  # Individual future timeout\n",
        "                        if facts:\n",
        "                            self.cache.set(source_name, entity, facts)\n",
        "\n",
        "                            if entity not in all_facts:\n",
        "                                all_facts[entity] = []\n",
        "                            all_facts[entity].extend(facts)\n",
        "\n",
        "                            self.stats['facts_retrieved'] += len(facts)\n",
        "\n",
        "                        logger.debug(f\"✅ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"❌ {source_name} failed for {entity}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except TimeoutError:\n",
        "                pending_count = total_futures - completed\n",
        "                logger.warning(f\"⏰ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
        "\n",
        "                # Cancel remaining futures\n",
        "                for future in futures:\n",
        "                    if not future.done():\n",
        "                        future.cancel()\n",
        "\n",
        "        self.stats['cache_hits'] += cache_hits\n",
        "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
        "        return all_facts\n",
        "\n",
        "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
        "        \"\"\"Format KG facts into context string\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        for entity, facts in kg_facts.items():\n",
        "            if facts:\n",
        "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
        "\n",
        "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
        "\n",
        "                by_source = {}\n",
        "                for fact in sorted_facts[:2]:\n",
        "                    if fact.source not in by_source:\n",
        "                        by_source[fact.source] = []\n",
        "                    by_source[fact.source].append(fact)\n",
        "\n",
        "                for source, source_facts in by_source.items():\n",
        "                    context_parts.append(f\"\\nFrom {source}:\")\n",
        "                    for fact in source_facts[:2]:\n",
        "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
        "                        if fact.confidence < 0.8:\n",
        "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
        "                        context_parts.append(fact_str)\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
        "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
        "        location_key = location_info.name.lower().strip()\n",
        "\n",
        "        if location_key in self.global_locations:\n",
        "            existing = self.global_locations[location_key]\n",
        "            if (location_info.latitude and location_info.longitude and\n",
        "                (not existing.latitude or not existing.longitude)):\n",
        "                self.global_locations[location_key] = location_info\n",
        "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
        "            else:\n",
        "                self.stats['location_duplicates_avoided'] += 1\n",
        "                logger.debug(f\"Location {location_info.name} already registered\")\n",
        "        else:\n",
        "            self.global_locations[location_key] = location_info\n",
        "            logger.info(f\"Registered new location: {location_info.name}\")\n",
        "\n",
        "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "        return f\"ste:Location_{clean_name}\"\n",
        "\n",
        "    def process_chunk(self, chunk: str, chunk_num: int, llm) -> str:\n",
        "        \"\"\"Process a single chunk of text with RAG-enhanced location extraction\"\"\"\n",
        "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars)\")\n",
        "\n",
        "        # 1. RAG RETRIEVAL: Get relevant context from vector store\n",
        "        relevant_context = \"\"\n",
        "        if self.vectorstore:\n",
        "            try:\n",
        "                # Use the chunk as a query to retrieve similar/relevant text\n",
        "                relevant_docs = self.vectorstore.similarity_search(chunk, k=4)\n",
        "                retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "                relevant_context = \"\\n---\\n\".join(retrieved_chunks)\n",
        "                logger.info(f\"Retrieved {len(retrieved_chunks)} relevant chunks via RAG for chunk {chunk_num}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"RAG retrieval failed for chunk {chunk_num}: {e}\")\n",
        "                relevant_context = \"\"\n",
        "\n",
        "        # 2. Extract entities and locations\n",
        "        entities = self.extract_entities_advanced(chunk)\n",
        "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
        "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
        "\n",
        "        # 3. Enrich locations with coordinates\n",
        "        enriched_locations = {}\n",
        "        for location_name in locations[:3]:  # Limit to 5 to avoid overwhelming\n",
        "            location_info = self.location_extractor.enrich_location(location_name)\n",
        "            if location_info:\n",
        "                self.register_global_location(location_info)\n",
        "                enriched_locations[location_name] = location_info\n",
        "                self.stats['locations_found'] += 1\n",
        "                if location_info.latitude and location_info.longitude:\n",
        "                    self.stats['locations_with_coordinates'] += 1\n",
        "\n",
        "        if not entities and not enriched_locations:\n",
        "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
        "            return \"\"\n",
        "\n",
        "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
        "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
        "\n",
        "        # 4. Get KG facts for entities\n",
        "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
        "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "        location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "        # 5. RAG-ENHANCED PROMPT: Use retrieved context + KG facts + locations\n",
        "        enhanced_prompt = f\"\"\"You are extracting historical events using RAG (Retrieval-Augmented Generation). Use ALL available context sources to enhance your extraction.\n",
        "\n",
        "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
        "{chunk}\n",
        "\n",
        "RAG RETRIEVED RELEVANT CONTEXT:\n",
        "{relevant_context if relevant_context else \"No relevant context retrieved.\"}\n",
        "\n",
        "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
        "{kg_context}\n",
        "\n",
        "LOCATION INFORMATION WITH COORDINATES:\n",
        "{location_context}\n",
        "\n",
        "TASK: Extract ONLY the events that are actually mentioned in the current text chunk. Use the RAG retrieved context, KG facts, and location coordinates to enhance details but stay faithful to what's actually in the current chunk.\n",
        "\n",
        "Requirements:\n",
        "1. Extract ONLY events mentioned in the CURRENT text chunk (not from retrieved context)\n",
        "2. Use RAG retrieved context to provide additional historical context and validation\n",
        "3. Use KG facts to enhance entity information\n",
        "4. Use location coordinates to provide precise geographical data\n",
        "5. Include ALL these properties for each event:\n",
        "   - ste:hasType (description of event)\n",
        "   - ste:hasAgent (who caused/led the event)\n",
        "   - ste:hasTime (when it happened)\n",
        "   - ste:hasLocation (location name from text)\n",
        "   - ste:hasLatitude (latitude coordinate if available)\n",
        "   - ste:hasLongitude (longitude coordinate if available)\n",
        "   - ste:hasCountry (country if available)\n",
        "   - ste:hasRegion (region if available)\n",
        "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
        "   - ste:hasResult (outcome/consequence)\n",
        "   - ste:hasRAGContext \"yes\" (to indicate this was RAG-enhanced)\n",
        "\n",
        "Output format (do not include prefixes, they will be added later):\n",
        "```turtle\n",
        "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
        "    ste:hasType \"specific description from current chunk\" ;\n",
        "    ste:hasAgent \"specific person from current chunk\" ;\n",
        "    ste:hasTime \"specific date from current chunk\" ;\n",
        "    ste:hasLocation \"specific location from current chunk\" ;\n",
        "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
        "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
        "    ste:hasCountry \"Italy\" ;\n",
        "    ste:hasRegion \"Sicily\" ;\n",
        "    ste:hasLocationSource \"wikidata\" ;\n",
        "    ste:hasResult \"specific outcome from current chunk\" ;\n",
        "    ste:hasRAGContext \"yes\" .\n",
        "```\n",
        "\n",
        "IMPORTANT:\n",
        "- The PRIMARY source is the CURRENT text chunk - extract events from IT\n",
        "- Use RAG retrieved context to validate and enhance your understanding\n",
        "- Use KG facts to enrich entity details\n",
        "- Include precise coordinates from location sources\n",
        "- Mark all events with ste:hasRAGContext \"yes\" to show RAG was used\n",
        "- Only extract events explicitly mentioned in the current chunk\n",
        "- If no clear events are found in current chunk, return empty\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = llm.invoke([HumanMessage(content=enhanced_prompt)])\n",
        "            turtle_output = self.clean_turtle(response.content)\n",
        "            self.stats['chunks_processed'] += 1\n",
        "            self.stats['rag_queries'] += 1  # Count as RAG usage\n",
        "            logger.info(f\"Generated RAG-enhanced RDF for chunk {chunk_num}\")\n",
        "            return turtle_output\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing chunk {chunk_num} with RAG: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
        "        \"\"\"Format location information into context string\"\"\"\n",
        "        if not enriched_locations:\n",
        "            return \"No location coordinates available.\"\n",
        "\n",
        "        context_parts = [\"\\n=== Location Information ===\"]\n",
        "\n",
        "        for location_name, location_info in enriched_locations.items():\n",
        "            context_parts.append(f\"\\n{location_name}:\")\n",
        "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
        "            else:\n",
        "                context_parts.append(\"  - Coordinates: Not available\")\n",
        "\n",
        "            if location_info.country:\n",
        "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
        "\n",
        "            if location_info.region:\n",
        "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
        "\n",
        "            if location_info.uri:\n",
        "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def generate_global_location_rdf(self) -> str:\n",
        "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
        "        if not self.global_locations:\n",
        "            return \"\"\n",
        "\n",
        "        location_rdf_parts = []\n",
        "\n",
        "        for location_key, location_info in self.global_locations.items():\n",
        "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "            location_id = f\"ste:Location_{clean_name}\"\n",
        "\n",
        "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
        "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
        "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
        "\n",
        "            if location_info.country:\n",
        "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
        "\n",
        "            if location_info.region:\n",
        "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
        "\n",
        "            if location_info.source:\n",
        "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
        "\n",
        "            if location_info.uri:\n",
        "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
        "\n",
        "            if rdf_lines[-1].endswith(' ;'):\n",
        "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
        "\n",
        "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
        "\n",
        "        return '\\n\\n'.join(location_rdf_parts)\n",
        "\n",
        "    def clean_turtle(self, raw_output: str) -> str:\n",
        "        \"\"\"Clean turtle output\"\"\"\n",
        "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "\n",
        "        lines = raw_output.strip().split('\\n')\n",
        "        turtle_lines = []\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if (stripped.startswith('@') or stripped.startswith('<') or\n",
        "                stripped.startswith(':') or stripped.startswith('_') or\n",
        "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
        "                turtle_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(turtle_lines)\n",
        "\n",
        "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
        "        \"\"\"Create vector store from text chunks for RAG retrieval\"\"\"\n",
        "        try:\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            from langchain_community.vectorstores import FAISS\n",
        "\n",
        "            embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "            )\n",
        "\n",
        "            # Prepare chunks with metadata\n",
        "            documents = []\n",
        "            metadatas = []\n",
        "\n",
        "            for i, chunk in enumerate(text_chunks):\n",
        "                if len(chunk.strip()) > 50:  # Only include substantial chunks\n",
        "                    documents.append(chunk)\n",
        "                    metadatas.append({\n",
        "                        'chunk_id': i,\n",
        "                        'length': len(chunk),\n",
        "                        'type': 'text_chunk'\n",
        "                    })\n",
        "\n",
        "            if documents:\n",
        "                self.vectorstore = FAISS.from_texts(documents, embeddings, metadatas=metadatas)\n",
        "                self.document_chunks = documents\n",
        "                logger.info(f\"Created vector store with {len(documents)} chunks\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.warning(\"No suitable chunks found for vector store\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating vector store: {e}\")\n",
        "            return False\n",
        "\n",
        "    def rag_query(self, query: str, llm, k: int = 2) -> Dict[str, Any]:\n",
        "        \"\"\"Perform RAG query: retrieve relevant chunks + KG facts, then generate response\"\"\"\n",
        "        self.stats['rag_queries'] += 1\n",
        "        logger.info(f\"Processing RAG query: '{query[:50]}...'\")\n",
        "\n",
        "        if not self.vectorstore:\n",
        "            return {\"error\": \"Vector store not initialized. Call prepare_vectorstore() first.\"}\n",
        "\n",
        "        try:\n",
        "            # 1. RETRIEVE: Get relevant text chunks\n",
        "            relevant_docs = self.vectorstore.similarity_search(query, k=k)\n",
        "            retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "            # 2. EXTRACT: Get entities from query\n",
        "            query_entities = self.extract_entities_advanced(query)\n",
        "\n",
        "            # 3. RETRIEVE: Get KG facts for entities\n",
        "            kg_facts = self.retrieve_kg_facts_enhanced(query_entities)\n",
        "            kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "\n",
        "            # 4. EXTRACT & ENRICH: Get locations from query\n",
        "            query_locations = self.location_extractor.extract_locations_from_text(query)\n",
        "            enriched_locations = {}\n",
        "            for location_name in query_locations[:5]:\n",
        "                location_info = self.location_extractor.enrich_location(location_name)\n",
        "                if location_info:\n",
        "                    enriched_locations[location_name] = location_info\n",
        "\n",
        "            location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "            # 5. AUGMENT: Create enhanced context for generation\n",
        "            context_parts = [\n",
        "                f\"QUERY: {query}\",\n",
        "                f\"\\nRETRIEVED RELEVANT TEXT CHUNKS:\",\n",
        "                \"\\n\" + \"\\n---\\n\".join(retrieved_chunks),\n",
        "                f\"\\nKNOWLEDGE GRAPH CONTEXT:\",\n",
        "                kg_context,\n",
        "                f\"\\nLOCATION CONTEXT:\",\n",
        "                location_context\n",
        "            ]\n",
        "\n",
        "            enhanced_context = \"\\n\".join(context_parts)\n",
        "\n",
        "            # 6. GENERATE: Create comprehensive response\n",
        "            rag_prompt = f\"\"\"You are an expert historian with access to multiple knowledge sources. Answer the question using the provided context.\n",
        "\n",
        "{enhanced_context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Answer the question comprehensively using ALL available context\n",
        "2. Cite specific information from the retrieved text chunks\n",
        "3. Incorporate relevant knowledge graph facts to enhance your answer\n",
        "4. Include precise location information and coordinates when relevant\n",
        "5. If information conflicts between sources, mention this\n",
        "6. Be specific about dates, places, people, and events\n",
        "7. If you cannot answer completely, explain what information is missing\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "            # 7. GENERATE: Get LLM response\n",
        "            response = llm.invoke([HumanMessage(content=rag_prompt)])\n",
        "\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"answer\": response.content,\n",
        "                \"retrieved_chunks\": retrieved_chunks,\n",
        "                \"entities_found\": query_entities,\n",
        "                \"kg_facts_count\": sum(len(facts) for facts in kg_facts.values()),\n",
        "                \"locations_found\": list(enriched_locations.keys()),\n",
        "                \"sources\": {\n",
        "                    \"text_chunks\": len(retrieved_chunks),\n",
        "                    \"kg_sources\": list(set(fact.source for facts in kg_facts.values() for fact in facts)),\n",
        "                    \"location_sources\": list(set(loc.source for loc in enriched_locations.values()))\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"RAG query failed: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def interactive_rag_session(self, llm):\n",
        "        \"\"\"Start an interactive RAG session\"\"\"\n",
        "        print(\"\\n🤖 Starting Interactive RAG Session\")\n",
        "        print(\"Ask questions about your text. Type 'quit' to exit.\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                query = input(\"\\n❓ Your question: \").strip()\n",
        "\n",
        "                if query.lower() in ['quit', 'exit', 'q']:\n",
        "                    print(\"👋 Goodbye!\")\n",
        "                    break\n",
        "\n",
        "                if not query:\n",
        "                    continue\n",
        "\n",
        "                print(\"\\n🔍 Processing your question...\")\n",
        "                result = self.rag_query(query, llm)\n",
        "\n",
        "                if \"error\" in result:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n👋 Goodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "                continue\n",
        "\n",
        "# Utility functions\n",
        "def load_api_key():\n",
        "    \"\"\"Llama via Ollama doesn't need an API key\"\"\"\n",
        "    print(\"✅ Using local Ollama - no API key needed.\")\n",
        "    return \"local\"\n",
        "\n",
        "def load_text_from_file(filepath: str) -> str:\n",
        "    \"\"\"Load text from file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        print(f\"Loaded text from {filepath}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def initialize_llm(api_key: str):\n",
        "    \"\"\"Initialize Llama LLM\"\"\"\n",
        "    try:\n",
        "        llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
        "        print(\"✅ Llama LLM initialized successfully.\")\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing LLM: {e}\")\n",
        "        print(\"💡 Make sure Ollama is running: ollama serve\")\n",
        "        print(\"💡 And pull the model: ollama pull llama3.2\")\n",
        "        return None\n",
        "\n",
        "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
        "    \"\"\"Create vector store from text\"\"\"\n",
        "    try:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "        )\n",
        "\n",
        "        # Split text into sentences for better retrieval\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        texts = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 20]\n",
        "\n",
        "        if not texts:\n",
        "            return None\n",
        "\n",
        "        vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "        multi_kg_system.vectorstore = vectorstore\n",
        "        multi_kg_system.document_chunks = texts\n",
        "        print(f\"📚 Vector store created with {len(texts)} text segments\")\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with chunking support and RAG capabilities - LLAMA VERSION\"\"\"\n",
        "    print(\"🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\")\n",
        "\n",
        "    api_key = load_api_key()\n",
        "    if not api_key:\n",
        "        return\n",
        "\n",
        "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
        "    if not domain_text:\n",
        "        print(\"⚠️  No input file found, using sample text\")\n",
        "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC.\n",
        "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes.\n",
        "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
        "    else:\n",
        "        print(f\"📄 Using YOUR text from {INPUT_TEXT_FILE}\")\n",
        "        print(f\"📝 Text length: {len(domain_text)} characters\")\n",
        "\n",
        "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
        "    llm = initialize_llm(api_key)\n",
        "\n",
        "    if not llm:\n",
        "        return\n",
        "\n",
        "    # Prepare vector store for RAG FIRST\n",
        "    print(\"\\n📚 Setting up RAG vector store...\")\n",
        "    vectorstore = prepare_vectorstore_from_text(domain_text, multi_kg_system)\n",
        "\n",
        "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
        "    print(f\"🔢 Total tokens in text: {token_count:,}\")\n",
        "\n",
        "    if token_count > 10000:  # Reduced for Llama\n",
        "        print(\"📊 Text is large, chunking into smaller pieces...\")\n",
        "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=10000)\n",
        "        print(f\"📄 Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(\"📄 Text is small enough to process as single chunk\")\n",
        "        chunks = [domain_text]\n",
        "\n",
        "    # Extract events and create RDF\n",
        "    all_turtle_outputs = []\n",
        "    all_entities = set()\n",
        "\n",
        "    print(\"\\n🔄 Processing chunks for event extraction...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"\\n🔄 Processing chunk {i}/{len(chunks)}...\")\n",
        "\n",
        "        turtle_output = multi_kg_system.process_chunk(chunk, i, llm)\n",
        "        if turtle_output:\n",
        "            all_turtle_outputs.append(turtle_output)\n",
        "\n",
        "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
        "        all_entities.update(chunk_entities)\n",
        "\n",
        "        if i < len(chunks):\n",
        "            time.sleep(1)\n",
        "\n",
        "    # Save RDF output\n",
        "    if all_turtle_outputs:\n",
        "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
        "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
        "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
        "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
        "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
        "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
        "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        final_output = prefixes + \"# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
        "\n",
        "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output)\n",
        "\n",
        "        print(f\"\\n✅ Saved enhanced RDF to {OUTPUT_RAG_TTL}\")\n",
        "        print(f\"📊 Processing Statistics:\")\n",
        "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
        "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
        "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
        "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
        "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
        "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
        "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
        "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
        "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
        "        print(f\"   - RAG queries for RDF generation: {multi_kg_system.stats['rag_queries']}\")\n",
        "\n",
        "        print(f\"\\n🔗 Knowledge Graph Connector Statistics:\")\n",
        "        for name, connector in multi_kg_system.connectors.items():\n",
        "            stats = connector.get_stats()\n",
        "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
        "\n",
        "        if multi_kg_system.location_extractor.location_cache:\n",
        "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
        "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
        "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
        "\n",
        "        print(f\"\\n📝 Sample of generated RDF:\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No events were extracted from any chunks\")\n",
        "\n",
        "    # START RAG SESSION HERE!\n",
        "    if vectorstore:\n",
        "        print(f\"\\n🤖 RAG System Ready!\")\n",
        "\n",
        "        # Show example queries\n",
        "        print(f\"\\n💡 Try asking questions like:\")\n",
        "        print(f\"   - 'What battles happened in Sicily?'\")\n",
        "        print(f\"   - 'Who were the main leaders mentioned?'\")\n",
        "        print(f\"   - 'What events occurred in 415 BC?'\")\n",
        "        print(f\"   - 'Describe the naval engagements'\")\n",
        "        print(f\"   - 'What was the outcome of the siege?'\")\n",
        "\n",
        "        # Ask if user wants to start interactive session\n",
        "        response = input(f\"\\n❓ Start interactive RAG session? (y/n): \").strip()\n",
        "\n",
        "        # Check if user typed a question instead of y/n\n",
        "        if response.lower() not in ['y', 'yes', 'n', 'no', '']:\n",
        "            # User typed a question directly!\n",
        "            print(f\"\\n🔍 Processing your question: '{response}'\")\n",
        "            result = multi_kg_system.rag_query(response, llm)\n",
        "\n",
        "            if \"error\" not in result:\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "            else:\n",
        "                print(f\"❌ Error: {result['error']}\")\n",
        "\n",
        "            # Ask if they want to continue with interactive session\n",
        "            continue_response = input(f\"\\n❓ Continue with interactive RAG session? (y/n): \").strip().lower()\n",
        "            if continue_response in ['y', 'yes', '']:\n",
        "                multi_kg_system.interactive_rag_session(llm)\n",
        "\n",
        "        elif response.lower() in ['y', 'yes', '']:\n",
        "            multi_kg_system.interactive_rag_session(llm)\n",
        "        else:\n",
        "            print(f\"\\n💡 You can also query programmatically:\")\n",
        "            print(f\"   result = multi_kg_system.rag_query('your question', llm)\")\n",
        "\n",
        "            # Offer a few sample queries\n",
        "            sample_queries = [\n",
        "                \"What are the main events mentioned in the text?\",\n",
        "                \"Which locations are mentioned?\",\n",
        "                \"Who are the key people involved?\"\n",
        "            ]\n",
        "\n",
        "            print(f\"\\n🔍 Running sample queries:\")\n",
        "            for query in sample_queries:\n",
        "                print(f\"\\n❓ Sample query: '{query}'\")\n",
        "                result = multi_kg_system.rag_query(query, llm)\n",
        "                if \"error\" not in result:\n",
        "                    print(f\"📝 Answer: {result['answer'][:200]}...\")\n",
        "                    print(f\"📊 Sources: {len(result['retrieved_chunks'])} chunks, {result['kg_facts_count']} KG facts\")\n",
        "                else:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️  Could not create vector store for RAG functionality\")\n",
        "\n",
        "    print(f\"\\n🎉 Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM0tnTpyUM_1",
        "outputId": "15c9a6c3-d92c-4693-ddd2-8edc8d0fa402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2+cpu)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.11/dist-packages (7.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "\n",
            "Verifying Langchain Ollama import...\n",
            "✅ ChatOllama successfully imported after reinstallation.\n",
            "🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\n",
            "✅ Using local Ollama - no API key needed.\n",
            "Loaded text from /content/drive/MyDrive/part_aa\n",
            "📄 Using YOUR text from /content/drive/MyDrive/part_aa\n",
            "📝 Text length: 398568 characters\n",
            "✅ Llama LLM initialized successfully.\n",
            "\n",
            "📚 Setting up RAG vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Vector store created with 1980 text segments\n",
            "🔢 Total tokens in text: 99,642\n",
            "📊 Text is large, chunking into smaller pieces...\n",
            "📄 Created 10 chunks\n",
            "\n",
            "🔄 Processing chunks for event extraction...\n",
            "\n",
            "🔄 Processing chunk 1/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:Wikidata returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:DBpedia returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:Wikidata returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:DBpedia returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:⏰ Timeout: 1/18 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 2/10...\n",
            "\n",
            "🔄 Processing chunk 3/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:Wikidata returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:DBpedia returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:⏰ Timeout: 1/19 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 4/10...\n",
            "\n",
            "🔄 Processing chunk 5/10...\n",
            "\n",
            "🔄 Processing chunk 6/10...\n",
            "\n",
            "🔄 Processing chunk 7/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VII\n",
            "Second Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VII\n",
            "Second Year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 8/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VIII\n",
            "Third Year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 9/10...\n",
            "\n",
            "🔄 Processing chunk 10/10...\n",
            "\n",
            "✅ Saved enhanced RDF to /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama_1.ttl\n",
            "📊 Processing Statistics:\n",
            "   - Total chunks processed: 10\n",
            "   - Successful chunks: 10\n",
            "   - Unique entities found: 97\n",
            "   - Total KG facts retrieved: 4\n",
            "   - Cache hits: 252\n",
            "   - Locations found: 10\n",
            "   - Locations with coordinates: 10\n",
            "   - Location duplicates avoided: 5\n",
            "   - Unique global locations: 5\n",
            "   - RAG queries for RDF generation: 10\n",
            "\n",
            "🔗 Knowledge Graph Connector Statistics:\n",
            "   - Wikidata: 26/34 requests (76.5% success)\n",
            "   - DBpedia: 21/29 requests (72.4% success)\n",
            "   - ConceptNet: 2/45 requests (4.4% success)\n",
            "   - Location enrichment: 55/178 locations enriched (30.9% success)\n",
            "\n",
            "📝 Sample of generated RDF:\n",
            "============================================================\n",
            "@prefix ste: <http://www.example.org/ste#> .\n",
            "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
            "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
            "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
            "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
            "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
            "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
            "\n",
            "# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\n",
            "ste:Event1_1 a ste:Event, dbp:SpecificEventType ;\n",
            "    ste:hasType \"The Corinthians sent a herald before them to declare war and, getting under way with seventy-five ships and two thousand heavy infantry, sailed for Epidamnus to give battle to the Corcyraeans.\" ;\n",
            "    ste:hasAgent \"Corinthians\" ;\n",
            "    ste:hasTime \"after the whole army had mustered\" ;\n",
            "    ste:hasLocation \"Actium in the territory of Anactorium\" ;\n",
            "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
            "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
            "    ste:hasCountry \"\" ;\n",
            "    ste:hasRegion \"\" ;\n",
            "    ste:hasLocatio...\n",
            "============================================================\n",
            "\n",
            "🤖 RAG System Ready!\n",
            "\n",
            "💡 Try asking questions like:\n",
            "   - 'What battles happened in Sicily?'\n",
            "   - 'Who were the main leaders mentioned?'\n",
            "   - 'What events occurred in 415 BC?'\n",
            "   - 'Describe the naval engagements'\n",
            "   - 'What was the outcome of the siege?'\n",
            "\n",
            "❓ Start interactive RAG session? (y/n): n\n",
            "\n",
            "💡 You can also query programmatically:\n",
            "   result = multi_kg_system.rag_query('your question', llm)\n",
            "\n",
            "🔍 Running sample queries:\n",
            "\n",
            "❓ Sample query: 'What are the main events mentioned in the text?'\n",
            "📝 Answer: Based on the retrieved text chunks and knowledge graph context, I can identify the following main events mentioned in the text:\n",
            "\n",
            "1. **The Second Congress at Lacedaemon**: This event is mentioned as a ...\n",
            "📊 Sources: 2 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Which locations are mentioned?'\n",
            "📝 Answer: Based on the provided context, I can identify several locations that are mentioned in the retrieved text chunks:\n",
            "\n",
            "1. The Thracian towns: Although not specific locations are mentioned, it can be inferr...\n",
            "📊 Sources: 2 chunks, 12 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Who are the key people involved?'\n",
            "📝 Answer: Based on the provided context, it appears that the question \"Who are the key people involved?\" is related to a speech or oration being delivered, but the text does not explicitly mention specific indi...\n",
            "📊 Sources: 2 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "🎉 Process complete! Check /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama_1.ttl for RDF results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 1: Mount Drive and Install Dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Dependency Compatibility Guard ---\n",
        "import os\n",
        "\n",
        "# Force compatible versions for transformers and sentence-transformers\n",
        "os.system(\"pip install transformers==4.40.2 sentence-transformers==2.4.0 --quiet\")\n",
        "\n",
        "# (Optional) If you use torch, ensure it's compatible too:\n",
        "# os.system(\"pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu --quiet\")\n",
        "\n",
        "# Now safe to import\n",
        "\n",
        "!pip install  torch accelerate rdflib\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Enhanced Multi-Knowledge Graph RAG System with Text Chunking - LLAMA VERSION\n",
        "Handles large texts by processing them in chunks to avoid token limits\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import requests\n",
        "\n",
        "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal # <--- This line is crucial\n",
        "from rdflib.namespace import XSD, SKOS\n",
        "# Add a direct import test here to diagnose immediately\n",
        "print(\"\\nVerifying Langchain Ollama import...\")\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "    print(\"✅ ChatOllama successfully imported after reinstallation.\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ CRITICAL ERROR: ChatOllama still cannot be imported after reinstallation: {e}\")\n",
        "    print(\"Please check your pip installation output for errors and ensure your Colab runtime is healthy.\")\n",
        "    # You might want to exit here if this is a persistent issue\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"❌ An unexpected error occurred during ChatOllama import verification: {e}\")\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "\n",
        "\n",
        "# Configuration\n",
        "INPUT_TEXT_FILE = \"/content/drive/MyDrive/part_aa\"\n",
        "ONTOLOGY_PATH = \"/content/drive/MyDrive/wiki.owl\"\n",
        "LOCATION_ONTOLOGY_PATH = \"/content/drive/MyDrive/locations.owl\"\n",
        "OUTPUT_RAG_TTL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl'\n",
        "OUTPUT_RAG_OWL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.owl'\n",
        "KG_CACHE_FILE = '/content/drive/MyDrive/kg_cache.json'\n",
        "LOCATION_CACHE_FILE = '/content/drive/MyDrive/location_cache.json'\n",
        "KG_ANALYSIS_REPORT = '/content/drive/MyDrive/multi_kg_analysis_report.txt'\n",
        "\n",
        "# Token limits - adjusted for Llama\n",
        "MAX_TOKENS_PER_REQUEST = 50000  # Conservative limit for Llama\n",
        "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
        "\n",
        "# Namespaces\n",
        "EX = Namespace(\"http://example.org/\")\n",
        "STE = Namespace(\"http://www.example.org/ste#\")\n",
        "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
        "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
        "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
        "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
        "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
        "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Imports\n",
        "try:\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_ollama import ChatOllama  # Changed from OpenAI to Ollama\n",
        "    from langchain.schema import HumanMessage\n",
        "except ImportError as e:\n",
        "    print(f\"ImportError: {e}\")\n",
        "    print(\"pip install rdflib python-dotenv langchain langchain-ollama langchain-community faiss-cpu sentence-transformers requests\")\n",
        "    exit(1)\n",
        "\n",
        "@dataclass\n",
        "class LocationInfo:\n",
        "    \"\"\"Location information with coordinates\"\"\"\n",
        "    name: str\n",
        "    latitude: Optional[float] = None\n",
        "    longitude: Optional[float] = None\n",
        "    country: Optional[str] = None\n",
        "    region: Optional[str] = None\n",
        "    source: str = \"extracted\"\n",
        "    confidence: float = 1.0\n",
        "    uri: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class EnhancedKnowledgeFact:\n",
        "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
        "    subject: str\n",
        "    predicate: str\n",
        "    object: str\n",
        "    source: str\n",
        "    confidence: float = 1.0\n",
        "    context: Optional[str] = None\n",
        "    temporal: Optional[str] = None\n",
        "    spatial: Optional[str] = None\n",
        "    evidence_score: float = 1.0\n",
        "    source_uri: Optional[str] = None\n",
        "\n",
        "class LocationExtractor:\n",
        "    \"\"\"Extracts and enriches location information\"\"\"\n",
        "\n",
        "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
        "        self.ontology_path = ontology_path\n",
        "        self.location_graph = None\n",
        "        self.location_cache = self._load_location_cache()\n",
        "        self.load_location_ontology()\n",
        "\n",
        "    def _load_location_cache(self) -> Dict:\n",
        "        \"\"\"Load location cache\"\"\"\n",
        "        if os.path.exists(LOCATION_CACHE_FILE):\n",
        "            try:\n",
        "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load location cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_location_cache(self):\n",
        "        \"\"\"Save location cache\"\"\"\n",
        "        try:\n",
        "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save location cache: {e}\")\n",
        "\n",
        "    def load_location_ontology(self):\n",
        "        \"\"\"Load locations.owl ontology\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.ontology_path):\n",
        "                self.location_graph = Graph()\n",
        "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
        "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
        "                self.location_graph = None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading location ontology: {e}\")\n",
        "            self.location_graph = None\n",
        "\n",
        "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract potential location names from text\"\"\"\n",
        "        location_patterns = [\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
        "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
        "        ]\n",
        "\n",
        "        locations = []\n",
        "        for pattern in location_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            locations.extend(matches)\n",
        "\n",
        "        location_stopwords = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If',\n",
        "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
        "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday',\n",
        "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
        "        }\n",
        "\n",
        "        filtered_locations = []\n",
        "        for loc in locations:\n",
        "            loc = loc.strip()\n",
        "            if (loc not in location_stopwords and len(loc) > 2 and\n",
        "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
        "                filtered_locations.append(loc)\n",
        "\n",
        "        return list(set(filtered_locations))\n",
        "\n",
        "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location info from local ontology\"\"\"\n",
        "        if not self.location_graph:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            query = f\"\"\"\n",
        "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
        "                ?location rdfs:label ?label .\n",
        "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
        "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
        "                OPTIONAL {{ ?location geo:long ?long }}\n",
        "                OPTIONAL {{ ?location dbp:country ?country }}\n",
        "                OPTIONAL {{ ?location dbp:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            results = self.location_graph.query(query)\n",
        "            for row in results:\n",
        "                return LocationInfo(\n",
        "                    name=location_name,\n",
        "                    latitude=float(row.lat) if row.lat else None,\n",
        "                    longitude=float(row.long) if row.long else None,\n",
        "                    country=str(row.country) if row.country else None,\n",
        "                    region=str(row.region) if row.region else None,\n",
        "                    source=\"local_ontology\",\n",
        "                    uri=str(row.location) if row.location else None\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
        "                <{entity_uri}> geo:lat ?lat ;\n",
        "                               geo:long ?long .\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                if bindings:\n",
        "                    binding = bindings[0]\n",
        "                    return LocationInfo(\n",
        "                        name=location_name,\n",
        "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
        "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
        "                        country=binding.get('country', {}).get('value', ''),\n",
        "                        region=binding.get('region', {}).get('value', ''),\n",
        "                        source=\"dbpedia\",\n",
        "                        uri=entity_uri\n",
        "                    )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            # Try multiple query strategies to get the right location\n",
        "            queries = [\n",
        "                # Try exact label match first\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\",\n",
        "                # Try with additional filters for places/locations\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\"\n",
        "            ]\n",
        "\n",
        "            for query in queries:\n",
        "                params = {'query': query, 'format': 'json'}\n",
        "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                    if bindings:\n",
        "                        # Prefer results with country information\n",
        "                        best_binding = None\n",
        "                        for binding in bindings:\n",
        "                            if binding.get('country'):\n",
        "                                best_binding = binding\n",
        "                                break\n",
        "\n",
        "                        if not best_binding:\n",
        "                            best_binding = bindings[0]\n",
        "\n",
        "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
        "\n",
        "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
        "                        if coord_match:\n",
        "                            longitude = float(coord_match.group(1))\n",
        "                            latitude = float(coord_match.group(2))\n",
        "\n",
        "                            return LocationInfo(\n",
        "                                name=location_name,\n",
        "                                latitude=latitude,\n",
        "                                longitude=longitude,\n",
        "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
        "                                source=\"wikidata\",\n",
        "                                uri=best_binding.get('item', {}).get('value', '')\n",
        "                            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
        "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
        "        if not location_info.latitude or not location_info.longitude:\n",
        "            return True\n",
        "\n",
        "        lat, lon = location_info.latitude, location_info.longitude\n",
        "\n",
        "        # Basic coordinate range validation\n",
        "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
        "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
        "        if location_name in self.location_cache:\n",
        "            cached = self.location_cache[location_name]\n",
        "            return LocationInfo(**cached) if cached else None\n",
        "\n",
        "        location_info = None\n",
        "\n",
        "        location_info = self.get_location_from_ontology(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_wikidata(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_dbpedia(location_name)\n",
        "\n",
        "        if location_info:\n",
        "            self.location_cache[location_name] = {\n",
        "                'name': location_info.name,\n",
        "                'latitude': location_info.latitude,\n",
        "                'longitude': location_info.longitude,\n",
        "                'country': location_info.country,\n",
        "                'region': location_info.region,\n",
        "                'source': location_info.source,\n",
        "                'confidence': location_info.confidence,\n",
        "                'uri': location_info.uri\n",
        "            }\n",
        "        else:\n",
        "            self.location_cache[location_name] = None\n",
        "\n",
        "        self._save_location_cache()\n",
        "\n",
        "        if location_info:\n",
        "            self.validate_coordinates(location_info)\n",
        "\n",
        "        return location_info\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Handles text chunking to manage token limits for Llama\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"llama3.2\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Approximate token count for Llama (roughly 4 chars per token)\"\"\"\n",
        "        return len(text) // 4\n",
        "\n",
        "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 10000) -> List[str]:\n",
        "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class BaseKGConnector:\n",
        "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
        "        self.name = name\n",
        "        self.base_url = base_url\n",
        "        self.rate_limit = rate_limit\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "        self.success_count = 0\n",
        "\n",
        "    def _rate_limit_wait(self):\n",
        "        \"\"\"Enforce rate limiting\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "        if time_since_last < self.rate_limit:\n",
        "            time.sleep(self.rate_limit - time_since_last)\n",
        "        self.last_request_time = time.time()\n",
        "        self.request_count += 1\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get connector statistics\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'requests': self.request_count,\n",
        "            'successes': self.success_count,\n",
        "            'success_rate': self.success_count / max(1, self.request_count)\n",
        "        }\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EnhancedWikidataConnector(BaseKGConnector):\n",
        "    \"\"\"Wikidata connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
        "              {{\n",
        "                ?subject ?label \"{entity}\"@en .\n",
        "              }} UNION {{\n",
        "                ?subject rdfs:label \"{entity}\"@en .\n",
        "              }}\n",
        "\n",
        "              ?subject ?predicate ?object .\n",
        "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
        "\n",
        "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
        "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
        "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
        "                        source=self.name,\n",
        "                        confidence=0.9,\n",
        "                        source_uri=binding.get('subject', {}).get('value')\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedDBpediaConnector(BaseKGConnector):\n",
        "    \"\"\"DBpedia connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
        "              <{entity_uri}> ?predicate ?object .\n",
        "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
        "              FILTER(!isBlank(?object))\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    predicate = binding.get('predicate', {}).get('value', '')\n",
        "                    obj = binding.get('object', {}).get('value', '')\n",
        "\n",
        "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
        "\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=entity,\n",
        "                        predicate=predicate_name,\n",
        "                        object=obj,\n",
        "                        source=self.name,\n",
        "                        confidence=0.85,\n",
        "                        source_uri=entity_uri\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedConceptNetConnector(BaseKGConnector):\n",
        "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
        "\n",
        "    def search_related_concepts(self, entity: str) -> List[str]:\n",
        "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
        "        try:\n",
        "            # Try search API first\n",
        "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
        "            response = requests.get(search_url, timeout=10)\n",
        "\n",
        "            related_concepts = []\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for edge in data.get('edges', []):\n",
        "                    start = edge.get('start', {}).get('label', '')\n",
        "                    end = edge.get('end', {}).get('label', '')\n",
        "\n",
        "                    # Extract concept paths and clean them\n",
        "                    for concept_path in [start, end]:\n",
        "                        if concept_path and '/c/en/' in concept_path:\n",
        "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
        "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
        "                                related_concepts.append(concept)\n",
        "\n",
        "            return list(set(related_concepts))[:5]  # Return top 5 unique concepts\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
        "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
        "        try:\n",
        "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
        "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return data.get('edges', [])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "            all_facts = []\n",
        "\n",
        "            # Strategy 1: Try direct query first\n",
        "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
        "\n",
        "            # Strategy 2: Search for related concepts and query them\n",
        "            related_concepts = self.search_related_concepts(entity)\n",
        "\n",
        "            # Process direct edges\n",
        "            for edge in direct_edges:\n",
        "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
        "                if fact:\n",
        "                    all_facts.append(fact)\n",
        "\n",
        "            # Process related concept edges\n",
        "            for concept in related_concepts:\n",
        "                concept_edges = self.query_concept_directly(concept, 5)\n",
        "                for edge in concept_edges:\n",
        "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
        "                    if fact:\n",
        "                        all_facts.append(fact)\n",
        "\n",
        "            if all_facts:\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
        "                if related_concepts:\n",
        "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
        "\n",
        "            return all_facts[:limit]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
        "        try:\n",
        "            start = edge.get('start', {})\n",
        "            end = edge.get('end', {})\n",
        "            relation = edge.get('rel', {})\n",
        "            weight = edge.get('weight', 1.0)\n",
        "\n",
        "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            rel_label = relation.get('label', 'related_to')\n",
        "\n",
        "            # Skip if labels are empty or too short\n",
        "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
        "                return None\n",
        "\n",
        "            # Determine confidence based on discovery method\n",
        "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
        "\n",
        "            return EnhancedKnowledgeFact(\n",
        "                subject=original_entity,\n",
        "                predicate=rel_label,\n",
        "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
        "                source=self.name,\n",
        "                confidence=min(weight * confidence_multiplier, 1.0),\n",
        "                context=f\"Discovered {discovery_method}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
        "            return None\n",
        "\n",
        "class MultiKGCache:\n",
        "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
        "\n",
        "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
        "        self.cache_file = cache_file\n",
        "        self.cache = self._load_cache()\n",
        "\n",
        "    def _load_cache(self) -> Dict:\n",
        "        \"\"\"Load cache from file\"\"\"\n",
        "        if os.path.exists(self.cache_file):\n",
        "            try:\n",
        "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_cache(self):\n",
        "        \"\"\"Save cache to file\"\"\"\n",
        "        try:\n",
        "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save cache: {e}\")\n",
        "\n",
        "    def get_cache_key(self, source: str, entity: str) -> str:\n",
        "        \"\"\"Generate cache key\"\"\"\n",
        "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
        "\n",
        "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Get cached facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        return self.cache.get(key)\n",
        "\n",
        "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
        "        \"\"\"Cache facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        serializable_facts = []\n",
        "        for fact in facts:\n",
        "            serializable_facts.append({\n",
        "                'subject': fact.subject,\n",
        "                'predicate': fact.predicate,\n",
        "                'object': fact.object,\n",
        "                'source': fact.source,\n",
        "                'confidence': fact.confidence,\n",
        "                'context': fact.context,\n",
        "                'temporal': fact.temporal,\n",
        "                'spatial': fact.spatial,\n",
        "                'evidence_score': fact.evidence_score,\n",
        "                'source_uri': fact.source_uri\n",
        "            })\n",
        "        self.cache[key] = serializable_facts\n",
        "        self._save_cache()\n",
        "\n",
        "class EnhancedMultiKGRAGSystem:\n",
        "    \"\"\"Multi-Knowledge Graph RAG system with chunking and location extraction - LLAMA VERSION\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.connectors = {\n",
        "            'wikidata': EnhancedWikidataConnector(),\n",
        "            'dbpedia': EnhancedDBpediaConnector(),\n",
        "            'conceptnet': EnhancedConceptNetConnector()\n",
        "        }\n",
        "        self.cache = MultiKGCache()\n",
        "        self.chunker = TextChunker()\n",
        "        self.location_extractor = LocationExtractor()\n",
        "        self.global_locations = {}\n",
        "        self.vectorstore = None  # For RAG retrieval\n",
        "        self.document_chunks = []  # Store processed chunks for RAG\n",
        "        self.stats = {\n",
        "            'queries_processed': 0,\n",
        "            'entities_extracted': 0,\n",
        "            'facts_retrieved': 0,\n",
        "            'cache_hits': 0,\n",
        "            'chunks_processed': 0,\n",
        "            'locations_found': 0,\n",
        "            'locations_with_coordinates': 0,\n",
        "            'location_duplicates_avoided': 0,\n",
        "            'rag_queries': 0\n",
        "        }\n",
        "\n",
        "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract entities from text\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
        "        matches = re.findall(pattern, text)\n",
        "        entities.extend(matches)\n",
        "\n",
        "        stop_words = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
        "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
        "        }\n",
        "\n",
        "        filtered_entities = []\n",
        "        for entity in entities:\n",
        "            entity = entity.strip()\n",
        "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
        "                filtered_entities.append(entity)\n",
        "\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for entity in filtered_entities:\n",
        "            if entity.lower() not in seen:\n",
        "                seen.add(entity.lower())\n",
        "                unique_entities.append(entity)\n",
        "\n",
        "        return unique_entities[:25]\n",
        "\n",
        "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
        "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
        "        all_facts = {}\n",
        "        cache_hits = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            for entity in entities:\n",
        "                for source_name, connector in self.connectors.items():\n",
        "                    # Check cache first\n",
        "                    cached_facts = self.cache.get(source_name, entity)\n",
        "                    if cached_facts:\n",
        "                        cache_hits += 1\n",
        "                        if entity not in all_facts:\n",
        "                            all_facts[entity] = []\n",
        "                        for fact_data in cached_facts:\n",
        "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
        "                            all_facts[entity].append(fact)\n",
        "                    else:\n",
        "                        future = executor.submit(connector.retrieve_facts, entity, 10)\n",
        "                        futures[future] = (entity, source_name)\n",
        "\n",
        "            # Collect results with better timeout handling\n",
        "            completed = 0\n",
        "            total_futures = len(futures)\n",
        "\n",
        "            try:\n",
        "                for future in as_completed(futures, timeout=45):  # Increased timeout\n",
        "                    entity, source_name = futures[future]\n",
        "                    completed += 1\n",
        "\n",
        "                    try:\n",
        "                        facts = future.result(timeout=5)  # Individual future timeout\n",
        "                        if facts:\n",
        "                            self.cache.set(source_name, entity, facts)\n",
        "\n",
        "                            if entity not in all_facts:\n",
        "                                all_facts[entity] = []\n",
        "                            all_facts[entity].extend(facts)\n",
        "\n",
        "                            self.stats['facts_retrieved'] += len(facts)\n",
        "\n",
        "                        logger.debug(f\"✅ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"❌ {source_name} failed for {entity}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except TimeoutError:\n",
        "                pending_count = total_futures - completed\n",
        "                logger.warning(f\"⏰ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
        "\n",
        "                # Cancel remaining futures\n",
        "                for future in futures:\n",
        "                    if not future.done():\n",
        "                        future.cancel()\n",
        "\n",
        "        self.stats['cache_hits'] += cache_hits\n",
        "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
        "        return all_facts\n",
        "\n",
        "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
        "        \"\"\"Format KG facts into context string\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        for entity, facts in kg_facts.items():\n",
        "            if facts:\n",
        "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
        "\n",
        "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
        "\n",
        "                by_source = {}\n",
        "                for fact in sorted_facts[:8]:\n",
        "                    if fact.source not in by_source:\n",
        "                        by_source[fact.source] = []\n",
        "                    by_source[fact.source].append(fact)\n",
        "\n",
        "                for source, source_facts in by_source.items():\n",
        "                    context_parts.append(f\"\\nFrom {source}:\")\n",
        "                    for fact in source_facts[:4]:\n",
        "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
        "                        if fact.confidence < 0.8:\n",
        "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
        "                        context_parts.append(fact_str)\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
        "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
        "        location_key = location_info.name.lower().strip()\n",
        "\n",
        "        if location_key in self.global_locations:\n",
        "            existing = self.global_locations[location_key]\n",
        "            if (location_info.latitude and location_info.longitude and\n",
        "                (not existing.latitude or not existing.longitude)):\n",
        "                self.global_locations[location_key] = location_info\n",
        "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
        "            else:\n",
        "                self.stats['location_duplicates_avoided'] += 1\n",
        "                logger.debug(f\"Location {location_info.name} already registered\")\n",
        "        else:\n",
        "            self.global_locations[location_key] = location_info\n",
        "            logger.info(f\"Registered new location: {location_info.name}\")\n",
        "\n",
        "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "        return f\"ste:Location_{clean_name}\"\n",
        "\n",
        "    def process_chunk(self, chunk: str, chunk_num: int, llm) -> str:\n",
        "        \"\"\"Process a single chunk of text with RAG-enhanced location extraction\"\"\"\n",
        "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars)\")\n",
        "\n",
        "        # 1. RAG RETRIEVAL: Get relevant context from vector store\n",
        "        relevant_context = \"\"\n",
        "        if self.vectorstore:\n",
        "            try:\n",
        "                # Use the chunk as a query to retrieve similar/relevant text\n",
        "                relevant_docs = self.vectorstore.similarity_search(chunk, k=15)\n",
        "                retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "                relevant_context = \"\\n---\\n\".join(retrieved_chunks)\n",
        "                logger.info(f\"Retrieved {len(retrieved_chunks)} relevant chunks via RAG for chunk {chunk_num}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"RAG retrieval failed for chunk {chunk_num}: {e}\")\n",
        "                relevant_context = \"\"\n",
        "\n",
        "        # 2. Extract entities and locations\n",
        "        entities = self.extract_entities_advanced(chunk)\n",
        "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
        "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
        "\n",
        "        # 3. Enrich locations with coordinates\n",
        "        enriched_locations = {}\n",
        "        for location_name in locations[:15]:  # Limit to 5 to avoid overwhelming\n",
        "            location_info = self.location_extractor.enrich_location(location_name)\n",
        "            if location_info:\n",
        "                self.register_global_location(location_info)\n",
        "                enriched_locations[location_name] = location_info\n",
        "                self.stats['locations_found'] += 1\n",
        "                if location_info.latitude and location_info.longitude:\n",
        "                    self.stats['locations_with_coordinates'] += 1\n",
        "\n",
        "        if not entities and not enriched_locations:\n",
        "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
        "            return \"\"\n",
        "\n",
        "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
        "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
        "\n",
        "        # 4. Get KG facts for entities\n",
        "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
        "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "        location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "        # 5. RAG-ENHANCED PROMPT: Use retrieved context + KG facts + locations\n",
        "        enhanced_prompt = f\"\"\"You are extracting historical events using RAG (Retrieval-Augmented Generation). Use ALL available context sources to enhance your extraction.\n",
        "\n",
        "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
        "{chunk}\n",
        "\n",
        "RAG RETRIEVED RELEVANT CONTEXT:\n",
        "{relevant_context if relevant_context else \"No relevant context retrieved.\"}\n",
        "\n",
        "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
        "{kg_context}\n",
        "\n",
        "LOCATION INFORMATION WITH COORDINATES:\n",
        "{location_context}\n",
        "\n",
        "TASK: Extract ONLY the events that are actually mentioned in the current text chunk. Use the RAG retrieved context, KG facts, and location coordinates to enhance details but stay faithful to what's actually in the current chunk.\n",
        "\n",
        "Requirements:\n",
        "1. Extract ONLY events mentioned in the CURRENT text chunk (not from retrieved context)\n",
        "2. Use RAG retrieved context to provide additional historical context and validation\n",
        "3. Use KG facts to enhance entity information\n",
        "4. Use location coordinates to provide precise geographical data\n",
        "5. Include ALL these properties for each event:\n",
        "   - ste:hasType (description of event)\n",
        "   - ste:hasAgent (who caused/led the event)\n",
        "   - ste:hasTime (when it happened)\n",
        "   - ste:hasLocation (location name from text)\n",
        "   - ste:hasLatitude (latitude coordinate if available)\n",
        "   - ste:hasLongitude (longitude coordinate if available)\n",
        "   - ste:hasCountry (country if available)\n",
        "   - ste:hasRegion (region if available)\n",
        "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
        "   - ste:hasResult (outcome/consequence)\n",
        "   - ste:hasRAGContext \"yes\" (to indicate this was RAG-enhanced)\n",
        "\n",
        "Output format (do not include prefixes, they will be added later):\n",
        "```turtle\n",
        "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
        "    ste:hasType \"specific description from current chunk\" ;\n",
        "    ste:hasAgent \"specific person from current chunk\" ;\n",
        "    ste:hasTime \"specific date from current chunk\" ;\n",
        "    ste:hasLocation \"specific location from current chunk\" ;\n",
        "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
        "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
        "    ste:hasCountry \"Italy\" ;\n",
        "    ste:hasRegion \"Sicily\" ;\n",
        "    ste:hasLocationSource \"wikidata\" ;\n",
        "    ste:hasResult \"specific outcome from current chunk\" ;\n",
        "    ste:hasRAGContext \"yes\" .\n",
        "```\n",
        "\n",
        "IMPORTANT:\n",
        "- The PRIMARY source is the CURRENT text chunk - extract events from IT\n",
        "- Use RAG retrieved context to validate and enhance your understanding\n",
        "- Use KG facts to enrich entity details\n",
        "- Include precise coordinates from location sources\n",
        "- Mark all events with ste:hasRAGContext \"yes\" to show RAG was used\n",
        "- Only extract events explicitly mentioned in the current chunk\n",
        "- If no clear events are found in current chunk, return empty\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = llm.invoke([HumanMessage(content=enhanced_prompt)])\n",
        "            turtle_output = self.clean_turtle(response.content)\n",
        "            self.stats['chunks_processed'] += 1\n",
        "            self.stats['rag_queries'] += 1  # Count as RAG usage\n",
        "            logger.info(f\"Generated RAG-enhanced RDF for chunk {chunk_num}\")\n",
        "            return turtle_output\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing chunk {chunk_num} with RAG: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
        "        \"\"\"Format location information into context string\"\"\"\n",
        "        if not enriched_locations:\n",
        "            return \"No location coordinates available.\"\n",
        "\n",
        "        context_parts = [\"\\n=== Location Information ===\"]\n",
        "\n",
        "        for location_name, location_info in enriched_locations.items():\n",
        "            context_parts.append(f\"\\n{location_name}:\")\n",
        "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
        "            else:\n",
        "                context_parts.append(\"  - Coordinates: Not available\")\n",
        "\n",
        "            if location_info.country:\n",
        "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
        "\n",
        "            if location_info.region:\n",
        "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
        "\n",
        "            if location_info.uri:\n",
        "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def generate_global_location_rdf(self) -> str:\n",
        "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
        "        if not self.global_locations:\n",
        "            return \"\"\n",
        "\n",
        "        location_rdf_parts = []\n",
        "\n",
        "        for location_key, location_info in self.global_locations.items():\n",
        "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "            location_id = f\"ste:Location_{clean_name}\"\n",
        "\n",
        "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
        "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
        "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
        "\n",
        "            if location_info.country:\n",
        "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
        "\n",
        "            if location_info.region:\n",
        "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
        "\n",
        "            if location_info.source:\n",
        "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
        "\n",
        "            if location_info.uri:\n",
        "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
        "\n",
        "            if rdf_lines[-1].endswith(' ;'):\n",
        "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
        "\n",
        "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
        "\n",
        "        return '\\n\\n'.join(location_rdf_parts)\n",
        "\n",
        "    def clean_turtle(self, raw_output: str) -> str:\n",
        "        \"\"\"Clean turtle output\"\"\"\n",
        "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "\n",
        "        lines = raw_output.strip().split('\\n')\n",
        "        turtle_lines = []\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if (stripped.startswith('@') or stripped.startswith('<') or\n",
        "                stripped.startswith(':') or stripped.startswith('_') or\n",
        "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
        "                turtle_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(turtle_lines)\n",
        "\n",
        "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
        "        \"\"\"Create vector store from text chunks for RAG retrieval\"\"\"\n",
        "        try:\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            from langchain_community.vectorstores import FAISS\n",
        "\n",
        "            embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "            )\n",
        "\n",
        "            # Prepare chunks with metadata\n",
        "            documents = []\n",
        "            metadatas = []\n",
        "\n",
        "            for i, chunk in enumerate(text_chunks):\n",
        "                if len(chunk.strip()) > 50:  # Only include substantial chunks\n",
        "                    documents.append(chunk)\n",
        "                    metadatas.append({\n",
        "                        'chunk_id': i,\n",
        "                        'length': len(chunk),\n",
        "                        'type': 'text_chunk'\n",
        "                    })\n",
        "\n",
        "            if documents:\n",
        "                self.vectorstore = FAISS.from_texts(documents, embeddings, metadatas=metadatas)\n",
        "                self.document_chunks = documents\n",
        "                logger.info(f\"Created vector store with {len(documents)} chunks\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.warning(\"No suitable chunks found for vector store\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating vector store: {e}\")\n",
        "            return False\n",
        "\n",
        "    def rag_query(self, query: str, llm, k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Perform RAG query: retrieve relevant chunks + KG facts, then generate response\"\"\"\n",
        "        self.stats['rag_queries'] += 1\n",
        "        logger.info(f\"Processing RAG query: '{query[:50]}...'\")\n",
        "\n",
        "        if not self.vectorstore:\n",
        "            return {\"error\": \"Vector store not initialized. Call prepare_vectorstore() first.\"}\n",
        "\n",
        "        try:\n",
        "            # 1. RETRIEVE: Get relevant text chunks\n",
        "            relevant_docs = self.vectorstore.similarity_search(query, k=k)\n",
        "            retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "            # 2. EXTRACT: Get entities from query\n",
        "            query_entities = self.extract_entities_advanced(query)\n",
        "\n",
        "            # 3. RETRIEVE: Get KG facts for entities\n",
        "            kg_facts = self.retrieve_kg_facts_enhanced(query_entities)\n",
        "            kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "\n",
        "            # 4. EXTRACT & ENRICH: Get locations from query\n",
        "            query_locations = self.location_extractor.extract_locations_from_text(query)\n",
        "            enriched_locations = {}\n",
        "            for location_name in query_locations[:5]:\n",
        "                location_info = self.location_extractor.enrich_location(location_name)\n",
        "                if location_info:\n",
        "                    enriched_locations[location_name] = location_info\n",
        "\n",
        "            location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "            # 5. AUGMENT: Create enhanced context for generation\n",
        "            context_parts = [\n",
        "                f\"QUERY: {query}\",\n",
        "                f\"\\nRETRIEVED RELEVANT TEXT CHUNKS:\",\n",
        "                \"\\n\" + \"\\n---\\n\".join(retrieved_chunks),\n",
        "                f\"\\nKNOWLEDGE GRAPH CONTEXT:\",\n",
        "                kg_context,\n",
        "                f\"\\nLOCATION CONTEXT:\",\n",
        "                location_context\n",
        "            ]\n",
        "\n",
        "            enhanced_context = \"\\n\".join(context_parts)\n",
        "\n",
        "            # 6. GENERATE: Create comprehensive response\n",
        "            rag_prompt = f\"\"\"You are an expert historian with access to multiple knowledge sources. Answer the question using the provided context.\n",
        "\n",
        "{enhanced_context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Answer the question comprehensively using ALL available context\n",
        "2. Cite specific information from the retrieved text chunks\n",
        "3. Incorporate relevant knowledge graph facts to enhance your answer\n",
        "4. Include precise location information and coordinates when relevant\n",
        "5. If information conflicts between sources, mention this\n",
        "6. Be specific about dates, places, people, and events\n",
        "7. If you cannot answer completely, explain what information is missing\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "            # 7. GENERATE: Get LLM response\n",
        "            response = llm.invoke([HumanMessage(content=rag_prompt)])\n",
        "\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"answer\": response.content,\n",
        "                \"retrieved_chunks\": retrieved_chunks,\n",
        "                \"entities_found\": query_entities,\n",
        "                \"kg_facts_count\": sum(len(facts) for facts in kg_facts.values()),\n",
        "                \"locations_found\": list(enriched_locations.keys()),\n",
        "                \"sources\": {\n",
        "                    \"text_chunks\": len(retrieved_chunks),\n",
        "                    \"kg_sources\": list(set(fact.source for facts in kg_facts.values() for fact in facts)),\n",
        "                    \"location_sources\": list(set(loc.source for loc in enriched_locations.values()))\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"RAG query failed: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def interactive_rag_session(self, llm):\n",
        "        \"\"\"Start an interactive RAG session\"\"\"\n",
        "        print(\"\\n🤖 Starting Interactive RAG Session\")\n",
        "        print(\"Ask questions about your text. Type 'quit' to exit.\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                query = input(\"\\n❓ Your question: \").strip()\n",
        "\n",
        "                if query.lower() in ['quit', 'exit', 'q']:\n",
        "                    print(\"👋 Goodbye!\")\n",
        "                    break\n",
        "\n",
        "                if not query:\n",
        "                    continue\n",
        "\n",
        "                print(\"\\n🔍 Processing your question...\")\n",
        "                result = self.rag_query(query, llm)\n",
        "\n",
        "                if \"error\" in result:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n👋 Goodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "                continue\n",
        "\n",
        "# Utility functions\n",
        "def load_api_key():\n",
        "    \"\"\"Llama via Ollama doesn't need an API key\"\"\"\n",
        "    print(\"✅ Using local Ollama - no API key needed.\")\n",
        "    return \"local\"\n",
        "\n",
        "def load_text_from_file(filepath: str) -> str:\n",
        "    \"\"\"Load text from file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        print(f\"Loaded text from {filepath}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def initialize_llm(api_key: str):\n",
        "    \"\"\"Initialize Llama LLM\"\"\"\n",
        "    try:\n",
        "        llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
        "        print(\"✅ Llama LLM initialized successfully.\")\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing LLM: {e}\")\n",
        "        print(\"💡 Make sure Ollama is running: ollama serve\")\n",
        "        print(\"💡 And pull the model: ollama pull llama3.2\")\n",
        "        return None\n",
        "\n",
        "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
        "    \"\"\"Create vector store from text\"\"\"\n",
        "    try:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "        )\n",
        "\n",
        "        # Split text into sentences for better retrieval\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        texts = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 20]\n",
        "\n",
        "        if not texts:\n",
        "            return None\n",
        "\n",
        "        vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "        multi_kg_system.vectorstore = vectorstore\n",
        "        multi_kg_system.document_chunks = texts\n",
        "        print(f\"📚 Vector store created with {len(texts)} text segments\")\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with chunking support and RAG capabilities - LLAMA VERSION\"\"\"\n",
        "    print(\"🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\")\n",
        "\n",
        "    api_key = load_api_key()\n",
        "    if not api_key:\n",
        "        return\n",
        "\n",
        "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
        "    if not domain_text:\n",
        "        print(\"⚠️  No input file found, using sample text\")\n",
        "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC.\n",
        "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes.\n",
        "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
        "    else:\n",
        "        print(f\"📄 Using YOUR text from {INPUT_TEXT_FILE}\")\n",
        "        print(f\"📝 Text length: {len(domain_text)} characters\")\n",
        "\n",
        "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
        "    llm = initialize_llm(api_key)\n",
        "\n",
        "    if not llm:\n",
        "        return\n",
        "\n",
        "    # Prepare vector store for RAG FIRST\n",
        "    print(\"\\n📚 Setting up RAG vector store...\")\n",
        "    vectorstore = prepare_vectorstore_from_text(domain_text, multi_kg_system)\n",
        "\n",
        "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
        "    print(f\"🔢 Total tokens in text: {token_count:,}\")\n",
        "\n",
        "    if token_count > 10000:  # Reduced for Llama\n",
        "        print(\"📊 Text is large, chunking into smaller pieces...\")\n",
        "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=10000)\n",
        "        print(f\"📄 Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(\"📄 Text is small enough to process as single chunk\")\n",
        "        chunks = [domain_text]\n",
        "\n",
        "    # Extract events and create RDF\n",
        "    all_turtle_outputs = []\n",
        "    all_entities = set()\n",
        "\n",
        "    print(\"\\n🔄 Processing chunks for event extraction...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"\\n🔄 Processing chunk {i}/{len(chunks)}...\")\n",
        "\n",
        "        turtle_output = multi_kg_system.process_chunk(chunk, i, llm)\n",
        "        if turtle_output:\n",
        "            all_turtle_outputs.append(turtle_output)\n",
        "\n",
        "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
        "        all_entities.update(chunk_entities)\n",
        "\n",
        "        if i < len(chunks):\n",
        "            time.sleep(1)\n",
        "\n",
        "    # Save RDF output\n",
        "    if all_turtle_outputs:\n",
        "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
        "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
        "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
        "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
        "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
        "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
        "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        final_output = prefixes + \"# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
        "\n",
        "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output)\n",
        "\n",
        "        print(f\"\\n✅ Saved enhanced RDF to {OUTPUT_RAG_TTL}\")\n",
        "        print(f\"📊 Processing Statistics:\")\n",
        "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
        "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
        "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
        "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
        "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
        "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
        "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
        "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
        "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
        "        print(f\"   - RAG queries for RDF generation: {multi_kg_system.stats['rag_queries']}\")\n",
        "\n",
        "        print(f\"\\n🔗 Knowledge Graph Connector Statistics:\")\n",
        "        for name, connector in multi_kg_system.connectors.items():\n",
        "            stats = connector.get_stats()\n",
        "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
        "\n",
        "        if multi_kg_system.location_extractor.location_cache:\n",
        "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
        "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
        "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
        "\n",
        "        print(f\"\\n📝 Sample of generated RDF:\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No events were extracted from any chunks\")\n",
        "\n",
        "    # START RAG SESSION HERE!\n",
        "    if vectorstore:\n",
        "        print(f\"\\n🤖 RAG System Ready!\")\n",
        "\n",
        "        # Show example queries\n",
        "        print(f\"\\n💡 Try asking questions like:\")\n",
        "        print(f\"   - 'What battles happened in Sicily?'\")\n",
        "        print(f\"   - 'Who were the main leaders mentioned?'\")\n",
        "        print(f\"   - 'What events occurred in 415 BC?'\")\n",
        "        print(f\"   - 'Describe the naval engagements'\")\n",
        "        print(f\"   - 'What was the outcome of the siege?'\")\n",
        "\n",
        "        # Ask if user wants to start interactive session\n",
        "        response = input(f\"\\n❓ Start interactive RAG session? (y/n): \").strip()\n",
        "\n",
        "        # Check if user typed a question instead of y/n\n",
        "        if response.lower() not in ['y', 'yes', 'n', 'no', '']:\n",
        "            # User typed a question directly!\n",
        "            print(f\"\\n🔍 Processing your question: '{response}'\")\n",
        "            result = multi_kg_system.rag_query(response, llm)\n",
        "\n",
        "            if \"error\" not in result:\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "            else:\n",
        "                print(f\"❌ Error: {result['error']}\")\n",
        "\n",
        "            # Ask if they want to continue with interactive session\n",
        "            continue_response = input(f\"\\n❓ Continue with interactive RAG session? (y/n): \").strip().lower()\n",
        "            if continue_response in ['y', 'yes', '']:\n",
        "                multi_kg_system.interactive_rag_session(llm)\n",
        "\n",
        "        elif response.lower() in ['y', 'yes', '']:\n",
        "            multi_kg_system.interactive_rag_session(llm)\n",
        "        else:\n",
        "            print(f\"\\n💡 You can also query programmatically:\")\n",
        "            print(f\"   result = multi_kg_system.rag_query('your question', llm)\")\n",
        "\n",
        "            # Offer a few sample queries\n",
        "            sample_queries = [\n",
        "                \"What are the main events mentioned in the text?\",\n",
        "                \"Which locations are mentioned?\",\n",
        "                \"Who are the key people involved?\"\n",
        "            ]\n",
        "\n",
        "            print(f\"\\n🔍 Running sample queries:\")\n",
        "            for query in sample_queries:\n",
        "                print(f\"\\n❓ Sample query: '{query}'\")\n",
        "                result = multi_kg_system.rag_query(query, llm)\n",
        "                if \"error\" not in result:\n",
        "                    print(f\"📝 Answer: {result['answer'][:200]}...\")\n",
        "                    print(f\"📊 Sources: {len(result['retrieved_chunks'])} chunks, {result['kg_facts_count']} KG facts\")\n",
        "                else:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️  Could not create vector store for RAG functionality\")\n",
        "\n",
        "    print(f\"\\n🎉 Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6yKw4cBga8O",
        "outputId": "d1b83cb0-96f4-49b7-ccc4-614b02a5f2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2+cpu)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.11/dist-packages (7.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "\n",
            "Verifying Langchain Ollama import...\n",
            "✅ ChatOllama successfully imported after reinstallation.\n",
            "🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\n",
            "✅ Using local Ollama - no API key needed.\n",
            "Loaded text from /content/drive/MyDrive/part_aa\n",
            "📄 Using YOUR text from /content/drive/MyDrive/part_aa\n",
            "📝 Text length: 398568 characters\n",
            "✅ Llama LLM initialized successfully.\n",
            "\n",
            "📚 Setting up RAG vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Vector store created with 1980 text segments\n",
            "🔢 Total tokens in text: 99,642\n",
            "📊 Text is large, chunking into smaller pieces...\n",
            "📄 Created 10 chunks\n",
            "\n",
            "🔄 Processing chunks for event extraction...\n",
            "\n",
            "🔄 Processing chunk 1/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:Wikidata returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:DBpedia returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:Wikidata returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:DBpedia returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:Wikidata returned status 400 for English\n",
            "Character\n",
            "WARNING:__main__:DBpedia returned status 400 for English\n",
            "Character\n",
            "WARNING:__main__:⏰ Timeout: 30/51 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 2/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 12/35 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 3/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:Wikidata returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:DBpedia returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:⏰ Timeout: 12/34 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 4/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 11/35 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 5/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 12/33 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 6/10...\n",
            "\n",
            "🔄 Processing chunk 7/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VII\n",
            "Second Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VII\n",
            "Second Year\n",
            "WARNING:__main__:⏰ Timeout: 1/24 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 8/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:Wikidata returned status 400 for Sitalces\n",
            "\n",
            "The\n",
            "WARNING:__main__:DBpedia returned status 400 for Sitalces\n",
            "\n",
            "The\n",
            "WARNING:__main__:⏰ Timeout: 6/31 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 9/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 24/44 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 10/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 21/38 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Saved enhanced RDF to /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl\n",
            "📊 Processing Statistics:\n",
            "   - Total chunks processed: 10\n",
            "   - Successful chunks: 10\n",
            "   - Unique entities found: 183\n",
            "   - Total KG facts retrieved: 528\n",
            "   - Cache hits: 402\n",
            "   - Locations found: 61\n",
            "   - Locations with coordinates: 61\n",
            "   - Location duplicates avoided: 34\n",
            "   - Unique global locations: 27\n",
            "   - RAG queries for RDF generation: 10\n",
            "\n",
            "🔗 Knowledge Graph Connector Statistics:\n",
            "   - Wikidata: 70/80 requests (87.5% success)\n",
            "   - DBpedia: 63/73 requests (86.3% success)\n",
            "   - ConceptNet: 15/91 requests (16.5% success)\n",
            "   - Location enrichment: 69/228 locations enriched (30.3% success)\n",
            "\n",
            "📝 Sample of generated RDF:\n",
            "============================================================\n",
            "@prefix ste: <http://www.example.org/ste#> .\n",
            "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
            "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
            "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
            "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
            "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
            "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
            "\n",
            "# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\n",
            "After analyzing the provided text chunk, I extracted the following events:\n",
            "\n",
            "\t* ste:hasType \"battle\" ;\n",
            "\t* ste:hasAgent \"the Athenians\" ;\n",
            "\t* ste:hasTime \"480 BCE\" ;\n",
            "\t* ste:hasLocation \"Plataea\" ;\n",
            "\t* ste:hasLatitude \"37.9833\"^^xsd:double ;\n",
            "\t* ste:hasLongitude \"22.4167\"^^xsd:double ;\n",
            "\t* ste:hasCountry \"Ancient Greece\" ;\n",
            "\t* ste:hasRegion \"Boeotia\" ;\n",
            "\t* ste:hasLocationSource \"wikidata\" ;\n",
            "\t* ste:hasResult \"Persian defeat\" ;\n",
            "\t* ste:hasRAGContext \"yes\" .\n",
            "\t* ste:hasType \"siege\" ;\n",
            "\t* ste:hasAgent \"the Persians\" ;\n",
            "\t* ste:hasTime \"480 BCE\" ;\n",
            "\t* ste:hasLocation \"Athens\" ;\n",
            "\t...\n",
            "============================================================\n",
            "\n",
            "🤖 RAG System Ready!\n",
            "\n",
            "💡 Try asking questions like:\n",
            "   - 'What battles happened in Sicily?'\n",
            "   - 'Who were the main leaders mentioned?'\n",
            "   - 'What events occurred in 415 BC?'\n",
            "   - 'Describe the naval engagements'\n",
            "   - 'What was the outcome of the siege?'\n",
            "\n",
            "❓ Start interactive RAG session? (y/n): n\n",
            "\n",
            "💡 You can also query programmatically:\n",
            "   result = multi_kg_system.rag_query('your question', llm)\n",
            "\n",
            "🔍 Running sample queries:\n",
            "\n",
            "❓ Sample query: 'What are the main events mentioned in the text?'\n",
            "📝 Answer: Based on the retrieved text chunks and knowledge graph context, I can identify several main events mentioned in the text:\n",
            "\n",
            "1. **The Second Congress at Lacedaemon**: This event is mentioned as a precur...\n",
            "📊 Sources: 3 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Which locations are mentioned?'\n",
            "📝 Answer: Based on the provided context, I can identify several locations mentioned in the retrieved text chunks:\n",
            "\n",
            "1. The Thracian towns: Although not specific locations are mentioned, it can be inferred that t...\n",
            "📊 Sources: 3 chunks, 12 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Who are the key people involved?'\n",
            "📝 Answer: Based on the provided context, it appears that the question \"Who are the key people involved?\" is related to a speech or oration being delivered, possibly in an ancient Greek setting. The text chunks ...\n",
            "📊 Sources: 3 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "🎉 Process complete! Check /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl for RDF results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 1: Mount Drive and Install Dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Dependency Compatibility Guard ---\n",
        "import os\n",
        "\n",
        "# Force compatible versions for transformers and sentence-transformers\n",
        "os.system(\"pip install transformers==4.40.2 sentence-transformers==2.4.0 --quiet\")\n",
        "\n",
        "# (Optional) If you use torch, ensure it's compatible too:\n",
        "# os.system(\"pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu --quiet\")\n",
        "\n",
        "# Now safe to import\n",
        "\n",
        "!pip install  torch accelerate rdflib\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Enhanced Multi-Knowledge Graph RAG System with Text Chunking - LLAMA VERSION\n",
        "Handles large texts by processing them in chunks to avoid token limits\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import requests\n",
        "\n",
        "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal # <--- This line is crucial\n",
        "from rdflib.namespace import XSD, SKOS\n",
        "# Add a direct import test here to diagnose immediately\n",
        "print(\"\\nVerifying Langchain Ollama import...\")\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "    print(\"✅ ChatOllama successfully imported after reinstallation.\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ CRITICAL ERROR: ChatOllama still cannot be imported after reinstallation: {e}\")\n",
        "    print(\"Please check your pip installation output for errors and ensure your Colab runtime is healthy.\")\n",
        "    # You might want to exit here if this is a persistent issue\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"❌ An unexpected error occurred during ChatOllama import verification: {e}\")\n",
        "    # import sys\n",
        "    # sys.exit(1)\n",
        "\n",
        "\n",
        "# Configuration\n",
        "INPUT_TEXT_FILE = \"/content/drive/MyDrive/part_aa\"\n",
        "ONTOLOGY_PATH = \"/content/drive/MyDrive/wiki.owl\"\n",
        "LOCATION_ONTOLOGY_PATH = \"/content/drive/MyDrive/locations.owl\"\n",
        "OUTPUT_RAG_TTL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl'\n",
        "OUTPUT_RAG_OWL = '/content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.owl'\n",
        "KG_CACHE_FILE = '/content/drive/MyDrive/kg_cache.json'\n",
        "LOCATION_CACHE_FILE = '/content/drive/MyDrive/location_cache.json'\n",
        "KG_ANALYSIS_REPORT = '/content/drive/MyDrive/multi_kg_analysis_report.txt'\n",
        "\n",
        "# Token limits - adjusted for Llama\n",
        "MAX_TOKENS_PER_REQUEST = 50000  # Conservative limit for Llama\n",
        "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
        "\n",
        "# Namespaces\n",
        "EX = Namespace(\"http://example.org/\")\n",
        "STE = Namespace(\"http://www.example.org/ste#\")\n",
        "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
        "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
        "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
        "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
        "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
        "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Imports\n",
        "try:\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_ollama import ChatOllama  # Changed from OpenAI to Ollama\n",
        "    from langchain.schema import HumanMessage\n",
        "except ImportError as e:\n",
        "    print(f\"ImportError: {e}\")\n",
        "    print(\"pip install rdflib python-dotenv langchain langchain-ollama langchain-community faiss-cpu sentence-transformers requests\")\n",
        "    exit(1)\n",
        "\n",
        "@dataclass\n",
        "class LocationInfo:\n",
        "    \"\"\"Location information with coordinates\"\"\"\n",
        "    name: str\n",
        "    latitude: Optional[float] = None\n",
        "    longitude: Optional[float] = None\n",
        "    country: Optional[str] = None\n",
        "    region: Optional[str] = None\n",
        "    source: str = \"extracted\"\n",
        "    confidence: float = 1.0\n",
        "    uri: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class EnhancedKnowledgeFact:\n",
        "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
        "    subject: str\n",
        "    predicate: str\n",
        "    object: str\n",
        "    source: str\n",
        "    confidence: float = 1.0\n",
        "    context: Optional[str] = None\n",
        "    temporal: Optional[str] = None\n",
        "    spatial: Optional[str] = None\n",
        "    evidence_score: float = 1.0\n",
        "    source_uri: Optional[str] = None\n",
        "\n",
        "class LocationExtractor:\n",
        "    \"\"\"Extracts and enriches location information\"\"\"\n",
        "\n",
        "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
        "        self.ontology_path = ontology_path\n",
        "        self.location_graph = None\n",
        "        self.location_cache = self._load_location_cache()\n",
        "        self.load_location_ontology()\n",
        "\n",
        "    def _load_location_cache(self) -> Dict:\n",
        "        \"\"\"Load location cache\"\"\"\n",
        "        if os.path.exists(LOCATION_CACHE_FILE):\n",
        "            try:\n",
        "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load location cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_location_cache(self):\n",
        "        \"\"\"Save location cache\"\"\"\n",
        "        try:\n",
        "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save location cache: {e}\")\n",
        "\n",
        "    def load_location_ontology(self):\n",
        "        \"\"\"Load locations.owl ontology\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.ontology_path):\n",
        "                self.location_graph = Graph()\n",
        "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
        "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
        "                self.location_graph = None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading location ontology: {e}\")\n",
        "            self.location_graph = None\n",
        "\n",
        "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract potential location names from text\"\"\"\n",
        "        location_patterns = [\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
        "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
        "        ]\n",
        "\n",
        "        locations = []\n",
        "        for pattern in location_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            locations.extend(matches)\n",
        "\n",
        "        location_stopwords = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If',\n",
        "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
        "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday',\n",
        "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
        "        }\n",
        "\n",
        "        filtered_locations = []\n",
        "        for loc in locations:\n",
        "            loc = loc.strip()\n",
        "            if (loc not in location_stopwords and len(loc) > 2 and\n",
        "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
        "                filtered_locations.append(loc)\n",
        "\n",
        "        return list(set(filtered_locations))\n",
        "\n",
        "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location info from local ontology\"\"\"\n",
        "        if not self.location_graph:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            query = f\"\"\"\n",
        "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
        "                ?location rdfs:label ?label .\n",
        "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
        "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
        "                OPTIONAL {{ ?location geo:long ?long }}\n",
        "                OPTIONAL {{ ?location dbp:country ?country }}\n",
        "                OPTIONAL {{ ?location dbp:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            results = self.location_graph.query(query)\n",
        "            for row in results:\n",
        "                return LocationInfo(\n",
        "                    name=location_name,\n",
        "                    latitude=float(row.lat) if row.lat else None,\n",
        "                    longitude=float(row.long) if row.long else None,\n",
        "                    country=str(row.country) if row.country else None,\n",
        "                    region=str(row.region) if row.region else None,\n",
        "                    source=\"local_ontology\",\n",
        "                    uri=str(row.location) if row.location else None\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
        "                <{entity_uri}> geo:lat ?lat ;\n",
        "                               geo:long ?long .\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                if bindings:\n",
        "                    binding = bindings[0]\n",
        "                    return LocationInfo(\n",
        "                        name=location_name,\n",
        "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
        "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
        "                        country=binding.get('country', {}).get('value', ''),\n",
        "                        region=binding.get('region', {}).get('value', ''),\n",
        "                        source=\"dbpedia\",\n",
        "                        uri=entity_uri\n",
        "                    )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            # Try multiple query strategies to get the right location\n",
        "            queries = [\n",
        "                # Try exact label match first\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\",\n",
        "                # Try with additional filters for places/locations\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\"\n",
        "            ]\n",
        "\n",
        "            for query in queries:\n",
        "                params = {'query': query, 'format': 'json'}\n",
        "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                    if bindings:\n",
        "                        # Prefer results with country information\n",
        "                        best_binding = None\n",
        "                        for binding in bindings:\n",
        "                            if binding.get('country'):\n",
        "                                best_binding = binding\n",
        "                                break\n",
        "\n",
        "                        if not best_binding:\n",
        "                            best_binding = bindings[0]\n",
        "\n",
        "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
        "\n",
        "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
        "                        if coord_match:\n",
        "                            longitude = float(coord_match.group(1))\n",
        "                            latitude = float(coord_match.group(2))\n",
        "\n",
        "                            return LocationInfo(\n",
        "                                name=location_name,\n",
        "                                latitude=latitude,\n",
        "                                longitude=longitude,\n",
        "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
        "                                source=\"wikidata\",\n",
        "                                uri=best_binding.get('item', {}).get('value', '')\n",
        "                            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
        "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
        "        if not location_info.latitude or not location_info.longitude:\n",
        "            return True\n",
        "\n",
        "        lat, lon = location_info.latitude, location_info.longitude\n",
        "\n",
        "        # Basic coordinate range validation\n",
        "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
        "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
        "        if location_name in self.location_cache:\n",
        "            cached = self.location_cache[location_name]\n",
        "            return LocationInfo(**cached) if cached else None\n",
        "\n",
        "        location_info = None\n",
        "\n",
        "        location_info = self.get_location_from_ontology(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_wikidata(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_dbpedia(location_name)\n",
        "\n",
        "        if location_info:\n",
        "            self.location_cache[location_name] = {\n",
        "                'name': location_info.name,\n",
        "                'latitude': location_info.latitude,\n",
        "                'longitude': location_info.longitude,\n",
        "                'country': location_info.country,\n",
        "                'region': location_info.region,\n",
        "                'source': location_info.source,\n",
        "                'confidence': location_info.confidence,\n",
        "                'uri': location_info.uri\n",
        "            }\n",
        "        else:\n",
        "            self.location_cache[location_name] = None\n",
        "\n",
        "        self._save_location_cache()\n",
        "\n",
        "        if location_info:\n",
        "            self.validate_coordinates(location_info)\n",
        "\n",
        "        return location_info\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Handles text chunking to manage token limits for Llama\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"llama3.2\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Approximate token count for Llama (roughly 4 chars per token)\"\"\"\n",
        "        return len(text) // 4\n",
        "\n",
        "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 10000) -> List[str]:\n",
        "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class BaseKGConnector:\n",
        "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
        "        self.name = name\n",
        "        self.base_url = base_url\n",
        "        self.rate_limit = rate_limit\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "        self.success_count = 0\n",
        "\n",
        "    def _rate_limit_wait(self):\n",
        "        \"\"\"Enforce rate limiting\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "        if time_since_last < self.rate_limit:\n",
        "            time.sleep(self.rate_limit - time_since_last)\n",
        "        self.last_request_time = time.time()\n",
        "        self.request_count += 1\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get connector statistics\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'requests': self.request_count,\n",
        "            'successes': self.success_count,\n",
        "            'success_rate': self.success_count / max(1, self.request_count)\n",
        "        }\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EnhancedWikidataConnector(BaseKGConnector):\n",
        "    \"\"\"Wikidata connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
        "              {{\n",
        "                ?subject ?label \"{entity}\"@en .\n",
        "              }} UNION {{\n",
        "                ?subject rdfs:label \"{entity}\"@en .\n",
        "              }}\n",
        "\n",
        "              ?subject ?predicate ?object .\n",
        "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
        "\n",
        "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
        "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
        "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
        "                        source=self.name,\n",
        "                        confidence=0.9,\n",
        "                        source_uri=binding.get('subject', {}).get('value')\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedDBpediaConnector(BaseKGConnector):\n",
        "    \"\"\"DBpedia connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
        "              <{entity_uri}> ?predicate ?object .\n",
        "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
        "              FILTER(!isBlank(?object))\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)  # Reduced timeout\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    predicate = binding.get('predicate', {}).get('value', '')\n",
        "                    obj = binding.get('object', {}).get('value', '')\n",
        "\n",
        "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
        "\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=entity,\n",
        "                        predicate=predicate_name,\n",
        "                        object=obj,\n",
        "                        source=self.name,\n",
        "                        confidence=0.85,\n",
        "                        source_uri=entity_uri\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedConceptNetConnector(BaseKGConnector):\n",
        "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
        "\n",
        "    def search_related_concepts(self, entity: str) -> List[str]:\n",
        "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
        "        try:\n",
        "            # Try search API first\n",
        "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
        "            response = requests.get(search_url, timeout=10)\n",
        "\n",
        "            related_concepts = []\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for edge in data.get('edges', []):\n",
        "                    start = edge.get('start', {}).get('label', '')\n",
        "                    end = edge.get('end', {}).get('label', '')\n",
        "\n",
        "                    # Extract concept paths and clean them\n",
        "                    for concept_path in [start, end]:\n",
        "                        if concept_path and '/c/en/' in concept_path:\n",
        "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
        "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
        "                                related_concepts.append(concept)\n",
        "\n",
        "            return list(set(related_concepts))[:5]  # Return top 5 unique concepts\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
        "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
        "        try:\n",
        "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
        "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return data.get('edges', [])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "            all_facts = []\n",
        "\n",
        "            # Strategy 1: Try direct query first\n",
        "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
        "\n",
        "            # Strategy 2: Search for related concepts and query them\n",
        "            related_concepts = self.search_related_concepts(entity)\n",
        "\n",
        "            # Process direct edges\n",
        "            for edge in direct_edges:\n",
        "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
        "                if fact:\n",
        "                    all_facts.append(fact)\n",
        "\n",
        "            # Process related concept edges\n",
        "            for concept in related_concepts:\n",
        "                concept_edges = self.query_concept_directly(concept, 5)\n",
        "                for edge in concept_edges:\n",
        "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
        "                    if fact:\n",
        "                        all_facts.append(fact)\n",
        "\n",
        "            if all_facts:\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
        "                if related_concepts:\n",
        "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
        "\n",
        "            return all_facts[:limit]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
        "        try:\n",
        "            start = edge.get('start', {})\n",
        "            end = edge.get('end', {})\n",
        "            relation = edge.get('rel', {})\n",
        "            weight = edge.get('weight', 1.0)\n",
        "\n",
        "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            rel_label = relation.get('label', 'related_to')\n",
        "\n",
        "            # Skip if labels are empty or too short\n",
        "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
        "                return None\n",
        "\n",
        "            # Determine confidence based on discovery method\n",
        "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
        "\n",
        "            return EnhancedKnowledgeFact(\n",
        "                subject=original_entity,\n",
        "                predicate=rel_label,\n",
        "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
        "                source=self.name,\n",
        "                confidence=min(weight * confidence_multiplier, 1.0),\n",
        "                context=f\"Discovered {discovery_method}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
        "            return None\n",
        "\n",
        "class MultiKGCache:\n",
        "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
        "\n",
        "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
        "        self.cache_file = cache_file\n",
        "        self.cache = self._load_cache()\n",
        "\n",
        "    def _load_cache(self) -> Dict:\n",
        "        \"\"\"Load cache from file\"\"\"\n",
        "        if os.path.exists(self.cache_file):\n",
        "            try:\n",
        "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_cache(self):\n",
        "        \"\"\"Save cache to file\"\"\"\n",
        "        try:\n",
        "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save cache: {e}\")\n",
        "\n",
        "    def get_cache_key(self, source: str, entity: str) -> str:\n",
        "        \"\"\"Generate cache key\"\"\"\n",
        "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
        "\n",
        "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Get cached facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        return self.cache.get(key)\n",
        "\n",
        "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
        "        \"\"\"Cache facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        serializable_facts = []\n",
        "        for fact in facts:\n",
        "            serializable_facts.append({\n",
        "                'subject': fact.subject,\n",
        "                'predicate': fact.predicate,\n",
        "                'object': fact.object,\n",
        "                'source': fact.source,\n",
        "                'confidence': fact.confidence,\n",
        "                'context': fact.context,\n",
        "                'temporal': fact.temporal,\n",
        "                'spatial': fact.spatial,\n",
        "                'evidence_score': fact.evidence_score,\n",
        "                'source_uri': fact.source_uri\n",
        "            })\n",
        "        self.cache[key] = serializable_facts\n",
        "        self._save_cache()\n",
        "\n",
        "class EnhancedMultiKGRAGSystem:\n",
        "    \"\"\"Multi-Knowledge Graph RAG system with chunking and location extraction - LLAMA VERSION\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.connectors = {\n",
        "            'wikidata': EnhancedWikidataConnector(),\n",
        "            'dbpedia': EnhancedDBpediaConnector(),\n",
        "            'conceptnet': EnhancedConceptNetConnector()\n",
        "        }\n",
        "        self.cache = MultiKGCache()\n",
        "        self.chunker = TextChunker()\n",
        "        self.location_extractor = LocationExtractor()\n",
        "        self.global_locations = {}\n",
        "        self.vectorstore = None  # For RAG retrieval\n",
        "        self.document_chunks = []  # Store processed chunks for RAG\n",
        "        self.stats = {\n",
        "            'queries_processed': 0,\n",
        "            'entities_extracted': 0,\n",
        "            'facts_retrieved': 0,\n",
        "            'cache_hits': 0,\n",
        "            'chunks_processed': 0,\n",
        "            'locations_found': 0,\n",
        "            'locations_with_coordinates': 0,\n",
        "            'location_duplicates_avoided': 0,\n",
        "            'rag_queries': 0\n",
        "        }\n",
        "\n",
        "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract entities from text\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
        "        matches = re.findall(pattern, text)\n",
        "        entities.extend(matches)\n",
        "\n",
        "        stop_words = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
        "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
        "        }\n",
        "\n",
        "        filtered_entities = []\n",
        "        for entity in entities:\n",
        "            entity = entity.strip()\n",
        "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
        "                filtered_entities.append(entity)\n",
        "\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for entity in filtered_entities:\n",
        "            if entity.lower() not in seen:\n",
        "                seen.add(entity.lower())\n",
        "                unique_entities.append(entity)\n",
        "\n",
        "        return unique_entities[:40]\n",
        "\n",
        "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
        "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
        "        all_facts = {}\n",
        "        cache_hits = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            for entity in entities:\n",
        "                for source_name, connector in self.connectors.items():\n",
        "                    # Check cache first\n",
        "                    cached_facts = self.cache.get(source_name, entity)\n",
        "                    if cached_facts:\n",
        "                        cache_hits += 1\n",
        "                        if entity not in all_facts:\n",
        "                            all_facts[entity] = []\n",
        "                        for fact_data in cached_facts:\n",
        "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
        "                            all_facts[entity].append(fact)\n",
        "                    else:\n",
        "                        future = executor.submit(connector.retrieve_facts, entity, 15)\n",
        "                        futures[future] = (entity, source_name)\n",
        "\n",
        "            # Collect results with better timeout handling\n",
        "            completed = 0\n",
        "            total_futures = len(futures)\n",
        "\n",
        "            try:\n",
        "                for future in as_completed(futures, timeout=45):  # Increased timeout\n",
        "                    entity, source_name = futures[future]\n",
        "                    completed += 1\n",
        "\n",
        "                    try:\n",
        "                        facts = future.result(timeout=5)  # Individual future timeout\n",
        "                        if facts:\n",
        "                            self.cache.set(source_name, entity, facts)\n",
        "\n",
        "                            if entity not in all_facts:\n",
        "                                all_facts[entity] = []\n",
        "                            all_facts[entity].extend(facts)\n",
        "\n",
        "                            self.stats['facts_retrieved'] += len(facts)\n",
        "\n",
        "                        logger.debug(f\"✅ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"❌ {source_name} failed for {entity}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except TimeoutError:\n",
        "                pending_count = total_futures - completed\n",
        "                logger.warning(f\"⏰ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
        "\n",
        "                # Cancel remaining futures\n",
        "                for future in futures:\n",
        "                    if not future.done():\n",
        "                        future.cancel()\n",
        "\n",
        "        self.stats['cache_hits'] += cache_hits\n",
        "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
        "        return all_facts\n",
        "\n",
        "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
        "        \"\"\"Format KG facts into context string\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        for entity, facts in kg_facts.items():\n",
        "            if facts:\n",
        "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
        "\n",
        "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
        "\n",
        "                by_source = {}\n",
        "                for fact in sorted_facts[:12]:\n",
        "                    if fact.source not in by_source:\n",
        "                        by_source[fact.source] = []\n",
        "                    by_source[fact.source].append(fact)\n",
        "\n",
        "                for source, source_facts in by_source.items():\n",
        "                    context_parts.append(f\"\\nFrom {source}:\")\n",
        "                    for fact in source_facts[:8]:\n",
        "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
        "                        if fact.confidence < 0.8:\n",
        "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
        "                        context_parts.append(fact_str)\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
        "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
        "        location_key = location_info.name.lower().strip()\n",
        "\n",
        "        if location_key in self.global_locations:\n",
        "            existing = self.global_locations[location_key]\n",
        "            if (location_info.latitude and location_info.longitude and\n",
        "                (not existing.latitude or not existing.longitude)):\n",
        "                self.global_locations[location_key] = location_info\n",
        "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
        "            else:\n",
        "                self.stats['location_duplicates_avoided'] += 1\n",
        "                logger.debug(f\"Location {location_info.name} already registered\")\n",
        "        else:\n",
        "            self.global_locations[location_key] = location_info\n",
        "            logger.info(f\"Registered new location: {location_info.name}\")\n",
        "\n",
        "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "        return f\"ste:Location_{clean_name}\"\n",
        "\n",
        "    def process_chunk(self, chunk: str, chunk_num: int, llm) -> str:\n",
        "        \"\"\"Process a single chunk of text with RAG-enhanced location extraction\"\"\"\n",
        "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars)\")\n",
        "\n",
        "        # 1. RAG RETRIEVAL: Get relevant context from vector store\n",
        "        relevant_context = \"\"\n",
        "        if self.vectorstore:\n",
        "            try:\n",
        "                # Use the chunk as a query to retrieve similar/relevant text\n",
        "                relevant_docs = self.vectorstore.similarity_search(chunk, k=25)\n",
        "                retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "                relevant_context = \"\\n---\\n\".join(retrieved_chunks)\n",
        "                logger.info(f\"Retrieved {len(retrieved_chunks)} relevant chunks via RAG for chunk {chunk_num}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"RAG retrieval failed for chunk {chunk_num}: {e}\")\n",
        "                relevant_context = \"\"\n",
        "\n",
        "        # 2. Extract entities and locations\n",
        "        entities = self.extract_entities_advanced(chunk)\n",
        "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
        "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
        "\n",
        "        # 3. Enrich locations with coordinates\n",
        "        enriched_locations = {}\n",
        "        for location_name in locations[:25]:  # Limit to 5 to avoid overwhelming\n",
        "            location_info = self.location_extractor.enrich_location(location_name)\n",
        "            if location_info:\n",
        "                self.register_global_location(location_info)\n",
        "                enriched_locations[location_name] = location_info\n",
        "                self.stats['locations_found'] += 1\n",
        "                if location_info.latitude and location_info.longitude:\n",
        "                    self.stats['locations_with_coordinates'] += 1\n",
        "\n",
        "        if not entities and not enriched_locations:\n",
        "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
        "            return \"\"\n",
        "\n",
        "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
        "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
        "\n",
        "        # 4. Get KG facts for entities\n",
        "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
        "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "        location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "        # 5. RAG-ENHANCED PROMPT: Use retrieved context + KG facts + locations\n",
        "        enhanced_prompt = f\"\"\"You are extracting historical events using RAG (Retrieval-Augmented Generation). Use ALL available context sources to enhance your extraction.\n",
        "\n",
        "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
        "{chunk}\n",
        "\n",
        "RAG RETRIEVED RELEVANT CONTEXT:\n",
        "{relevant_context if relevant_context else \"No relevant context retrieved.\"}\n",
        "\n",
        "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
        "{kg_context}\n",
        "\n",
        "LOCATION INFORMATION WITH COORDINATES:\n",
        "{location_context}\n",
        "\n",
        "TASK: Extract ONLY the events that are actually mentioned in the current text chunk. Use the RAG retrieved context, KG facts, and location coordinates to enhance details but stay faithful to what's actually in the current chunk.\n",
        "\n",
        "Requirements:\n",
        "1. Extract ONLY events mentioned in the CURRENT text chunk (not from retrieved context)\n",
        "2. Use RAG retrieved context to provide additional historical context and validation\n",
        "3. Use KG facts to enhance entity information\n",
        "4. Use location coordinates to provide precise geographical data\n",
        "5. Include ALL these properties for each event:\n",
        "   - ste:hasType (description of event)\n",
        "   - ste:hasAgent (who caused/led the event)\n",
        "   - ste:hasTime (when it happened)\n",
        "   - ste:hasLocation (location name from text)\n",
        "   - ste:hasLatitude (latitude coordinate if available)\n",
        "   - ste:hasLongitude (longitude coordinate if available)\n",
        "   - ste:hasCountry (country if available)\n",
        "   - ste:hasRegion (region if available)\n",
        "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
        "   - ste:hasResult (outcome/consequence)\n",
        "   - ste:hasRAGContext \"yes\" (to indicate this was RAG-enhanced)\n",
        "\n",
        "Output format (do not include prefixes, they will be added later):\n",
        "```turtle\n",
        "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
        "    ste:hasType \"specific description from current chunk\" ;\n",
        "    ste:hasAgent \"specific person from current chunk\" ;\n",
        "    ste:hasTime \"specific date from current chunk\" ;\n",
        "    ste:hasLocation \"specific location from current chunk\" ;\n",
        "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
        "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
        "    ste:hasCountry \"Italy\" ;\n",
        "    ste:hasRegion \"Sicily\" ;\n",
        "    ste:hasLocationSource \"wikidata\" ;\n",
        "    ste:hasResult \"specific outcome from current chunk\" ;\n",
        "    ste:hasRAGContext \"yes\" .\n",
        "```\n",
        "\n",
        "IMPORTANT:\n",
        "- The PRIMARY source is the CURRENT text chunk - extract events from IT\n",
        "- Use RAG retrieved context to validate and enhance your understanding\n",
        "- Use KG facts to enrich entity details\n",
        "- Include precise coordinates from location sources\n",
        "- Mark all events with ste:hasRAGContext \"yes\" to show RAG was used\n",
        "- Only extract events explicitly mentioned in the current chunk\n",
        "- If no clear events are found in current chunk, return empty\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = llm.invoke([HumanMessage(content=enhanced_prompt)])\n",
        "            turtle_output = self.clean_turtle(response.content)\n",
        "            self.stats['chunks_processed'] += 1\n",
        "            self.stats['rag_queries'] += 1  # Count as RAG usage\n",
        "            logger.info(f\"Generated RAG-enhanced RDF for chunk {chunk_num}\")\n",
        "            return turtle_output\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing chunk {chunk_num} with RAG: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
        "        \"\"\"Format location information into context string\"\"\"\n",
        "        if not enriched_locations:\n",
        "            return \"No location coordinates available.\"\n",
        "\n",
        "        context_parts = [\"\\n=== Location Information ===\"]\n",
        "\n",
        "        for location_name, location_info in enriched_locations.items():\n",
        "            context_parts.append(f\"\\n{location_name}:\")\n",
        "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
        "            else:\n",
        "                context_parts.append(\"  - Coordinates: Not available\")\n",
        "\n",
        "            if location_info.country:\n",
        "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
        "\n",
        "            if location_info.region:\n",
        "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
        "\n",
        "            if location_info.uri:\n",
        "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def generate_global_location_rdf(self) -> str:\n",
        "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
        "        if not self.global_locations:\n",
        "            return \"\"\n",
        "\n",
        "        location_rdf_parts = []\n",
        "\n",
        "        for location_key, location_info in self.global_locations.items():\n",
        "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "            location_id = f\"ste:Location_{clean_name}\"\n",
        "\n",
        "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
        "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
        "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
        "\n",
        "            if location_info.country:\n",
        "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
        "\n",
        "            if location_info.region:\n",
        "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
        "\n",
        "            if location_info.source:\n",
        "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
        "\n",
        "            if location_info.uri:\n",
        "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
        "\n",
        "            if rdf_lines[-1].endswith(' ;'):\n",
        "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
        "\n",
        "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
        "\n",
        "        return '\\n\\n'.join(location_rdf_parts)\n",
        "\n",
        "    def clean_turtle(self, raw_output: str) -> str:\n",
        "        \"\"\"Clean turtle output\"\"\"\n",
        "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "\n",
        "        lines = raw_output.strip().split('\\n')\n",
        "        turtle_lines = []\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if (stripped.startswith('@') or stripped.startswith('<') or\n",
        "                stripped.startswith(':') or stripped.startswith('_') or\n",
        "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
        "                turtle_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(turtle_lines)\n",
        "\n",
        "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
        "        \"\"\"Create vector store from text chunks for RAG retrieval\"\"\"\n",
        "        try:\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            from langchain_community.vectorstores import FAISS\n",
        "\n",
        "            embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "            )\n",
        "\n",
        "            # Prepare chunks with metadata\n",
        "            documents = []\n",
        "            metadatas = []\n",
        "\n",
        "            for i, chunk in enumerate(text_chunks):\n",
        "                if len(chunk.strip()) > 50:  # Only include substantial chunks\n",
        "                    documents.append(chunk)\n",
        "                    metadatas.append({\n",
        "                        'chunk_id': i,\n",
        "                        'length': len(chunk),\n",
        "                        'type': 'text_chunk'\n",
        "                    })\n",
        "\n",
        "            if documents:\n",
        "                self.vectorstore = FAISS.from_texts(documents, embeddings, metadatas=metadatas)\n",
        "                self.document_chunks = documents\n",
        "                logger.info(f\"Created vector store with {len(documents)} chunks\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.warning(\"No suitable chunks found for vector store\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating vector store: {e}\")\n",
        "            return False\n",
        "\n",
        "    def rag_query(self, query: str, llm, k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Perform RAG query: retrieve relevant chunks + KG facts, then generate response\"\"\"\n",
        "        self.stats['rag_queries'] += 1\n",
        "        logger.info(f\"Processing RAG query: '{query[:50]}...'\")\n",
        "\n",
        "        if not self.vectorstore:\n",
        "            return {\"error\": \"Vector store not initialized. Call prepare_vectorstore() first.\"}\n",
        "\n",
        "        try:\n",
        "            # 1. RETRIEVE: Get relevant text chunks\n",
        "            relevant_docs = self.vectorstore.similarity_search(query, k=k)\n",
        "            retrieved_chunks = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "            # 2. EXTRACT: Get entities from query\n",
        "            query_entities = self.extract_entities_advanced(query)\n",
        "\n",
        "            # 3. RETRIEVE: Get KG facts for entities\n",
        "            kg_facts = self.retrieve_kg_facts_enhanced(query_entities)\n",
        "            kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "\n",
        "            # 4. EXTRACT & ENRICH: Get locations from query\n",
        "            query_locations = self.location_extractor.extract_locations_from_text(query)\n",
        "            enriched_locations = {}\n",
        "            for location_name in query_locations[:5]:\n",
        "                location_info = self.location_extractor.enrich_location(location_name)\n",
        "                if location_info:\n",
        "                    enriched_locations[location_name] = location_info\n",
        "\n",
        "            location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "            # 5. AUGMENT: Create enhanced context for generation\n",
        "            context_parts = [\n",
        "                f\"QUERY: {query}\",\n",
        "                f\"\\nRETRIEVED RELEVANT TEXT CHUNKS:\",\n",
        "                \"\\n\" + \"\\n---\\n\".join(retrieved_chunks),\n",
        "                f\"\\nKNOWLEDGE GRAPH CONTEXT:\",\n",
        "                kg_context,\n",
        "                f\"\\nLOCATION CONTEXT:\",\n",
        "                location_context\n",
        "            ]\n",
        "\n",
        "            enhanced_context = \"\\n\".join(context_parts)\n",
        "\n",
        "            # 6. GENERATE: Create comprehensive response\n",
        "            rag_prompt = f\"\"\"You are an expert historian with access to multiple knowledge sources. Answer the question using the provided context.\n",
        "\n",
        "{enhanced_context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Answer the question comprehensively using ALL available context\n",
        "2. Cite specific information from the retrieved text chunks\n",
        "3. Incorporate relevant knowledge graph facts to enhance your answer\n",
        "4. Include precise location information and coordinates when relevant\n",
        "5. If information conflicts between sources, mention this\n",
        "6. Be specific about dates, places, people, and events\n",
        "7. If you cannot answer completely, explain what information is missing\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "            # 7. GENERATE: Get LLM response\n",
        "            response = llm.invoke([HumanMessage(content=rag_prompt)])\n",
        "\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"answer\": response.content,\n",
        "                \"retrieved_chunks\": retrieved_chunks,\n",
        "                \"entities_found\": query_entities,\n",
        "                \"kg_facts_count\": sum(len(facts) for facts in kg_facts.values()),\n",
        "                \"locations_found\": list(enriched_locations.keys()),\n",
        "                \"sources\": {\n",
        "                    \"text_chunks\": len(retrieved_chunks),\n",
        "                    \"kg_sources\": list(set(fact.source for facts in kg_facts.values() for fact in facts)),\n",
        "                    \"location_sources\": list(set(loc.source for loc in enriched_locations.values()))\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"RAG query failed: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def interactive_rag_session(self, llm):\n",
        "        \"\"\"Start an interactive RAG session\"\"\"\n",
        "        print(\"\\n🤖 Starting Interactive RAG Session\")\n",
        "        print(\"Ask questions about your text. Type 'quit' to exit.\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                query = input(\"\\n❓ Your question: \").strip()\n",
        "\n",
        "                if query.lower() in ['quit', 'exit', 'q']:\n",
        "                    print(\"👋 Goodbye!\")\n",
        "                    break\n",
        "\n",
        "                if not query:\n",
        "                    continue\n",
        "\n",
        "                print(\"\\n🔍 Processing your question...\")\n",
        "                result = self.rag_query(query, llm)\n",
        "\n",
        "                if \"error\" in result:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n👋 Goodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "                continue\n",
        "\n",
        "# Utility functions\n",
        "def load_api_key():\n",
        "    \"\"\"Llama via Ollama doesn't need an API key\"\"\"\n",
        "    print(\"✅ Using local Ollama - no API key needed.\")\n",
        "    return \"local\"\n",
        "\n",
        "def load_text_from_file(filepath: str) -> str:\n",
        "    \"\"\"Load text from file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        print(f\"Loaded text from {filepath}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def initialize_llm(api_key: str):\n",
        "    \"\"\"Initialize Llama LLM\"\"\"\n",
        "    try:\n",
        "        llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
        "        print(\"✅ Llama LLM initialized successfully.\")\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing LLM: {e}\")\n",
        "        print(\"💡 Make sure Ollama is running: ollama serve\")\n",
        "        print(\"💡 And pull the model: ollama pull llama3.2\")\n",
        "        return None\n",
        "\n",
        "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
        "    \"\"\"Create vector store from text\"\"\"\n",
        "    try:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "        )\n",
        "\n",
        "        # Split text into sentences for better retrieval\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        texts = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 20]\n",
        "\n",
        "        if not texts:\n",
        "            return None\n",
        "\n",
        "        vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "        multi_kg_system.vectorstore = vectorstore\n",
        "        multi_kg_system.document_chunks = texts\n",
        "        print(f\"📚 Vector store created with {len(texts)} text segments\")\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with chunking support and RAG capabilities - LLAMA VERSION\"\"\"\n",
        "    print(\"🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\")\n",
        "\n",
        "    api_key = load_api_key()\n",
        "    if not api_key:\n",
        "        return\n",
        "\n",
        "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
        "    if not domain_text:\n",
        "        print(\"⚠️  No input file found, using sample text\")\n",
        "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC.\n",
        "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes.\n",
        "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
        "    else:\n",
        "        print(f\"📄 Using YOUR text from {INPUT_TEXT_FILE}\")\n",
        "        print(f\"📝 Text length: {len(domain_text)} characters\")\n",
        "\n",
        "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
        "    llm = initialize_llm(api_key)\n",
        "\n",
        "    if not llm:\n",
        "        return\n",
        "\n",
        "    # Prepare vector store for RAG FIRST\n",
        "    print(\"\\n📚 Setting up RAG vector store...\")\n",
        "    vectorstore = prepare_vectorstore_from_text(domain_text, multi_kg_system)\n",
        "\n",
        "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
        "    print(f\"🔢 Total tokens in text: {token_count:,}\")\n",
        "\n",
        "    if token_count > 10000:  # Reduced for Llama\n",
        "        print(\"📊 Text is large, chunking into smaller pieces...\")\n",
        "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=10000)\n",
        "        print(f\"📄 Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(\"📄 Text is small enough to process as single chunk\")\n",
        "        chunks = [domain_text]\n",
        "\n",
        "    # Extract events and create RDF\n",
        "    all_turtle_outputs = []\n",
        "    all_entities = set()\n",
        "\n",
        "    print(\"\\n🔄 Processing chunks for event extraction...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"\\n🔄 Processing chunk {i}/{len(chunks)}...\")\n",
        "\n",
        "        turtle_output = multi_kg_system.process_chunk(chunk, i, llm)\n",
        "        if turtle_output:\n",
        "            all_turtle_outputs.append(turtle_output)\n",
        "\n",
        "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
        "        all_entities.update(chunk_entities)\n",
        "\n",
        "        if i < len(chunks):\n",
        "            time.sleep(1)\n",
        "\n",
        "    # Save RDF output\n",
        "    if all_turtle_outputs:\n",
        "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
        "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
        "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
        "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
        "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
        "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
        "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        final_output = prefixes + \"# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
        "\n",
        "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output)\n",
        "\n",
        "        print(f\"\\n✅ Saved enhanced RDF to {OUTPUT_RAG_TTL}\")\n",
        "        print(f\"📊 Processing Statistics:\")\n",
        "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
        "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
        "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
        "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
        "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
        "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
        "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
        "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
        "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
        "        print(f\"   - RAG queries for RDF generation: {multi_kg_system.stats['rag_queries']}\")\n",
        "\n",
        "        print(f\"\\n🔗 Knowledge Graph Connector Statistics:\")\n",
        "        for name, connector in multi_kg_system.connectors.items():\n",
        "            stats = connector.get_stats()\n",
        "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
        "\n",
        "        if multi_kg_system.location_extractor.location_cache:\n",
        "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
        "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
        "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
        "\n",
        "        print(f\"\\n📝 Sample of generated RDF:\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No events were extracted from any chunks\")\n",
        "\n",
        "    # START RAG SESSION HERE!\n",
        "    if vectorstore:\n",
        "        print(f\"\\n🤖 RAG System Ready!\")\n",
        "\n",
        "        # Show example queries\n",
        "        print(f\"\\n💡 Try asking questions like:\")\n",
        "        print(f\"   - 'What battles happened in Sicily?'\")\n",
        "        print(f\"   - 'Who were the main leaders mentioned?'\")\n",
        "        print(f\"   - 'What events occurred in 415 BC?'\")\n",
        "        print(f\"   - 'Describe the naval engagements'\")\n",
        "        print(f\"   - 'What was the outcome of the siege?'\")\n",
        "\n",
        "        # Ask if user wants to start interactive session\n",
        "        response = input(f\"\\n❓ Start interactive RAG session? (y/n): \").strip()\n",
        "\n",
        "        # Check if user typed a question instead of y/n\n",
        "        if response.lower() not in ['y', 'yes', 'n', 'no', '']:\n",
        "            # User typed a question directly!\n",
        "            print(f\"\\n🔍 Processing your question: '{response}'\")\n",
        "            result = multi_kg_system.rag_query(response, llm)\n",
        "\n",
        "            if \"error\" not in result:\n",
        "                print(f\"\\n📝 **Answer:**\")\n",
        "                print(result['answer'])\n",
        "\n",
        "                print(f\"\\n📊 **Sources Used:**\")\n",
        "                print(f\"   - Text chunks: {result['sources']['text_chunks']}\")\n",
        "                print(f\"   - KG sources: {result['sources']['kg_sources']}\")\n",
        "                print(f\"   - Entities: {', '.join(result['entities_found'][:5])}\")\n",
        "                if result['locations_found']:\n",
        "                    print(f\"   - Locations: {', '.join(result['locations_found'])}\")\n",
        "            else:\n",
        "                print(f\"❌ Error: {result['error']}\")\n",
        "\n",
        "            # Ask if they want to continue with interactive session\n",
        "            continue_response = input(f\"\\n❓ Continue with interactive RAG session? (y/n): \").strip().lower()\n",
        "            if continue_response in ['y', 'yes', '']:\n",
        "                multi_kg_system.interactive_rag_session(llm)\n",
        "\n",
        "        elif response.lower() in ['y', 'yes', '']:\n",
        "            multi_kg_system.interactive_rag_session(llm)\n",
        "        else:\n",
        "            print(f\"\\n💡 You can also query programmatically:\")\n",
        "            print(f\"   result = multi_kg_system.rag_query('your question', llm)\")\n",
        "\n",
        "            # Offer a few sample queries\n",
        "            sample_queries = [\n",
        "                \"What are the main events mentioned in the text?\",\n",
        "                \"Which locations are mentioned?\",\n",
        "                \"Who are the key people involved?\"\n",
        "            ]\n",
        "\n",
        "            print(f\"\\n🔍 Running sample queries:\")\n",
        "            for query in sample_queries:\n",
        "                print(f\"\\n❓ Sample query: '{query}'\")\n",
        "                result = multi_kg_system.rag_query(query, llm)\n",
        "                if \"error\" not in result:\n",
        "                    print(f\"📝 Answer: {result['answer'][:200]}...\")\n",
        "                    print(f\"📊 Sources: {len(result['retrieved_chunks'])} chunks, {result['kg_facts_count']} KG facts\")\n",
        "                else:\n",
        "                    print(f\"❌ Error: {result['error']}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️  Could not create vector store for RAG functionality\")\n",
        "\n",
        "    print(f\"\\n🎉 Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY5WPrwTkw5U",
        "outputId": "d56b97aa-c840-4154-a33a-7146db8ba3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2+cpu)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.11/dist-packages (7.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "\n",
            "Verifying Langchain Ollama import...\n",
            "✅ ChatOllama successfully imported after reinstallation.\n",
            "🚀 Starting Multi-Knowledge Graph RAG System with Chunking (LLAMA)\n",
            "✅ Using local Ollama - no API key needed.\n",
            "Loaded text from /content/drive/MyDrive/part_aa\n",
            "📄 Using YOUR text from /content/drive/MyDrive/part_aa\n",
            "📝 Text length: 398568 characters\n",
            "✅ Llama LLM initialized successfully.\n",
            "\n",
            "📚 Setting up RAG vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Vector store created with 1980 text segments\n",
            "🔢 Total tokens in text: 99,642\n",
            "📊 Text is large, chunking into smaller pieces...\n",
            "📄 Created 10 chunks\n",
            "\n",
            "🔄 Processing chunks for event extraction...\n",
            "\n",
            "🔄 Processing chunk 1/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:Wikidata returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:DBpedia returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:Wikidata returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:DBpedia returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:Wikidata returned status 400 for English\n",
            "Character\n",
            "WARNING:__main__:DBpedia returned status 400 for English\n",
            "Character\n",
            "WARNING:__main__:⏰ Timeout: 75/94 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 2/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 429 for Corcyraean\n",
            "WARNING:__main__:⏰ Timeout: 37/57 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 3/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 429 for Sermylians\n",
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:Wikidata returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:DBpedia returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:⏰ Timeout: 30/53 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 4/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 40/62 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 5/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 42/56 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 6/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 429 for Melesippus\n",
            "WARNING:__main__:⏰ Timeout: 18/43 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 7/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VII\n",
            "Second Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VII\n",
            "Second Year\n",
            "WARNING:__main__:⏰ Timeout: 18/40 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 8/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:Wikidata returned status 400 for Sitalces\n",
            "\n",
            "The\n",
            "WARNING:__main__:DBpedia returned status 400 for Sitalces\n",
            "\n",
            "The\n",
            "WARNING:__main__:⏰ Timeout: 22/46 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 9/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 52/65 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 10/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:⏰ Timeout: 51/66 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Saved enhanced RDF to /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl\n",
            "📊 Processing Statistics:\n",
            "   - Total chunks processed: 10\n",
            "   - Successful chunks: 10\n",
            "   - Unique entities found: 283\n",
            "   - Total KG facts retrieved: 337\n",
            "   - Cache hits: 618\n",
            "   - Locations found: 97\n",
            "   - Locations with coordinates: 97\n",
            "   - Location duplicates avoided: 53\n",
            "   - Unique global locations: 44\n",
            "   - RAG queries for RDF generation: 10\n",
            "\n",
            "🔗 Knowledge Graph Connector Statistics:\n",
            "   - Wikidata: 59/72 requests (81.9% success)\n",
            "   - DBpedia: 48/58 requests (82.8% success)\n",
            "   - ConceptNet: 5/97 requests (5.2% success)\n",
            "   - Location enrichment: 86/287 locations enriched (30.0% success)\n",
            "\n",
            "📝 Sample of generated RDF:\n",
            "============================================================\n",
            "@prefix ste: <http://www.example.org/ste#> .\n",
            "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
            "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
            "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
            "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
            "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
            "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
            "\n",
            "# Historical Events with RAG-Enhanced Embedded Location Data (LLAMA)\n",
            "After analyzing the provided text chunk, I extracted the following events:\n",
            "\n",
            "\t* ste:hasType \"expansion of empire\" ;\n",
            "\t* ste:hasAgent \"Athenians\" ;\n",
            "\t* ste:hasTime \"5th century BC\" ;\n",
            "\t* ste:hasLocation \"Peloponnese, Greece\" ;\n",
            "\t* ste:hasLatitude 37.1234^^xsd:double ;\n",
            "\t* ste:hasLongitude 22.5678^^xsd:double ;\n",
            "\t* ste:hasCountry \"Greece\" ;\n",
            "\t* ste:hasRegion \"Peloponnese\" ;\n",
            "\t* ste:hasLocationSource \"wikidata\" ;\n",
            "\t* ste:hasResult \"conquest of surrounding territories\" ;\n",
            "\t* ste:hasRAGContext \"yes\" .\n",
            "\t* ste:hasType \"battle\" ;\n",
            "\t* ste:hasAgent \"Athenians\" ;\n",
            "\t* ste:hasTime \"490...\n",
            "============================================================\n",
            "\n",
            "🤖 RAG System Ready!\n",
            "\n",
            "💡 Try asking questions like:\n",
            "   - 'What battles happened in Sicily?'\n",
            "   - 'Who were the main leaders mentioned?'\n",
            "   - 'What events occurred in 415 BC?'\n",
            "   - 'Describe the naval engagements'\n",
            "   - 'What was the outcome of the siege?'\n",
            "\n",
            "❓ Start interactive RAG session? (y/n): n\n",
            "\n",
            "💡 You can also query programmatically:\n",
            "   result = multi_kg_system.rag_query('your question', llm)\n",
            "\n",
            "🔍 Running sample queries:\n",
            "\n",
            "❓ Sample query: 'What are the main events mentioned in the text?'\n",
            "📝 Answer: Based on the retrieved text chunks and knowledge graph context, I can identify several main events mentioned in the text:\n",
            "\n",
            "1. **The Second Congress at Lacedaemon**: This event is mentioned as a precur...\n",
            "📊 Sources: 3 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Which locations are mentioned?'\n",
            "📝 Answer: Based on the provided context, I can identify several locations that are mentioned in the retrieved text chunks:\n",
            "\n",
            "1. The Thracian towns: Although not specific locations, it is implied that there are m...\n",
            "📊 Sources: 3 chunks, 12 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "❓ Sample query: 'Who are the key people involved?'\n",
            "📝 Answer: Based on the provided context, it appears that the question \"Who are the key people involved?\" is related to a speech or oration being delivered, possibly in an ancient Greek setting. The text chunks ...\n",
            "📊 Sources: 3 chunks, 0 KG facts\n",
            "----------------------------------------\n",
            "\n",
            "🎉 Process complete! Check /content/drive/MyDrive/extracted_events_rag_with_multi_kg_llama.ttl for RDF results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced Multi-Knowledge Graph System with Text Chunking - LLAMA VERSION\n",
        "Handles large texts by processing them in chunks to avoid token limits\n",
        "RAG FUNCTIONALITY DEACTIVATED (Knowledge Graph Enhancement Still Active)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import requests\n",
        "\n",
        "from rdflib import Graph, RDFS, RDF, OWL, URIRef, Namespace, Literal\n",
        "from rdflib.namespace import XSD, SKOS\n",
        "\n",
        "# Configuration - UPDATED FOR GOOGLE COLAB\n",
        "INPUT_TEXT_FILE = \"/content/drive/MyDrive/part_aa\"  # Update this path as needed\n",
        "ONTOLOGY_PATH = \"/content/drive/MyDrive/wiki.owl\"\n",
        "LOCATION_ONTOLOGY_PATH = \"/content/drive/MyDrive/locations.owl\"\n",
        "OUTPUT_RAG_TTL = '/content/drive/MyDrive/extracted_events_norag_with_multi_kg_llama.ttl'\n",
        "OUTPUT_RAG_OWL = '/content/drive/MyDrive/extracted_events_norag_with_multi_kg_llama.owl'\n",
        "KG_CACHE_FILE = '/content/drive/MyDrive/kg_cache.json'\n",
        "LOCATION_CACHE_FILE = '/content/drive/MyDrive/location_cache.json'\n",
        "KG_ANALYSIS_REPORT = '/content/drive/MyDrive/multi_kg_analysis_report.txt'\n",
        "\n",
        "# Token limits - adjusted for Llama\n",
        "MAX_TOKENS_PER_REQUEST = 50000  # Conservative limit for Llama\n",
        "CHUNK_OVERLAP = 200  # Characters to overlap between chunks\n",
        "\n",
        "# RAG DEACTIVATION FLAG\n",
        "RAG_ENABLED = False  # Set to False to deactivate RAG\n",
        "\n",
        "# Namespaces\n",
        "EX = Namespace(\"http://example.org/\")\n",
        "STE = Namespace(\"http://www.example.org/ste#\")\n",
        "DBP = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "LAC = Namespace(\"http://ontologia.fr/OTB/lac#\")\n",
        "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
        "YAGO = Namespace(\"http://yago-knowledge.org/resource/\")\n",
        "CN = Namespace(\"http://conceptnet.io/c/en/\")\n",
        "GEO = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
        "DBPR = Namespace(\"http://dbpedia.org/resource/\")\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Imports\n",
        "try:\n",
        "    if RAG_ENABLED:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "    from langchain_ollama import ChatOllama  # Changed from OpenAI to Ollama\n",
        "    from langchain.schema import HumanMessage\n",
        "except ImportError as e:\n",
        "    print(f\"ImportError: {e}\")\n",
        "    print(\"Install dependencies: pip install rdflib langchain langchain-ollama langchain-community requests\")\n",
        "    exit(1)\n",
        "\n",
        "@dataclass\n",
        "class LocationInfo:\n",
        "    \"\"\"Location information with coordinates\"\"\"\n",
        "    name: str\n",
        "    latitude: Optional[float] = None\n",
        "    longitude: Optional[float] = None\n",
        "    country: Optional[str] = None\n",
        "    region: Optional[str] = None\n",
        "    source: str = \"extracted\"\n",
        "    confidence: float = 1.0\n",
        "    uri: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class EnhancedKnowledgeFact:\n",
        "    \"\"\"Enhanced knowledge fact with metadata\"\"\"\n",
        "    subject: str\n",
        "    predicate: str\n",
        "    object: str\n",
        "    source: str\n",
        "    confidence: float = 1.0\n",
        "    context: Optional[str] = None\n",
        "    temporal: Optional[str] = None\n",
        "    spatial: Optional[str] = None\n",
        "    evidence_score: float = 1.0\n",
        "    source_uri: Optional[str] = None\n",
        "\n",
        "class LocationExtractor:\n",
        "    \"\"\"Extracts and enriches location information\"\"\"\n",
        "\n",
        "    def __init__(self, ontology_path: str = LOCATION_ONTOLOGY_PATH):\n",
        "        self.ontology_path = ontology_path\n",
        "        self.location_graph = None\n",
        "        self.location_cache = self._load_location_cache()\n",
        "        self.load_location_ontology()\n",
        "\n",
        "    def _load_location_cache(self) -> Dict:\n",
        "        \"\"\"Load location cache\"\"\"\n",
        "        if os.path.exists(LOCATION_CACHE_FILE):\n",
        "            try:\n",
        "                with open(LOCATION_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load location cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_location_cache(self):\n",
        "        \"\"\"Save location cache\"\"\"\n",
        "        try:\n",
        "            with open(LOCATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.location_cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save location cache: {e}\")\n",
        "\n",
        "    def load_location_ontology(self):\n",
        "        \"\"\"Load locations.owl ontology\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.ontology_path):\n",
        "                self.location_graph = Graph()\n",
        "                self.location_graph.parse(self.ontology_path, format=\"xml\")\n",
        "                logger.info(f\"Loaded location ontology from {self.ontology_path}\")\n",
        "            else:\n",
        "                logger.warning(f\"Location ontology not found at {self.ontology_path}\")\n",
        "                self.location_graph = None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading location ontology: {e}\")\n",
        "            self.location_graph = None\n",
        "\n",
        "    def extract_locations_from_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract potential location names from text\"\"\"\n",
        "        location_patterns = [\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?:\\s+(?:City|County|State|Province|Country|Region|Island|Bay|Sea|Ocean|River|Mountain|Valley|Desert))\\b',\n",
        "            r'\\b(?:Mount|Lake|River|Cape|Fort|Port|Saint|St\\.)\\s+[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*(?=\\s+(?:in|near|at|from|to))\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]{2,}(?:\\s+[A-Z][a-zA-Z]{2,})*\\b'\n",
        "        ]\n",
        "\n",
        "        locations = []\n",
        "        for pattern in location_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            locations.extend(matches)\n",
        "\n",
        "        location_stopwords = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If',\n",
        "            'When', 'Where', 'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Most',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Before', 'After', 'During',\n",
        "            'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "            'September', 'October', 'November', 'December', 'Monday', 'Tuesday',\n",
        "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
        "        }\n",
        "\n",
        "        filtered_locations = []\n",
        "        for loc in locations:\n",
        "            loc = loc.strip()\n",
        "            if (loc not in location_stopwords and len(loc) > 2 and\n",
        "                not loc.isdigit() and not re.match(r'^\\d+', loc)):\n",
        "                filtered_locations.append(loc)\n",
        "\n",
        "        return list(set(filtered_locations))\n",
        "\n",
        "    def get_location_from_ontology(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location info from local ontology\"\"\"\n",
        "        if not self.location_graph:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            query = f\"\"\"\n",
        "            SELECT DISTINCT ?location ?lat ?long ?country ?region WHERE {{\n",
        "                ?location rdfs:label ?label .\n",
        "                FILTER(regex(?label, \"{location_name}\", \"i\"))\n",
        "                OPTIONAL {{ ?location geo:lat ?lat }}\n",
        "                OPTIONAL {{ ?location geo:long ?long }}\n",
        "                OPTIONAL {{ ?location dbp:country ?country }}\n",
        "                OPTIONAL {{ ?location dbp:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            results = self.location_graph.query(query)\n",
        "            for row in results:\n",
        "                return LocationInfo(\n",
        "                    name=location_name,\n",
        "                    latitude=float(row.lat) if row.lat else None,\n",
        "                    longitude=float(row.long) if row.long else None,\n",
        "                    country=str(row.country) if row.country else None,\n",
        "                    region=str(row.region) if row.region else None,\n",
        "                    source=\"local_ontology\",\n",
        "                    uri=str(row.location) if row.location else None\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Ontology query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_dbpedia(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from DBpedia\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{location_name.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?lat ?long ?country ?region WHERE {{\n",
        "                <{entity_uri}> geo:lat ?lat ;\n",
        "                               geo:long ?long .\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:country ?country }}\n",
        "                OPTIONAL {{ <{entity_uri}> dbo:region ?region }}\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(\"https://dbpedia.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                if bindings:\n",
        "                    binding = bindings[0]\n",
        "                    return LocationInfo(\n",
        "                        name=location_name,\n",
        "                        latitude=float(binding.get('lat', {}).get('value', 0)),\n",
        "                        longitude=float(binding.get('long', {}).get('value', 0)),\n",
        "                        country=binding.get('country', {}).get('value', ''),\n",
        "                        region=binding.get('region', {}).get('value', ''),\n",
        "                        source=\"dbpedia\",\n",
        "                        uri=entity_uri\n",
        "                    )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"DBpedia location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_location_from_wikidata(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get location coordinates from Wikidata with disambiguation\"\"\"\n",
        "        try:\n",
        "            time.sleep(0.5)\n",
        "\n",
        "            # Try multiple query strategies to get the right location\n",
        "            queries = [\n",
        "                # Try exact label match first\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\",\n",
        "                # Try with additional filters for places/locations\n",
        "                f\"\"\"\n",
        "                SELECT DISTINCT ?item ?itemLabel ?coord ?country ?countryLabel WHERE {{\n",
        "                  ?item rdfs:label \"{location_name}\"@en .\n",
        "                  ?item wdt:P625 ?coord .\n",
        "                  ?item wdt:P31/wdt:P279* wd:Q486972 .  # human settlement\n",
        "                  OPTIONAL {{ ?item wdt:P17 ?country }}\n",
        "                  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "                }}\n",
        "                LIMIT 5\n",
        "                \"\"\"\n",
        "            ]\n",
        "\n",
        "            for query in queries:\n",
        "                params = {'query': query, 'format': 'json'}\n",
        "                response = requests.get(\"https://query.wikidata.org/sparql\", params=params, timeout=10)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    bindings = data.get('results', {}).get('bindings', [])\n",
        "\n",
        "                    if bindings:\n",
        "                        # Prefer results with country information\n",
        "                        best_binding = None\n",
        "                        for binding in bindings:\n",
        "                            if binding.get('country'):\n",
        "                                best_binding = binding\n",
        "                                break\n",
        "\n",
        "                        if not best_binding:\n",
        "                            best_binding = bindings[0]\n",
        "\n",
        "                        coord_str = best_binding.get('coord', {}).get('value', '')\n",
        "\n",
        "                        coord_match = re.search(r'Point\\(([+-]?\\d*\\.?\\d+)\\s+([+-]?\\d*\\.?\\d+)\\)', coord_str)\n",
        "                        if coord_match:\n",
        "                            longitude = float(coord_match.group(1))\n",
        "                            latitude = float(coord_match.group(2))\n",
        "\n",
        "                            return LocationInfo(\n",
        "                                name=location_name,\n",
        "                                latitude=latitude,\n",
        "                                longitude=longitude,\n",
        "                                country=best_binding.get('countryLabel', {}).get('value', ''),\n",
        "                                source=\"wikidata\",\n",
        "                                uri=best_binding.get('item', {}).get('value', '')\n",
        "                            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Wikidata location query failed for {location_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def validate_coordinates(self, location_info: LocationInfo) -> bool:\n",
        "        \"\"\"Validate that coordinates make sense for the location\"\"\"\n",
        "        if not location_info.latitude or not location_info.longitude:\n",
        "            return True\n",
        "\n",
        "        lat, lon = location_info.latitude, location_info.longitude\n",
        "\n",
        "        # Basic coordinate range validation\n",
        "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
        "            logger.warning(f\"Invalid coordinates for {location_info.name}: {lat}, {lon}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def enrich_location(self, location_name: str) -> Optional[LocationInfo]:\n",
        "        \"\"\"Get enriched location information with coordinates\"\"\"\n",
        "        if location_name in self.location_cache:\n",
        "            cached = self.location_cache[location_name]\n",
        "            return LocationInfo(**cached) if cached else None\n",
        "\n",
        "        location_info = None\n",
        "\n",
        "        location_info = self.get_location_from_ontology(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_wikidata(location_name)\n",
        "\n",
        "        if not location_info:\n",
        "            location_info = self.get_location_from_dbpedia(location_name)\n",
        "\n",
        "        if location_info:\n",
        "            self.location_cache[location_name] = {\n",
        "                'name': location_info.name,\n",
        "                'latitude': location_info.latitude,\n",
        "                'longitude': location_info.longitude,\n",
        "                'country': location_info.country,\n",
        "                'region': location_info.region,\n",
        "                'source': location_info.source,\n",
        "                'confidence': location_info.confidence,\n",
        "                'uri': location_info.uri\n",
        "            }\n",
        "        else:\n",
        "            self.location_cache[location_name] = None\n",
        "\n",
        "        self._save_location_cache()\n",
        "\n",
        "        if location_info:\n",
        "            self.validate_coordinates(location_info)\n",
        "\n",
        "        return location_info\n",
        "\n",
        "class TextChunker:\n",
        "    \"\"\"Handles text chunking to manage token limits for Llama\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"llama3.2\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Approximate token count for Llama (roughly 4 chars per token)\"\"\"\n",
        "        return len(text) // 4\n",
        "\n",
        "    def chunk_text_by_sentences(self, text: str, max_tokens: int = 10000) -> List[str]:\n",
        "        \"\"\"Chunk text by sentences to maintain coherence\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk = test_chunk\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class BaseKGConnector:\n",
        "    \"\"\"Base class for knowledge graph connectors\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, base_url: str, rate_limit: float = 1.0):\n",
        "        self.name = name\n",
        "        self.base_url = base_url\n",
        "        self.rate_limit = rate_limit\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "        self.success_count = 0\n",
        "\n",
        "    def _rate_limit_wait(self):\n",
        "        \"\"\"Enforce rate limiting\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "        if time_since_last < self.rate_limit:\n",
        "            time.sleep(self.rate_limit - time_since_last)\n",
        "        self.last_request_time = time.time()\n",
        "        self.request_count += 1\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get connector statistics\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'requests': self.request_count,\n",
        "            'successes': self.success_count,\n",
        "            'success_rate': self.success_count / max(1, self.request_count)\n",
        "        }\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Abstract method to retrieve facts\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EnhancedWikidataConnector(BaseKGConnector):\n",
        "    \"\"\"Wikidata connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Wikidata\", \"https://query.wikidata.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from Wikidata with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?subject ?subjectLabel ?predicate ?predicateLabel ?object ?objectLabel WHERE {{\n",
        "              {{\n",
        "                ?subject ?label \"{entity}\"@en .\n",
        "              }} UNION {{\n",
        "                ?subject rdfs:label \"{entity}\"@en .\n",
        "              }}\n",
        "\n",
        "              ?subject ?predicate ?object .\n",
        "              FILTER(?predicate != wdt:P31 && ?predicate != wdt:P279)\n",
        "\n",
        "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=binding.get('subjectLabel', {}).get('value', entity),\n",
        "                        predicate=binding.get('predicateLabel', {}).get('value', 'related_to'),\n",
        "                        object=binding.get('objectLabel', {}).get('value', ''),\n",
        "                        source=self.name,\n",
        "                        confidence=0.9,\n",
        "                        source_uri=binding.get('subject', {}).get('value')\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from Wikidata for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"Wikidata returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"Wikidata query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Wikidata query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedDBpediaConnector(BaseKGConnector):\n",
        "    \"\"\"DBpedia connector\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"DBpedia\", \"https://dbpedia.org/sparql\", 1.0)\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 3) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from DBpedia with timeout protection\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "\n",
        "            entity_uri = f\"http://dbpedia.org/resource/{entity.replace(' ', '_')}\"\n",
        "\n",
        "            sparql_query = f\"\"\"\n",
        "            SELECT DISTINCT ?predicate ?object WHERE {{\n",
        "              <{entity_uri}> ?predicate ?object .\n",
        "              FILTER(LANG(?object) = \"en\" || !isLiteral(?object))\n",
        "              FILTER(!isBlank(?object))\n",
        "            }}\n",
        "            LIMIT {limit}\n",
        "            \"\"\"\n",
        "\n",
        "            params = {'query': sparql_query, 'format': 'json'}\n",
        "            response = requests.get(self.base_url, params=params, timeout=12)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                facts = []\n",
        "\n",
        "                for binding in data.get('results', {}).get('bindings', []):\n",
        "                    predicate = binding.get('predicate', {}).get('value', '')\n",
        "                    obj = binding.get('object', {}).get('value', '')\n",
        "\n",
        "                    predicate_name = predicate.split('/')[-1].replace('_', ' ')\n",
        "\n",
        "                    fact = EnhancedKnowledgeFact(\n",
        "                        subject=entity,\n",
        "                        predicate=predicate_name,\n",
        "                        object=obj,\n",
        "                        source=self.name,\n",
        "                        confidence=0.85,\n",
        "                        source_uri=entity_uri\n",
        "                    )\n",
        "                    facts.append(fact)\n",
        "\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(facts)} facts from DBpedia for '{entity}'\")\n",
        "                return facts\n",
        "            else:\n",
        "                logger.warning(f\"DBpedia returned status {response.status_code} for {entity}\")\n",
        "\n",
        "        except requests.Timeout:\n",
        "            logger.warning(f\"DBpedia query timeout for '{entity}'\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"DBpedia query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "class EnhancedConceptNetConnector(BaseKGConnector):\n",
        "    \"\"\"ConceptNet connector with dynamic concept discovery\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"ConceptNet\", \"http://api.conceptnet.io\", 0.5)\n",
        "\n",
        "    def search_related_concepts(self, entity: str) -> List[str]:\n",
        "        \"\"\"Search for related concepts using ConceptNet's search API\"\"\"\n",
        "        try:\n",
        "            search_url = f\"{self.base_url}/search?text={entity.replace(' ', '%20')}&limit=10\"\n",
        "            response = requests.get(search_url, timeout=10)\n",
        "\n",
        "            related_concepts = []\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for edge in data.get('edges', []):\n",
        "                    start = edge.get('start', {}).get('label', '')\n",
        "                    end = edge.get('end', {}).get('label', '')\n",
        "\n",
        "                    for concept_path in [start, end]:\n",
        "                        if concept_path and '/c/en/' in concept_path:\n",
        "                            concept = concept_path.replace('/c/en/', '').replace('_', ' ')\n",
        "                            if concept.lower() != entity.lower() and len(concept) > 2:\n",
        "                                related_concepts.append(concept)\n",
        "\n",
        "            return list(set(related_concepts))[:5]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet search failed for {entity}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_concept_directly(self, concept: str, limit: int = 20) -> List[dict]:\n",
        "        \"\"\"Query a specific concept and return raw edges\"\"\"\n",
        "        try:\n",
        "            concept_path = f\"/c/en/{concept.lower().replace(' ', '_')}\"\n",
        "            url = f\"{self.base_url}{concept_path}?limit={limit}\"\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return data.get('edges', [])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"ConceptNet direct query failed for {concept}: {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def retrieve_facts(self, entity: str, limit: int = 100) -> List[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Retrieve facts from ConceptNet through dynamic discovery\"\"\"\n",
        "        try:\n",
        "            self._rate_limit_wait()\n",
        "            all_facts = []\n",
        "\n",
        "            direct_edges = self.query_concept_directly(entity, limit//2)\n",
        "            related_concepts = self.search_related_concepts(entity)\n",
        "\n",
        "            for edge in direct_edges:\n",
        "                fact = self._edge_to_fact(edge, entity, \"direct\")\n",
        "                if fact:\n",
        "                    all_facts.append(fact)\n",
        "\n",
        "            for concept in related_concepts:\n",
        "                concept_edges = self.query_concept_directly(concept, 5)\n",
        "                for edge in concept_edges:\n",
        "                    fact = self._edge_to_fact(edge, entity, f\"via_{concept}\")\n",
        "                    if fact:\n",
        "                        all_facts.append(fact)\n",
        "\n",
        "            if all_facts:\n",
        "                self.success_count += 1\n",
        "                logger.info(f\"Retrieved {len(all_facts)} facts from ConceptNet for '{entity}'\")\n",
        "                if related_concepts:\n",
        "                    logger.info(f\"  - Found related concepts: {related_concepts}\")\n",
        "\n",
        "            return all_facts[:limit]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ConceptNet query failed for '{entity}': {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "    def _edge_to_fact(self, edge: dict, original_entity: str, discovery_method: str) -> Optional[EnhancedKnowledgeFact]:\n",
        "        \"\"\"Convert ConceptNet edge to EnhancedKnowledgeFact\"\"\"\n",
        "        try:\n",
        "            start = edge.get('start', {})\n",
        "            end = edge.get('end', {})\n",
        "            relation = edge.get('rel', {})\n",
        "            weight = edge.get('weight', 1.0)\n",
        "\n",
        "            start_label = start.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            end_label = end.get('label', '').replace('/c/en/', '').replace('_', ' ')\n",
        "            rel_label = relation.get('label', 'related_to')\n",
        "\n",
        "            if not start_label or not end_label or len(start_label) < 2 or len(end_label) < 2:\n",
        "                return None\n",
        "\n",
        "            confidence_multiplier = 1.0 if discovery_method == \"direct\" else 0.6\n",
        "\n",
        "            return EnhancedKnowledgeFact(\n",
        "                subject=original_entity,\n",
        "                predicate=rel_label,\n",
        "                object=end_label if start_label.lower() in original_entity.lower() else start_label,\n",
        "                source=self.name,\n",
        "                confidence=min(weight * confidence_multiplier, 1.0),\n",
        "                context=f\"Discovered {discovery_method}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error converting edge to fact: {e}\")\n",
        "            return None\n",
        "\n",
        "class MultiKGCache:\n",
        "    \"\"\"Caching system for knowledge graph facts\"\"\"\n",
        "\n",
        "    def __init__(self, cache_file: str = KG_CACHE_FILE):\n",
        "        self.cache_file = cache_file\n",
        "        self.cache = self._load_cache()\n",
        "\n",
        "    def _load_cache(self) -> Dict:\n",
        "        \"\"\"Load cache from file\"\"\"\n",
        "        if os.path.exists(self.cache_file):\n",
        "            try:\n",
        "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load cache: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def _save_cache(self):\n",
        "        \"\"\"Save cache to file\"\"\"\n",
        "        try:\n",
        "            with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.cache, f, indent=2, ensure_ascii=False)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not save cache: {e}\")\n",
        "\n",
        "    def get_cache_key(self, source: str, entity: str) -> str:\n",
        "        \"\"\"Generate cache key\"\"\"\n",
        "        return f\"{source}:{hashlib.md5(entity.encode()).hexdigest()}\"\n",
        "\n",
        "    def get(self, source: str, entity: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Get cached facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        return self.cache.get(key)\n",
        "\n",
        "    def set(self, source: str, entity: str, facts: List[EnhancedKnowledgeFact]):\n",
        "        \"\"\"Cache facts\"\"\"\n",
        "        key = self.get_cache_key(source, entity)\n",
        "        serializable_facts = []\n",
        "        for fact in facts:\n",
        "            serializable_facts.append({\n",
        "                'subject': fact.subject,\n",
        "                'predicate': fact.predicate,\n",
        "                'object': fact.object,\n",
        "                'source': fact.source,\n",
        "                'confidence': fact.confidence,\n",
        "                'context': fact.context,\n",
        "                'temporal': fact.temporal,\n",
        "                'spatial': fact.spatial,\n",
        "                'evidence_score': fact.evidence_score,\n",
        "                'source_uri': fact.source_uri\n",
        "            })\n",
        "        self.cache[key] = serializable_facts\n",
        "        self._save_cache()\n",
        "\n",
        "class EnhancedMultiKGRAGSystem:\n",
        "    \"\"\"Multi-Knowledge Graph system with RAG functionality DEACTIVATED - LLAMA VERSION\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.connectors = {\n",
        "            'wikidata': EnhancedWikidataConnector(),\n",
        "            'dbpedia': EnhancedDBpediaConnector(),\n",
        "            'conceptnet': EnhancedConceptNetConnector()\n",
        "        }\n",
        "        self.cache = MultiKGCache()\n",
        "        self.chunker = TextChunker()\n",
        "        self.location_extractor = LocationExtractor()\n",
        "        self.global_locations = {}\n",
        "        # RAG components deactivated\n",
        "        self.vectorstore = None\n",
        "        self.document_chunks = []\n",
        "        self.stats = {\n",
        "            'queries_processed': 0,\n",
        "            'entities_extracted': 0,\n",
        "            'facts_retrieved': 0,\n",
        "            'cache_hits': 0,\n",
        "            'chunks_processed': 0,\n",
        "            'locations_found': 0,\n",
        "            'locations_with_coordinates': 0,\n",
        "            'location_duplicates_avoided': 0,\n",
        "            'rag_queries': 0\n",
        "        }\n",
        "\n",
        "    def extract_entities_advanced(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract entities from text\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        pattern = r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b'\n",
        "        matches = re.findall(pattern, text)\n",
        "        entities.extend(matches)\n",
        "\n",
        "        stop_words = {\n",
        "            'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'Or', 'So', 'If', 'When', 'Where',\n",
        "            'Who', 'What', 'How', 'Why', 'All', 'Some', 'Many', 'Few', 'Most', 'Each', 'Every',\n",
        "            'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Before', 'After', 'During'\n",
        "        }\n",
        "\n",
        "        filtered_entities = []\n",
        "        for entity in entities:\n",
        "            entity = entity.strip()\n",
        "            if (entity not in stop_words and len(entity) > 2 and not entity.isdigit()):\n",
        "                filtered_entities.append(entity)\n",
        "\n",
        "        seen = set()\n",
        "        unique_entities = []\n",
        "        for entity in filtered_entities:\n",
        "            if entity.lower() not in seen:\n",
        "                seen.add(entity.lower())\n",
        "                unique_entities.append(entity)\n",
        "\n",
        "        return unique_entities[:15]\n",
        "\n",
        "    def retrieve_kg_facts_enhanced(self, entities: List[str]) -> Dict[str, List[EnhancedKnowledgeFact]]:\n",
        "        \"\"\"Retrieve facts from knowledge graphs with improved timeout handling\"\"\"\n",
        "        all_facts = {}\n",
        "        cache_hits = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "            futures = {}\n",
        "\n",
        "            for entity in entities:\n",
        "                for source_name, connector in self.connectors.items():\n",
        "                    # Check cache first\n",
        "                    cached_facts = self.cache.get(source_name, entity)\n",
        "                    if cached_facts:\n",
        "                        cache_hits += 1\n",
        "                        if entity not in all_facts:\n",
        "                            all_facts[entity] = []\n",
        "                        for fact_data in cached_facts:\n",
        "                            fact = EnhancedKnowledgeFact(**fact_data)\n",
        "                            all_facts[entity].append(fact)\n",
        "                    else:\n",
        "                        future = executor.submit(connector.retrieve_facts, entity, 3)\n",
        "                        futures[future] = (entity, source_name)\n",
        "\n",
        "            # Collect results with better timeout handling\n",
        "            completed = 0\n",
        "            total_futures = len(futures)\n",
        "\n",
        "            try:\n",
        "                for future in as_completed(futures, timeout=45):\n",
        "                    entity, source_name = futures[future]\n",
        "                    completed += 1\n",
        "\n",
        "                    try:\n",
        "                        facts = future.result(timeout=5)\n",
        "                        if facts:\n",
        "                            self.cache.set(source_name, entity, facts)\n",
        "\n",
        "                            if entity not in all_facts:\n",
        "                                all_facts[entity] = []\n",
        "                            all_facts[entity].extend(facts)\n",
        "\n",
        "                            self.stats['facts_retrieved'] += len(facts)\n",
        "\n",
        "                        logger.debug(f\"✅ {source_name} completed for {entity} ({completed}/{total_futures})\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"❌ {source_name} failed for {entity}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except TimeoutError:\n",
        "                pending_count = total_futures - completed\n",
        "                logger.warning(f\"⏰ Timeout: {pending_count}/{total_futures} KG queries still pending, continuing with available results\")\n",
        "\n",
        "                # Cancel remaining futures\n",
        "                for future in futures:\n",
        "                    if not future.done():\n",
        "                        future.cancel()\n",
        "\n",
        "        self.stats['cache_hits'] += cache_hits\n",
        "        logger.info(f\"KG retrieval completed: {completed}/{total_futures} successful, {cache_hits} cache hits\")\n",
        "        return all_facts\n",
        "\n",
        "    def format_kg_context_enhanced(self, kg_facts: Dict[str, List[EnhancedKnowledgeFact]]) -> str:\n",
        "        \"\"\"Format KG facts into context string\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        for entity, facts in kg_facts.items():\n",
        "            if facts:\n",
        "                sorted_facts = sorted(facts, key=lambda f: f.confidence, reverse=True)\n",
        "\n",
        "                context_parts.append(f\"\\n=== Knowledge about {entity} ===\")\n",
        "\n",
        "                by_source = {}\n",
        "                for fact in sorted_facts[:3]:\n",
        "                    if fact.source not in by_source:\n",
        "                        by_source[fact.source] = []\n",
        "                    by_source[fact.source].append(fact)\n",
        "\n",
        "                for source, source_facts in by_source.items():\n",
        "                    context_parts.append(f\"\\nFrom {source}:\")\n",
        "                    for fact in source_facts[:2]:\n",
        "                        fact_str = f\"- {fact.subject} {fact.predicate} {fact.object}\"\n",
        "                        if fact.confidence < 0.8:\n",
        "                            fact_str += f\" (confidence: {fact.confidence:.2f})\"\n",
        "                        context_parts.append(fact_str)\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def register_global_location(self, location_info: LocationInfo) -> str:\n",
        "        \"\"\"Register location globally and return unique identifier\"\"\"\n",
        "        location_key = location_info.name.lower().strip()\n",
        "\n",
        "        if location_key in self.global_locations:\n",
        "            existing = self.global_locations[location_key]\n",
        "            if (location_info.latitude and location_info.longitude and\n",
        "                (not existing.latitude or not existing.longitude)):\n",
        "                self.global_locations[location_key] = location_info\n",
        "                logger.info(f\"Updated coordinates for {location_info.name}\")\n",
        "            else:\n",
        "                self.stats['location_duplicates_avoided'] += 1\n",
        "                logger.debug(f\"Location {location_info.name} already registered\")\n",
        "        else:\n",
        "            self.global_locations[location_key] = location_info\n",
        "            logger.info(f\"Registered new location: {location_info.name}\")\n",
        "\n",
        "        clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "        return f\"ste:Location_{clean_name}\"\n",
        "\n",
        "    def process_chunk(self, chunk: str, chunk_num: int, llm) -> str:\n",
        "        \"\"\"Process a single chunk of text WITHOUT RAG (RAG DEACTIVATED) - LLAMA VERSION\"\"\"\n",
        "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk)} chars) - RAG DISABLED\")\n",
        "\n",
        "        # RAG retrieval DEACTIVATED - skip this step\n",
        "        # relevant_context = \"\"\n",
        "\n",
        "        # Extract entities and locations (this remains the same)\n",
        "        entities = self.extract_entities_advanced(chunk)\n",
        "        locations = self.location_extractor.extract_locations_from_text(chunk)\n",
        "        logger.info(f\"Found potential locations in chunk {chunk_num}: {locations}\")\n",
        "\n",
        "        # Enrich locations with coordinates\n",
        "        enriched_locations = {}\n",
        "        for location_name in locations[:10]:\n",
        "            location_info = self.location_extractor.enrich_location(location_name)\n",
        "            if location_info:\n",
        "                self.register_global_location(location_info)\n",
        "                enriched_locations[location_name] = location_info\n",
        "                self.stats['locations_found'] += 1\n",
        "                if location_info.latitude and location_info.longitude:\n",
        "                    self.stats['locations_with_coordinates'] += 1\n",
        "\n",
        "        if not entities and not enriched_locations:\n",
        "            logger.info(f\"No entities or locations found in chunk {chunk_num}\")\n",
        "            return \"\"\n",
        "\n",
        "        logger.info(f\"Found entities in chunk {chunk_num}: {entities[:5]}...\")\n",
        "        logger.info(f\"Enriched {len(enriched_locations)} locations with coordinates\")\n",
        "\n",
        "        # Get KG facts for entities (this remains the same)\n",
        "        kg_facts = self.retrieve_kg_facts_enhanced(entities)\n",
        "        kg_context = self.format_kg_context_enhanced(kg_facts)\n",
        "        location_context = self.format_location_context(enriched_locations)\n",
        "\n",
        "        # SIMPLIFIED PROMPT WITHOUT RAG - OPTIMIZED FOR LLAMA\n",
        "        simplified_prompt = f\"\"\"You are extracting historical events from text chunks. Use knowledge graph facts and location coordinates to enhance your extraction.\n",
        "\n",
        "CURRENT TEXT CHUNK {chunk_num} TO ANALYZE:\n",
        "{chunk}\n",
        "\n",
        "KNOWLEDGE GRAPH FACTS FOR ENTITIES IN THIS CHUNK:\n",
        "{kg_context}\n",
        "\n",
        "LOCATION INFORMATION WITH COORDINATES:\n",
        "{location_context}\n",
        "\n",
        "TASK: Extract ONLY the events that are actually mentioned in the current text chunk.\n",
        "\n",
        "Requirements:\n",
        "1. Extract ONLY events mentioned in the CURRENT text chunk\n",
        "2. Use KG facts to enhance entity information\n",
        "3. Use location coordinates to provide precise geographical data\n",
        "4. Include ALL these properties for each event:\n",
        "   - ste:hasType (description of event)\n",
        "   - ste:hasAgent (who caused/led the event)\n",
        "   - ste:hasTime (when it happened)\n",
        "   - ste:hasLocation (location name from text)\n",
        "   - ste:hasLatitude (latitude coordinate if available)\n",
        "   - ste:hasLongitude (longitude coordinate if available)\n",
        "   - ste:hasCountry (country if available)\n",
        "   - ste:hasRegion (region if available)\n",
        "   - ste:hasLocationSource (source of coordinates: wikidata/dbpedia/local_ontology)\n",
        "   - ste:hasResult (outcome/consequence)\n",
        "\n",
        "Output format (do not include prefixes, they will be added later):\n",
        "```turtle\n",
        "ste:Event{chunk_num}_1 a ste:Event, dbp:SpecificEventType ;\n",
        "    ste:hasType \"specific description from current chunk\" ;\n",
        "    ste:hasAgent \"specific person from current chunk\" ;\n",
        "    ste:hasTime \"specific date from current chunk\" ;\n",
        "    ste:hasLocation \"specific location from current chunk\" ;\n",
        "    ste:hasLatitude \"37.1234\"^^xsd:double ;\n",
        "    ste:hasLongitude \"15.5678\"^^xsd:double ;\n",
        "    ste:hasCountry \"Italy\" ;\n",
        "    ste:hasRegion \"Sicily\" ;\n",
        "    ste:hasLocationSource \"wikidata\" ;\n",
        "    ste:hasResult \"specific outcome from current chunk\" .\n",
        "```\n",
        "\n",
        "IMPORTANT:\n",
        "- Extract events ONLY from the CURRENT text chunk\n",
        "- Use KG facts to enrich entity details\n",
        "- Include precise coordinates from location sources\n",
        "- Only extract events explicitly mentioned in the current chunk\n",
        "- If no clear events are found in current chunk, return empty\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = llm.invoke([HumanMessage(content=simplified_prompt)])\n",
        "            turtle_output = self.clean_turtle(response.content)\n",
        "            self.stats['chunks_processed'] += 1\n",
        "            logger.info(f\"Generated RDF for chunk {chunk_num} (without RAG)\")\n",
        "            return turtle_output\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing chunk {chunk_num}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def format_location_context(self, enriched_locations: Dict[str, LocationInfo]) -> str:\n",
        "        \"\"\"Format location information into context string\"\"\"\n",
        "        if not enriched_locations:\n",
        "            return \"No location coordinates available.\"\n",
        "\n",
        "        context_parts = [\"\\n=== Location Information ===\"]\n",
        "\n",
        "        for location_name, location_info in enriched_locations.items():\n",
        "            context_parts.append(f\"\\n{location_name}:\")\n",
        "            context_parts.append(f\"  - Source: {location_info.source}\")\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                context_parts.append(f\"  - Coordinates: {location_info.latitude}, {location_info.longitude}\")\n",
        "                if location_info.source == \"corrected\":\n",
        "                    context_parts.append(f\"  - NOTE: Coordinates were corrected for historical accuracy\")\n",
        "            else:\n",
        "                context_parts.append(\"  - Coordinates: Not available\")\n",
        "\n",
        "            if location_info.country:\n",
        "                context_parts.append(f\"  - Country: {location_info.country}\")\n",
        "\n",
        "            if location_info.region:\n",
        "                context_parts.append(f\"  - Region: {location_info.region}\")\n",
        "\n",
        "            if location_info.uri:\n",
        "                context_parts.append(f\"  - URI: {location_info.uri}\")\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def generate_global_location_rdf(self) -> str:\n",
        "        \"\"\"Generate RDF for all unique locations found across all chunks\"\"\"\n",
        "        if not self.global_locations:\n",
        "            return \"\"\n",
        "\n",
        "        location_rdf_parts = []\n",
        "\n",
        "        for location_key, location_info in self.global_locations.items():\n",
        "            clean_name = re.sub(r'[^a-zA-Z0-9]', '', location_info.name)\n",
        "            location_id = f\"ste:Location_{clean_name}\"\n",
        "\n",
        "            rdf_lines = [f'{location_id} a ste:Location ;']\n",
        "            rdf_lines.append(f'    rdfs:label \"{location_info.name}\" ;')\n",
        "\n",
        "            if location_info.latitude and location_info.longitude:\n",
        "                rdf_lines.append(f'    geo:lat \"{location_info.latitude}\"^^xsd:double ;')\n",
        "                rdf_lines.append(f'    geo:long \"{location_info.longitude}\"^^xsd:double ;')\n",
        "\n",
        "            if location_info.country:\n",
        "                rdf_lines.append(f'    ste:hasCountry \"{location_info.country}\" ;')\n",
        "\n",
        "            if location_info.region:\n",
        "                rdf_lines.append(f'    ste:hasRegion \"{location_info.region}\" ;')\n",
        "\n",
        "            if location_info.source:\n",
        "                rdf_lines.append(f'    ste:hasSource \"{location_info.source}\" ;')\n",
        "\n",
        "            if location_info.uri:\n",
        "                rdf_lines.append(f'    ste:hasURI <{location_info.uri}> ;')\n",
        "\n",
        "            if rdf_lines[-1].endswith(' ;'):\n",
        "                rdf_lines[-1] = rdf_lines[-1][:-2] + ' .'\n",
        "\n",
        "            location_rdf_parts.append('\\n'.join(rdf_lines))\n",
        "\n",
        "        return '\\n\\n'.join(location_rdf_parts)\n",
        "\n",
        "    def clean_turtle(self, raw_output: str) -> str:\n",
        "        \"\"\"Clean turtle output\"\"\"\n",
        "        m = re.search(r\"```(?:turtle)?\\s*(.*?)```\", raw_output, re.DOTALL | re.IGNORECASE)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "\n",
        "        lines = raw_output.strip().split('\\n')\n",
        "        turtle_lines = []\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if (stripped.startswith('@') or stripped.startswith('<') or\n",
        "                stripped.startswith(':') or stripped.startswith('_') or\n",
        "                stripped.startswith('a ') or ':' in stripped or stripped == ''):\n",
        "                turtle_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(turtle_lines)\n",
        "\n",
        "    # RAG METHODS DEACTIVATED\n",
        "    def prepare_vectorstore(self, text_chunks: List[str]):\n",
        "        \"\"\"RAG DEACTIVATED: Vector store preparation disabled\"\"\"\n",
        "        logger.info(\"RAG functionality is DEACTIVATED - vectorstore not created\")\n",
        "        return False\n",
        "\n",
        "    def rag_query(self, query: str, llm, k: int = 20) -> Dict[str, Any]:\n",
        "        \"\"\"RAG DEACTIVATED: RAG queries disabled\"\"\"\n",
        "        return {\"error\": \"RAG functionality is DEACTIVATED. Set RAG_ENABLED=True to enable RAG features.\"}\n",
        "\n",
        "    def interactive_rag_session(self, llm):\n",
        "        \"\"\"RAG DEACTIVATED: Interactive RAG session disabled\"\"\"\n",
        "        print(\"\\n❌ RAG functionality is DEACTIVATED\")\n",
        "        print(\"To enable RAG, set RAG_ENABLED=True at the top of the script\")\n",
        "\n",
        "# Utility functions\n",
        "def load_api_key():\n",
        "    \"\"\"Llama via Ollama doesn't need an API key\"\"\"\n",
        "    print(\"✅ Using local Ollama - no API key needed.\")\n",
        "    return \"local\"\n",
        "\n",
        "def load_text_from_file(filepath: str) -> str:\n",
        "    \"\"\"Load text from file\"\"\"\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(f\"File not found: {filepath}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        print(f\"Loaded text from {filepath}\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def initialize_llm(api_key: str):\n",
        "    \"\"\"Initialize Llama LLM\"\"\"\n",
        "    try:\n",
        "        llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
        "        print(\"✅ Llama LLM initialized successfully.\")\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing LLM: {e}\")\n",
        "        print(\"💡 Make sure Ollama is running: ollama serve\")\n",
        "        print(\"💡 And pull the model: ollama pull llama3.2\")\n",
        "        return None\n",
        "\n",
        "def prepare_vectorstore_from_text(text: str, multi_kg_system):\n",
        "    \"\"\"RAG DEACTIVATED: Vector store creation disabled\"\"\"\n",
        "    if not RAG_ENABLED:\n",
        "        logger.info(\"RAG functionality is DEACTIVATED - vectorstore not created\")\n",
        "        return None\n",
        "\n",
        "    # Original code would go here if RAG_ENABLED was True\n",
        "    return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with chunking support (RAG DEACTIVATED) - LLAMA VERSION\"\"\"\n",
        "    print(\"🚀 Starting Multi-Knowledge Graph System with Chunking (RAG DEACTIVATED) - LLAMA\")\n",
        "\n",
        "    api_key = load_api_key()\n",
        "    if not api_key:\n",
        "        return\n",
        "\n",
        "    domain_text = load_text_from_file(INPUT_TEXT_FILE)\n",
        "    if not domain_text:\n",
        "        print(\"⚠️  No input file found, using sample text\")\n",
        "        domain_text = \"\"\"The Battle of Salamis was a decisive naval battle in 480 BC.\n",
        "        Themistocles led the Greek fleet to victory over the Persians commanded by Xerxes.\n",
        "        This victory established Greek naval supremacy in the Aegean Sea.\"\"\"\n",
        "    else:\n",
        "        print(f\"📄 Using YOUR text from {INPUT_TEXT_FILE}\")\n",
        "        print(f\"📝 Text length: {len(domain_text)} characters\")\n",
        "\n",
        "    multi_kg_system = EnhancedMultiKGRAGSystem()\n",
        "    llm = initialize_llm(api_key)\n",
        "\n",
        "    if not llm:\n",
        "        return\n",
        "\n",
        "    # Vector store preparation SKIPPED (RAG deactivated)\n",
        "    print(\"\\n❌ RAG vector store setup SKIPPED (RAG is DEACTIVATED)\")\n",
        "\n",
        "    token_count = multi_kg_system.chunker.count_tokens(domain_text)\n",
        "    print(f\"🔢 Total tokens in text: {token_count:,}\")\n",
        "\n",
        "    if token_count > 10000:  # Reduced for Llama\n",
        "        print(\"📊 Text is large, chunking into smaller pieces...\")\n",
        "        chunks = multi_kg_system.chunker.chunk_text_by_sentences(domain_text, max_tokens=10000)\n",
        "        print(f\"📄 Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(\"📄 Text is small enough to process as single chunk\")\n",
        "        chunks = [domain_text]\n",
        "\n",
        "    # Extract events and create RDF (without RAG)\n",
        "    all_turtle_outputs = []\n",
        "    all_entities = set()\n",
        "\n",
        "    print(\"\\n🔄 Processing chunks for event extraction (without RAG)...\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"\\n🔄 Processing chunk {i}/{len(chunks)}...\")\n",
        "\n",
        "        turtle_output = multi_kg_system.process_chunk(chunk, i, llm)\n",
        "        if turtle_output:\n",
        "            all_turtle_outputs.append(turtle_output)\n",
        "\n",
        "        chunk_entities = multi_kg_system.extract_entities_advanced(chunk)\n",
        "        all_entities.update(chunk_entities)\n",
        "\n",
        "        if i < len(chunks):\n",
        "            time.sleep(1)\n",
        "\n",
        "    # Save RDF output\n",
        "    if all_turtle_outputs:\n",
        "        prefixes = \"\"\"@prefix ste: <http://www.example.org/ste#> .\n",
        "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
        "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
        "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
        "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
        "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
        "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        final_output = prefixes + \"# Historical Events with Knowledge Graph Enhanced Location Data (RAG DEACTIVATED) - LLAMA\\n\" + \"\\n\\n\".join(all_turtle_outputs)\n",
        "\n",
        "        with open(OUTPUT_RAG_TTL, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output)\n",
        "\n",
        "        print(f\"\\n✅ Saved RDF to {OUTPUT_RAG_TTL}\")\n",
        "        print(f\"📊 Processing Statistics:\")\n",
        "        print(f\"   - Total chunks processed: {len(chunks)}\")\n",
        "        print(f\"   - Successful chunks: {len(all_turtle_outputs)}\")\n",
        "        print(f\"   - Unique entities found: {len(all_entities)}\")\n",
        "        print(f\"   - Total KG facts retrieved: {multi_kg_system.stats['facts_retrieved']}\")\n",
        "        print(f\"   - Cache hits: {multi_kg_system.stats['cache_hits']}\")\n",
        "        print(f\"   - Locations found: {multi_kg_system.stats['locations_found']}\")\n",
        "        print(f\"   - Locations with coordinates: {multi_kg_system.stats['locations_with_coordinates']}\")\n",
        "        print(f\"   - Location duplicates avoided: {multi_kg_system.stats['location_duplicates_avoided']}\")\n",
        "        print(f\"   - Unique global locations: {len(multi_kg_system.global_locations)}\")\n",
        "        print(f\"   - RAG status: DEACTIVATED\")\n",
        "        print(f\"   - LLM: Llama 3.2 via Ollama\")\n",
        "\n",
        "        print(f\"\\n🔗 Knowledge Graph Connector Statistics:\")\n",
        "        for name, connector in multi_kg_system.connectors.items():\n",
        "            stats = connector.get_stats()\n",
        "            print(f\"   - {stats['name']}: {stats['successes']}/{stats['requests']} requests ({stats['success_rate']:.1%} success)\")\n",
        "\n",
        "        if multi_kg_system.location_extractor.location_cache:\n",
        "            successful_locations = sum(1 for v in multi_kg_system.location_extractor.location_cache.values() if v is not None)\n",
        "            total_locations = len(multi_kg_system.location_extractor.location_cache)\n",
        "            print(f\"   - Location enrichment: {successful_locations}/{total_locations} locations enriched ({successful_locations/total_locations:.1%} success)\")\n",
        "\n",
        "        print(f\"\\n📝 Sample of generated RDF:\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_output[:1000] + \"...\" if len(final_output) > 1000 else final_output)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No events were extracted from any chunks\")\n",
        "\n",
        "    # RAG SESSION DEACTIVATED\n",
        "    print(f\"\\n❌ RAG System is DEACTIVATED\")\n",
        "    print(f\"💡 To enable RAG functionality:\")\n",
        "    print(f\"   1. Set RAG_ENABLED = True at the top of the script\")\n",
        "    print(f\"   2. Ensure langchain dependencies are installed\")\n",
        "    print(f\"   3. Re-run the script\")\n",
        "\n",
        "    print(f\"\\n🎉 Process complete! Check {OUTPUT_RAG_TTL} for RDF results.\")\n",
        "    print(f\"📊 System ran in NON-RAG mode - only Knowledge Graph and Location enrichment was used.\")\n",
        "    print(f\"🦙 Using Llama 3.2 via Ollama for processing.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET62R8jln-7O",
        "outputId": "675ec075-4614-4b68-a40e-4f522d0695ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Multi-Knowledge Graph System with Chunking (RAG DEACTIVATED) - LLAMA\n",
            "✅ Using local Ollama - no API key needed.\n",
            "Loaded text from /content/drive/MyDrive/part_aa\n",
            "📄 Using YOUR text from /content/drive/MyDrive/part_aa\n",
            "📝 Text length: 398568 characters\n",
            "✅ Llama LLM initialized successfully.\n",
            "\n",
            "❌ RAG vector store setup SKIPPED (RAG is DEACTIVATED)\n",
            "🔢 Total tokens in text: 99,642\n",
            "📊 Text is large, chunking into smaller pieces...\n",
            "📄 Created 10 chunks\n",
            "\n",
            "🔄 Processing chunks for event extraction (without RAG)...\n",
            "\n",
            "🔄 Processing chunk 1/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "This\n",
            "WARNING:__main__:Wikidata returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:DBpedia returned status 400 for Peloponnesian War\n",
            "Author\n",
            "WARNING:__main__:Wikidata returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:DBpedia returned status 400 for Thucydides\n",
            "Translator\n",
            "WARNING:__main__:Wikidata returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:DBpedia returned status 400 for Richard Crawley\n",
            "Release Date\n",
            "WARNING:__main__:Wikidata returned status 400 for English\n",
            "Character\n",
            "WARNING:__main__:DBpedia returned status 400 for English\n",
            "Character\n",
            "WARNING:__main__:⏰ Timeout: 1/21 KG queries still pending, continuing with available results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 2/10...\n",
            "\n",
            "🔄 Processing chunk 3/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER III\n",
            "Congress\n",
            "WARNING:__main__:Wikidata returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n",
            "WARNING:__main__:DBpedia returned status 400 for Lacedaemon\n",
            "\n",
            "The Athenians\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 4/10...\n",
            "\n",
            "🔄 Processing chunk 5/10...\n",
            "\n",
            "🔄 Processing chunk 6/10...\n",
            "\n",
            "🔄 Processing chunk 7/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VII\n",
            "Second Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VII\n",
            "Second Year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 8/10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Wikidata returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:DBpedia returned status 400 for CHAPTER VIII\n",
            "Third Year\n",
            "WARNING:__main__:Wikidata returned status 400 for Sitalces\n",
            "\n",
            "The\n",
            "WARNING:__main__:DBpedia returned status 400 for Sitalces\n",
            "\n",
            "The\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Processing chunk 9/10...\n",
            "\n",
            "🔄 Processing chunk 10/10...\n",
            "\n",
            "✅ Saved RDF to /content/drive/MyDrive/extracted_events_norag_with_multi_kg_llama.ttl\n",
            "📊 Processing Statistics:\n",
            "   - Total chunks processed: 10\n",
            "   - Successful chunks: 10\n",
            "   - Unique entities found: 123\n",
            "   - Total KG facts retrieved: 1\n",
            "   - Cache hits: 314\n",
            "   - Locations found: 37\n",
            "   - Locations with coordinates: 37\n",
            "   - Location duplicates avoided: 21\n",
            "   - Unique global locations: 16\n",
            "   - RAG status: DEACTIVATED\n",
            "   - LLM: Llama 3.2 via Ollama\n",
            "\n",
            "🔗 Knowledge Graph Connector Statistics:\n",
            "   - Wikidata: 36/46 requests (78.3% success)\n",
            "   - DBpedia: 26/36 requests (72.2% success)\n",
            "   - ConceptNet: 1/54 requests (1.9% success)\n",
            "   - Location enrichment: 86/287 locations enriched (30.0% success)\n",
            "\n",
            "📝 Sample of generated RDF:\n",
            "============================================================\n",
            "@prefix ste: <http://www.example.org/ste#> .\n",
            "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
            "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
            "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
            "@prefix dbp: <http://dbpedia.org/ontology/> .\n",
            "@prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .\n",
            "@prefix dbpr: <http://dbpedia.org/resource/> .\n",
            "\n",
            "# Historical Events with Knowledge Graph Enhanced Location Data (RAG DEACTIVATED) - LLAMA\n",
            "ste:Event1_1 a ste:Event ;\n",
            "    ste:hasType \"The Corinthians sent a herald before them to declare war and, getting under way with seventy-five ships and two thousand heavy infantry, sailed for Epidamnus to give battle to the Corcyraeans\" ;\n",
            "    ste:hasAgent \"Corinthians\" ;\n",
            "    ste:hasTime \"after the engagement\" ;\n",
            "    ste:hasLocation \"Epidamnus\" ;\n",
            "    ste:hasLatitude \"\" ;\n",
            "    ste:hasLongitude \"\" ;\n",
            "    ste:hasCountry \"\" ;\n",
            "    ste:hasRegion \"\" ;\n",
            "    ste:hasLocationSource \"wikidata\" ;\n",
            "    ste:hasResult \"defeated at sea, the Corinthians and their...\n",
            "============================================================\n",
            "\n",
            "❌ RAG System is DEACTIVATED\n",
            "💡 To enable RAG functionality:\n",
            "   1. Set RAG_ENABLED = True at the top of the script\n",
            "   2. Ensure langchain dependencies are installed\n",
            "   3. Re-run the script\n",
            "\n",
            "🎉 Process complete! Check /content/drive/MyDrive/extracted_events_norag_with_multi_kg_llama.ttl for RDF results.\n",
            "📊 System ran in NON-RAG mode - only Knowledge Graph and Location enrichment was used.\n",
            "🦙 Using Llama 3.2 via Ollama for processing.\n"
          ]
        }
      ]
    }
  ]
}